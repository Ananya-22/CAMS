{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ananya-22/CAMS/blob/main/Basic_code%2B5_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Importing and installation"
      ],
      "metadata": {
        "id": "t_jTHNmjessZ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ty9n-AC7EIub",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e8e2b62-b95e-4b1b-d9bf-55d23eec8dfc"
      },
      "source": [
        "# Installations\n",
        "import sys\n",
        "if 'google.colab' in sys.modules:\n",
        "    !pip install emoji --upgrade\n",
        "    !pip install pandas-profiling==2.*\n",
        "    !pip install plotly==4.*\n",
        "    !python -m spacy download en_core_web_lg\n",
        "    !pip install pyldavis\n",
        "    !pip install gensim\n",
        "    !pip install chart_studio\n",
        "    !pip install --upgrade autopep8"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting emoji\n",
            "  Downloading emoji-2.2.0.tar.gz (240 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.9/240.9 KB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-2.2.0-py3-none-any.whl size=234926 sha256=8f7856355a06a6d11ca3caf2c10cfce2dd8f41beee1aa642acf0814209b5e127\n",
            "  Stored in directory: /root/.cache/pip/wheels/9a/b8/0f/f580817231cbf59f6ade9fd132ff60ada1de9f7dc85521f857\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji\n",
            "Successfully installed emoji-2.2.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pandas-profiling==2.*\n",
            "  Downloading pandas_profiling-2.13.0-py2.py3-none-any.whl (245 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.2/245.2 KB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tangled-up-in-unicode>=0.0.6 in /usr/local/lib/python3.9/dist-packages (from pandas-profiling==2.*) (0.2.0)\n",
            "Requirement already satisfied: jinja2>=2.11.1 in /usr/local/lib/python3.9/dist-packages (from pandas-profiling==2.*) (3.1.2)\n",
            "Requirement already satisfied: attrs>=19.3.0 in /usr/local/lib/python3.9/dist-packages (from pandas-profiling==2.*) (22.2.0)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.9/dist-packages (from pandas-profiling==2.*) (1.22.4)\n",
            "Requirement already satisfied: matplotlib>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from pandas-profiling==2.*) (3.7.1)\n",
            "Requirement already satisfied: tqdm>=4.48.2 in /usr/local/lib/python3.9/dist-packages (from pandas-profiling==2.*) (4.65.0)\n",
            "Requirement already satisfied: seaborn>=0.10.1 in /usr/local/lib/python3.9/dist-packages (from pandas-profiling==2.*) (0.12.2)\n",
            "Collecting visions[type_image_path]==0.7.1\n",
            "  Downloading visions-0.7.1-py3-none-any.whl (102 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.0/103.0 KB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: phik>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from pandas-profiling==2.*) (0.12.3)\n",
            "Collecting confuse>=1.0.0\n",
            "  Downloading confuse-2.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: htmlmin>=0.1.12 in /usr/local/lib/python3.9/dist-packages (from pandas-profiling==2.*) (0.1.12)\n",
            "Requirement already satisfied: requests>=2.24.0 in /usr/local/lib/python3.9/dist-packages (from pandas-profiling==2.*) (2.27.1)\n",
            "Requirement already satisfied: pandas!=1.0.0,!=1.0.1,!=1.0.2,!=1.1.0,>=0.25.3 in /usr/local/lib/python3.9/dist-packages (from pandas-profiling==2.*) (1.4.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from pandas-profiling==2.*) (1.1.1)\n",
            "Requirement already satisfied: missingno>=0.4.2 in /usr/local/lib/python3.9/dist-packages (from pandas-profiling==2.*) (0.5.2)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.9/dist-packages (from pandas-profiling==2.*) (1.10.1)\n",
            "Collecting bottleneck\n",
            "  Downloading Bottleneck-1.3.7-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (353 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m353.1/353.1 KB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multimethod==1.4\n",
            "  Downloading multimethod-1.4-py2.py3-none-any.whl (7.3 kB)\n",
            "Requirement already satisfied: networkx>=2.4 in /usr/local/lib/python3.9/dist-packages (from visions[type_image_path]==0.7.1->pandas-profiling==2.*) (3.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.9/dist-packages (from visions[type_image_path]==0.7.1->pandas-profiling==2.*) (8.4.0)\n",
            "Requirement already satisfied: imagehash in /usr/local/lib/python3.9/dist-packages (from visions[type_image_path]==0.7.1->pandas-profiling==2.*) (4.3.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.9/dist-packages (from confuse>=1.0.0->pandas-profiling==2.*) (6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2>=2.11.1->pandas-profiling==2.*) (2.1.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.2.0->pandas-profiling==2.*) (23.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.2.0->pandas-profiling==2.*) (1.0.7)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.2.0->pandas-profiling==2.*) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.2.0->pandas-profiling==2.*) (1.4.4)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.2.0->pandas-profiling==2.*) (4.39.3)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.2.0->pandas-profiling==2.*) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.2.0->pandas-profiling==2.*) (0.11.0)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.2.0->pandas-profiling==2.*) (5.12.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas!=1.0.0,!=1.0.1,!=1.0.2,!=1.1.0,>=0.25.3->pandas-profiling==2.*) (2022.7.1)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.24.0->pandas-profiling==2.*) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.24.0->pandas-profiling==2.*) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.24.0->pandas-profiling==2.*) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.24.0->pandas-profiling==2.*) (1.26.15)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from importlib-resources>=3.2.0->matplotlib>=3.2.0->pandas-profiling==2.*) (3.15.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.7->matplotlib>=3.2.0->pandas-profiling==2.*) (1.16.0)\n",
            "Requirement already satisfied: PyWavelets in /usr/local/lib/python3.9/dist-packages (from imagehash->visions[type_image_path]==0.7.1->pandas-profiling==2.*) (1.4.1)\n",
            "Installing collected packages: multimethod, confuse, bottleneck, visions, pandas-profiling\n",
            "  Attempting uninstall: multimethod\n",
            "    Found existing installation: multimethod 1.9.1\n",
            "    Uninstalling multimethod-1.9.1:\n",
            "      Successfully uninstalled multimethod-1.9.1\n",
            "  Attempting uninstall: visions\n",
            "    Found existing installation: visions 0.7.4\n",
            "    Uninstalling visions-0.7.4:\n",
            "      Successfully uninstalled visions-0.7.4\n",
            "  Attempting uninstall: pandas-profiling\n",
            "    Found existing installation: pandas-profiling 3.2.0\n",
            "    Uninstalling pandas-profiling-3.2.0:\n",
            "      Successfully uninstalled pandas-profiling-3.2.0\n",
            "Successfully installed bottleneck-1.3.7 confuse-2.0.0 multimethod-1.4 pandas-profiling-2.13.0 visions-0.7.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting plotly==4.*\n",
            "  Downloading plotly-4.14.3-py2.py3-none-any.whl (13.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m69.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting retrying>=1.3.3\n",
            "  Downloading retrying-1.3.4-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from plotly==4.*) (1.16.0)\n",
            "Installing collected packages: retrying, plotly\n",
            "  Attempting uninstall: plotly\n",
            "    Found existing installation: plotly 5.13.1\n",
            "    Uninstalling plotly-5.13.1:\n",
            "      Successfully uninstalled plotly-5.13.1\n",
            "Successfully installed plotly-4.14.3 retrying-1.3.4\n",
            "/usr/local/lib/python3.9/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n",
            "2023-04-01 15:56:14.238018: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-04-01 15:56:15.561986: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-04-01 15:56:17.705827: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:266] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en-core-web-lg==3.5.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.5.0/en_core_web_lg-3.5.0-py3-none-any.whl (587.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m587.7/587.7 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /usr/local/lib/python3.9/dist-packages (from en-core-web-lg==3.5.0) (3.5.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (3.0.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (23.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.22.4)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.0.7)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (4.65.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.10.7)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (8.1.9)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (0.7.0)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.1.1)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.0.9)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (6.3.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (0.10.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.4.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.27.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (3.0.12)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.0.8)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (3.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (67.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (3.1.2)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.9/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (3.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.9/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.9/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (0.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.9/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.1.2)\n",
            "Installing collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-3.5.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_lg')\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyldavis\n",
            "  Downloading pyLDAvis-3.4.0-py3-none-any.whl (2.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting funcy\n",
            "  Downloading funcy-2.0-py2.py3-none-any.whl (30 kB)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from pyldavis) (3.1.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from pyldavis) (1.10.1)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.9/dist-packages (from pyldavis) (2.8.4)\n",
            "Requirement already satisfied: pandas>=1.3.4 in /usr/local/lib/python3.9/dist-packages (from pyldavis) (1.4.4)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from pyldavis) (1.2.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from pyldavis) (67.6.1)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.9/dist-packages (from pyldavis) (4.3.1)\n",
            "Collecting joblib>=1.2.0\n",
            "  Downloading joblib-1.2.0-py3-none-any.whl (297 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 KB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.22.0 in /usr/local/lib/python3.9/dist-packages (from pyldavis) (1.22.4)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.3.4->pyldavis) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.3.4->pyldavis) (2.8.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=1.0.0->pyldavis) (3.1.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.9/dist-packages (from gensim->pyldavis) (6.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->pyldavis) (2.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas>=1.3.4->pyldavis) (1.16.0)\n",
            "Installing collected packages: funcy, joblib, pyldavis\n",
            "  Attempting uninstall: joblib\n",
            "    Found existing installation: joblib 1.1.1\n",
            "    Uninstalling joblib-1.1.1:\n",
            "      Successfully uninstalled joblib-1.1.1\n",
            "Successfully installed funcy-2.0 joblib-1.2.0 pyldavis-3.4.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.9/dist-packages (4.3.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.9/dist-packages (from gensim) (6.3.0)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.9/dist-packages (from gensim) (1.10.1)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.9/dist-packages (from gensim) (1.22.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting chart_studio\n",
            "  Downloading chart_studio-1.1.0-py3-none-any.whl (64 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.4/64.4 KB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from chart_studio) (1.16.0)\n",
            "Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.9/dist-packages (from chart_studio) (1.3.4)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.9/dist-packages (from chart_studio) (4.14.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from chart_studio) (2.27.1)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->chart_studio) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->chart_studio) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->chart_studio) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->chart_studio) (2022.12.7)\n",
            "Installing collected packages: chart_studio\n",
            "Successfully installed chart_studio-1.1.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting autopep8\n",
            "  Downloading autopep8-2.0.2-py2.py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 KB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tomli in /usr/local/lib/python3.9/dist-packages (from autopep8) (2.0.1)\n",
            "Collecting pycodestyle>=2.10.0\n",
            "  Downloading pycodestyle-2.10.0-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 KB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pycodestyle, autopep8\n",
            "Successfully installed autopep8-2.0.2 pycodestyle-2.10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#installing tweet-preprocessor\n",
        "!pip install tweet-preprocessor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x40CSMZGowhN",
        "outputId": "a4fcc055-ff77-45bf-9e53-d41f4d43d757"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tweet-preprocessor\n",
            "  Downloading tweet_preprocessor-0.6.0-py3-none-any.whl (27 kB)\n",
            "Installing collected packages: tweet-preprocessor\n",
            "Successfully installed tweet-preprocessor-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APMx7pRNEQFa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93f1132c-3bd6-40b6-ca3b-4c769fca5826"
      },
      "source": [
        "# Required Libraries\n",
        "\n",
        "#Base and Cleaning \n",
        "import json\n",
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import emoji\n",
        "import regex\n",
        "import re\n",
        "import string\n",
        "from collections import Counter\n",
        "\n",
        "#Visualizations\n",
        "import plotly.express as px\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt \n",
        "import pyLDAvis.gensim\n",
        "import chart_studio\n",
        "import chart_studio.plotly as py \n",
        "import chart_studio.tools as tls\n",
        "\n",
        "#Natural Language Processing (NLP)\n",
        "import spacy\n",
        "import gensim\n",
        "from spacy.tokenizer import Tokenizer\n",
        "from gensim.corpora import Dictionary\n",
        "from gensim.models.ldamulticore import LdaMulticore\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "from gensim.parsing.preprocessing import STOPWORDS as SW\n",
        "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from pprint import pprint\n",
        "from wordcloud import STOPWORDS\n",
        "stopwords = set(STOPWORDS)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n",
            "/usr/local/lib/python3.9/dist-packages/google/rpc/__init__.py:20: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google.rpc')`.\n",
            "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
            "  pkg_resources.declare_namespace(__name__)\n",
            "/usr/local/lib/python3.9/dist-packages/pkg_resources/__init__.py:2349: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.\n",
            "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
            "  declare_namespace(parent)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Dataset upload"
      ],
      "metadata": {
        "id": "oScqC398hta9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('sentiment_tweets3.csv')\n",
        "df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5adDW_q0hsnD",
        "outputId": "28eac981-a8c4-4c0c-cda1-81de7ef620d5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Index', 'message to examine', 'label (depression result)'], dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df=df.drop(['Index'],axis=1)\n",
        "df.rename(columns = {'message to examine':'original_tweets'}, inplace = True)"
      ],
      "metadata": {
        "id": "MfM3aB8oiOC3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4dce3eb-0b81-434c-f8ea-61d296a2b601"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "Xwrxh1vDjLIH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        },
        "outputId": "ec4ccf76-3847-4cbd-c613-2a950bf9df0b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                     original_tweets  \\\n",
              "0  just had a real good moment. i missssssssss hi...   \n",
              "1         is reading manga  http://plurk.com/p/mzp1e   \n",
              "2  @comeagainjen http://twitpic.com/2y2lx - http:...   \n",
              "3  @lapcat Need to send 'em to my accountant tomo...   \n",
              "4      ADD ME ON MYSPACE!!!  myspace.com/LookThunder   \n",
              "\n",
              "   label (depression result)  \n",
              "0                          0  \n",
              "1                          0  \n",
              "2                          0  \n",
              "3                          0  \n",
              "4                          0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-04e23f94-65b3-4267-b290-5996d0e1602e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>original_tweets</th>\n",
              "      <th>label (depression result)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>just had a real good moment. i missssssssss hi...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>is reading manga  http://plurk.com/p/mzp1e</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>@comeagainjen http://twitpic.com/2y2lx - http:...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>@lapcat Need to send 'em to my accountant tomo...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ADD ME ON MYSPACE!!!  myspace.com/LookThunder</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-04e23f94-65b3-4267-b290-5996d0e1602e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-04e23f94-65b3-4267-b290-5996d0e1602e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-04e23f94-65b3-4267-b290-5996d0e1602e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data cleaning"
      ],
      "metadata": {
        "id": "W0jfd-ZaepS7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "slang_abbrev_dict = {\n",
        "    'AFAIK': 'As Far As I Know',\n",
        "    'AFK': 'Away From Keyboard',\n",
        "    'ASAP': 'As Soon As Possible',\n",
        "    'ATK': 'At The Keyboard',\n",
        "    'ATM': 'At The Moment',\n",
        "    'A3': 'Anytime, Anywhere, Anyplace',\n",
        "    'BAK': 'Back At Keyboard',\n",
        "    'BBL': 'Be Back Later',\n",
        "    'BBS': 'Be Back Soon',\n",
        "    'BFN': 'Bye For Now',\n",
        "    'B4N': 'Bye For Now',\n",
        "    'BRB': 'Be Right Back',\n",
        "    'BRT': 'Be Right There',\n",
        "    'BTW': 'By The Way',\n",
        "    'B4': 'Before',\n",
        "    'B4N': 'Bye For Now',\n",
        "    'CU': 'See You',\n",
        "    'CUL8R': 'See You Later',\n",
        "    'CYA': 'See You',\n",
        "    'FAQ': 'Frequently Asked Questions',\n",
        "    'FC': 'Fingers Crossed',\n",
        "    'FWIW': 'For What It\\'s Worth',\n",
        "    'FYI': 'For Your Information',\n",
        "    'GAL': 'Get A Life',\n",
        "    'GG': 'Good Game',\n",
        "    'GN': 'Good Night',\n",
        "    'GMTA': 'Great Minds Think Alike',\n",
        "    'GR8': 'Great!',\n",
        "    'G9': 'Genius',\n",
        "    'IC': 'I See',\n",
        "    'ICQ': 'I Seek you',\n",
        "    'ILU': 'I Love You',\n",
        "    'IMHO': 'In My Humble Opinion',\n",
        "    'IMO': 'In My Opinion',\n",
        "    'IOW': 'In Other Words',\n",
        "    'IRL': 'In Real Life',\n",
        "    'KISS': 'Keep It Simple, Stupid',\n",
        "    'LDR': 'Long Distance Relationship',\n",
        "    'LMAO': 'Laugh My Ass Off',\n",
        "    'LOL': 'Laughing Out Loud',\n",
        "    'LTNS': 'Long Time No See',\n",
        "    'L8R': 'Later',\n",
        "    'MTE': 'My Thoughts Exactly',\n",
        "    'M8': 'Mate',\n",
        "    'NRN': 'No Reply Necessary',\n",
        "    'OIC': 'Oh I See',\n",
        "    'OMG': 'Oh My God',\n",
        "    'PITA': 'Pain In The Ass',\n",
        "    'PRT': 'Party',\n",
        "    'PRW': 'Parents Are Watching',\n",
        "    'QPSA?': 'Que Pasa?',\n",
        "    'ROFL': 'Rolling On The Floor Laughing',\n",
        "    'ROFLOL': 'Rolling On The Floor Laughing Out Loud',\n",
        "    'ROTFLMAO': 'Rolling On The Floor Laughing My Ass Off',\n",
        "    'SK8': 'Skate',\n",
        "    'STATS': 'Your sex and age',\n",
        "    'ASL': 'Age, Sex, Location',\n",
        "    'THX': 'Thank You',\n",
        "    'TTFN': 'Ta-Ta For Now!',\n",
        "    'TTYL': 'Talk To You Later',\n",
        "    'U': 'You',\n",
        "    'U2': 'You Too',\n",
        "    'U4E': 'Yours For Ever',\n",
        "    'WB': 'Welcome Back',\n",
        "    'WTF': 'What The Fuck',\n",
        "    'WTG': 'Way To Go!',\n",
        "    'WUF': 'Where Are You From?',\n",
        "    'W8': 'Wait',\n",
        "    '7K': 'Sick:-D Laugher'\n",
        "}"
      ],
      "metadata": {
        "id": "HUwVu1x2k2FO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26c3cb28-f161-458d-f9d1-764d9916084b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# spell = SpellChecker()\n",
        "\n",
        "# def correct_spellings(text):\n",
        "#     corrected_text = []\n",
        "#     misspelled_words = spell.unknown(text.split())\n",
        "#     for word in text.split():\n",
        "#         if word in misspelled_words:\n",
        "#             corrected_text.append(spell.correction(word))\n",
        "#         else:\n",
        "#             corrected_text.append(word)\n",
        "#     return \" \".join(corrected_text)"
      ],
      "metadata": {
        "id": "p5-koyDPtl55",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bab1679f-a7e6-41ed-d0bd-7e686b59c0fb"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def remove_emoji(text):\n",
        "#     \"\"\"\n",
        "#     Removes emoji's from tweets\n",
        "#     Accepts:\n",
        "#         Text (tweets)\n",
        "#     Returns:\n",
        "#         Text (emoji free tweets)\n",
        "#     \"\"\"\n",
        "#     emoji_list = [c for c in text if c in emoji_data.unicode_emoji]\n",
        "#     clean_text = ' '.join([str for str in text.split() if not any(i in str for i in emoji_list)])\n",
        "#     return clean_text\n",
        "\n",
        "def remove_emoji(text):\n",
        "    emoji_pattern = re.compile(\n",
        "        \"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "        u\"\\U00002702-\\U000027B0\"\n",
        "        u\"\\U000024C2-\\U0001F251\"\n",
        "        \"]+\",\n",
        "        flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', text)\n",
        "\n",
        "\n",
        "def url_free_text(text):    \n",
        "    # Cleans text from urls\n",
        "    text = re.sub(r'http\\S+', '', text)\n",
        "    return text\n",
        "\n",
        "def username_free_text(text):\n",
        "    # remove @username from tweets\n",
        "    text = re.sub('@[\\w]+','',text)\n",
        "    # remove hashtags\n",
        "    text = re.sub(r'#\\w+ ?', '', text)\n",
        "    # #remove reserved word such as RT,FAV\n",
        "    # text= p.OPT.RESERVED(text)\n",
        "    return text\n",
        "\n",
        "def decontracted(phrase):\n",
        "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
        "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
        "    phrase = re.sub(r\"n't\", \" not\", phrase) \n",
        "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
        "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
        "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
        "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
        "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
        "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
        "    return phrase\n",
        "\n",
        "\n",
        "def unslang(text):\n",
        "    # Converts text like \"OMG\" into \"Oh my God\"\n",
        "  \n",
        "    if text.upper() in slang_abbrev_dict.keys():\n",
        "        return slang_abbrev_dict[text.upper()]\n",
        "    else:\n",
        "        return text\n",
        "\n",
        "def remove_punc(text):\n",
        "    # remove numbers\n",
        "    text_nonum = re.sub(r'\\d+', '', text)\n",
        "\n",
        "    # remove punctuations and convert characters to lower case\n",
        "    text_nopunct = \"\".join([char.lower() for char in text_nonum if char not in string.punctuation]) \n",
        "    \n",
        "    # substitute multiple whitespace with single whitespace Also, removes leading and trailing whitespaces\n",
        "    text_no_doublespace = re.sub('\\s+', ' ', text_nopunct).strip()\n",
        "    return text_no_doublespace\n",
        "    \n",
        "\n",
        "# Apply the function above and get tweets free of emoji's\n",
        "call_emoji_free = lambda x: remove_emoji(x)\n",
        "\n",
        "# Apply `call_emoji_free` which calls the function to remove all emoji's\n",
        "df['emoji_free_tweets'] = df['original_tweets'].apply(remove_emoji)\n",
        "df['original_tweets'] = df['emoji_free_tweets'].astype(str)\n",
        "\n",
        "#Create a new column with url free tweets\n",
        "df['url_free_tweets'] = df['original_tweets'].apply(url_free_text)\n",
        "\n",
        "#Create a new column with username free tweets\n",
        "df['username_free_tweets'] = df['url_free_tweets'].apply(username_free_text)\n",
        "\n",
        "#Create a new column with removing can't with cannot tweets\n",
        "df['slang_free_tweets'] = df['username_free_tweets'].apply(decontracted)\n",
        "\n",
        "#Create a new column  removing OMG with oh my god tweets\n",
        "df['slang_free_tweets'] = df['slang_free_tweets'].apply(unslang)\n",
        "\n",
        "#Create a new column with no pun\n",
        "df['punc_free_tweets'] = df['slang_free_tweets'].apply(remove_punc)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "g2G6yYwHenUV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d96ea8e-7a44-41c5-ddde-7b74dbbf2685"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "<>:34: DeprecationWarning: invalid escape sequence \\w\n",
            "<>:71: DeprecationWarning: invalid escape sequence \\s\n",
            "<>:34: DeprecationWarning: invalid escape sequence \\w\n",
            "<>:71: DeprecationWarning: invalid escape sequence \\s\n",
            "<ipython-input-12-8dce7b2b5bb6>:34: DeprecationWarning: invalid escape sequence \\w\n",
            "  text = re.sub('@[\\w]+','',text)\n",
            "<ipython-input-12-8dce7b2b5bb6>:71: DeprecationWarning: invalid escape sequence \\s\n",
            "  text_no_doublespace = re.sub('\\s+', ' ', text_nopunct).strip()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "W3qQoStxvjDz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "outputId": "a0d62455-f544-4feb-9543-07f1d2737b0e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                     original_tweets  \\\n",
              "0  just had a real good moment. i missssssssss hi...   \n",
              "1         is reading manga  http://plurk.com/p/mzp1e   \n",
              "2  @comeagainjen http://twitpic.com/2y2lx - http:...   \n",
              "3  @lapcat Need to send 'em to my accountant tomo...   \n",
              "4      ADD ME ON MYSPACE!!!  myspace.com/LookThunder   \n",
              "\n",
              "   label (depression result)  \\\n",
              "0                          0   \n",
              "1                          0   \n",
              "2                          0   \n",
              "3                          0   \n",
              "4                          0   \n",
              "\n",
              "                                     url_free_tweets  \\\n",
              "0  just had a real good moment. i missssssssss hi...   \n",
              "1                                 is reading manga     \n",
              "2                                 @comeagainjen  -     \n",
              "3  @lapcat Need to send 'em to my accountant tomo...   \n",
              "4      ADD ME ON MYSPACE!!!  myspace.com/LookThunder   \n",
              "\n",
              "                                username_free_tweets  \\\n",
              "0  just had a real good moment. i missssssssss hi...   \n",
              "1                                 is reading manga     \n",
              "2                                                -     \n",
              "3   Need to send 'em to my accountant tomorrow. O...   \n",
              "4      ADD ME ON MYSPACE!!!  myspace.com/LookThunder   \n",
              "\n",
              "                                   slang_free_tweets  \\\n",
              "0  just had a real good moment. i missssssssss hi...   \n",
              "1                                 is reading manga     \n",
              "2                                                -     \n",
              "3   Need to send 'em to my accountant tomorrow. O...   \n",
              "4      ADD ME ON MYSPACE!!!  myspace.com/LookThunder   \n",
              "\n",
              "                                    punc_free_tweets  \\\n",
              "0  just had a real good moment i missssssssss him...   \n",
              "1                                   is reading manga   \n",
              "2                                                      \n",
              "3  need to send em to my accountant tomorrow oddl...   \n",
              "4            add me on myspace myspacecomlookthunder   \n",
              "\n",
              "                                   emoji_free_tweets  \n",
              "0  just had a real good moment. i missssssssss hi...  \n",
              "1         is reading manga  http://plurk.com/p/mzp1e  \n",
              "2  @comeagainjen http://twitpic.com/2y2lx - http:...  \n",
              "3  @lapcat Need to send 'em to my accountant tomo...  \n",
              "4      ADD ME ON MYSPACE!!!  myspace.com/LookThunder  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0816f3ff-f0a5-45a8-9763-c2c3e07cab7d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>original_tweets</th>\n",
              "      <th>label (depression result)</th>\n",
              "      <th>url_free_tweets</th>\n",
              "      <th>username_free_tweets</th>\n",
              "      <th>slang_free_tweets</th>\n",
              "      <th>punc_free_tweets</th>\n",
              "      <th>emoji_free_tweets</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>just had a real good moment. i missssssssss hi...</td>\n",
              "      <td>0</td>\n",
              "      <td>just had a real good moment. i missssssssss hi...</td>\n",
              "      <td>just had a real good moment. i missssssssss hi...</td>\n",
              "      <td>just had a real good moment. i missssssssss hi...</td>\n",
              "      <td>just had a real good moment i missssssssss him...</td>\n",
              "      <td>just had a real good moment. i missssssssss hi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>is reading manga  http://plurk.com/p/mzp1e</td>\n",
              "      <td>0</td>\n",
              "      <td>is reading manga</td>\n",
              "      <td>is reading manga</td>\n",
              "      <td>is reading manga</td>\n",
              "      <td>is reading manga</td>\n",
              "      <td>is reading manga  http://plurk.com/p/mzp1e</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>@comeagainjen http://twitpic.com/2y2lx - http:...</td>\n",
              "      <td>0</td>\n",
              "      <td>@comeagainjen  -</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td></td>\n",
              "      <td>@comeagainjen http://twitpic.com/2y2lx - http:...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>@lapcat Need to send 'em to my accountant tomo...</td>\n",
              "      <td>0</td>\n",
              "      <td>@lapcat Need to send 'em to my accountant tomo...</td>\n",
              "      <td>Need to send 'em to my accountant tomorrow. O...</td>\n",
              "      <td>Need to send 'em to my accountant tomorrow. O...</td>\n",
              "      <td>need to send em to my accountant tomorrow oddl...</td>\n",
              "      <td>@lapcat Need to send 'em to my accountant tomo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ADD ME ON MYSPACE!!!  myspace.com/LookThunder</td>\n",
              "      <td>0</td>\n",
              "      <td>ADD ME ON MYSPACE!!!  myspace.com/LookThunder</td>\n",
              "      <td>ADD ME ON MYSPACE!!!  myspace.com/LookThunder</td>\n",
              "      <td>ADD ME ON MYSPACE!!!  myspace.com/LookThunder</td>\n",
              "      <td>add me on myspace myspacecomlookthunder</td>\n",
              "      <td>ADD ME ON MYSPACE!!!  myspace.com/LookThunder</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0816f3ff-f0a5-45a8-9763-c2c3e07cab7d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0816f3ff-f0a5-45a8-9763-c2c3e07cab7d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0816f3ff-f0a5-45a8-9763-c2c3e07cab7d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data preprocessing"
      ],
      "metadata": {
        "id": "Rf_HZ9ocxnM2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load spacy\n",
        "# Make sure to restart the runtime after running installations and libraries tab\n",
        "nlp = spacy.load('en_core_web_lg')"
      ],
      "metadata": {
        "id": "OKTGaUYsvpPr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df5383f2-1224-4200-f1aa-28b306d559f9"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizer\n",
        "tokenizer = Tokenizer(nlp.vocab)\n",
        "\n",
        "\n",
        "# Custom stopwords\n",
        "custom_stopwords = ['hi','\\n','\\n\\n', '&', ' ', '.', '-', 'got', \"it's\", 'it’s', \"i'm\", 'i’m', 'im', 'want', 'like', '$', '@']\n",
        "\n",
        "# Customize stop words by adding to the default list\n",
        "STOP_WORDS = nlp.Defaults.stop_words.union(custom_stopwords)\n",
        "\n",
        "# ALL_STOP_WORDS = spacy + gensim + wordcloud\n",
        "ALL_STOP_WORDS = STOP_WORDS.union(SW).union(stopwords)\n",
        "\n",
        "\n",
        "tokens = []\n",
        "\n",
        "for doc in tokenizer.pipe(df['punc_free_tweets'], batch_size=500):\n",
        "    doc_tokens = []    \n",
        "    for token in doc: \n",
        "        if token.text.lower() not in STOP_WORDS:\n",
        "            doc_tokens.append(token.text.lower())   \n",
        "    tokens.append(doc_tokens)\n",
        "\n",
        "# Makes tokens column\n",
        "df['tokens'] = tokens\n",
        "     "
      ],
      "metadata": {
        "id": "VdBUWYsHxq7p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc5ed607-a845-4662-eec6-528cb6c82bc6"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make tokens a string again\n",
        "df['tokens_back_to_text'] = [' '.join(map(str, l)) for l in df['tokens']]\n",
        "\n",
        "def get_lemmas(text):\n",
        "    '''Used to lemmatize the processed tweets'''\n",
        "    lemmas = []\n",
        "    \n",
        "    doc = nlp(text)\n",
        "    \n",
        "    # Something goes here :P\n",
        "    for token in doc: \n",
        "        if ((token.is_stop == False) and (token.is_punct == False)) and (token.pos_ != 'PRON'):\n",
        "            lemmas.append(token.lemma_)\n",
        "    \n",
        "    return lemmas\n",
        "\n",
        "df['lemmas'] = df['tokens_back_to_text'].apply(get_lemmas)"
      ],
      "metadata": {
        "id": "k6smXp7VxvdJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef5c6375-044a-4fdd-e593-254d23b60812"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make lemmas a string again\n",
        "df['lemmas_back_to_text'] = [' '.join(map(str, l)) for l in df['lemmas']]\n",
        "\n",
        "# Tokenizer function\n",
        "def tokenize(text):\n",
        "    \"\"\"\n",
        "    Parses a string into a list of semantic units (words)\n",
        "    Args:\n",
        "        text (str): The string that the function will tokenize.\n",
        "    Returns:\n",
        "        list: tokens parsed out\n",
        "    \"\"\"\n",
        "    # Removing url's\n",
        "    pattern = r\"http\\S+\"\n",
        "    \n",
        "    tokens = re.sub(pattern, \"\", text) # https://www.youtube.com/watch?v=O2onA4r5UaY\n",
        "    tokens = re.sub('[^a-zA-Z 0-9]', '', text)\n",
        "    tokens = re.sub('[%s]' % re.escape(string.punctuation), '', text) # Remove punctuation\n",
        "    tokens = re.sub('\\w*\\d\\w*', '', text) # Remove words containing numbers\n",
        "    # tokens = re.sub('@*!*$*', '', text) # Remove @ ! $\n",
        "    tokens = tokens.strip(',') # TESTING THIS LINE\n",
        "    tokens = tokens.strip('?') # TESTING THIS LINE\n",
        "    tokens = tokens.strip('!') # TESTING THIS LINE\n",
        "    tokens = tokens.strip(\"'\") # TESTING THIS LINE\n",
        "    tokens = tokens.strip(\".\") # TESTING THIS LINE\n",
        "\n",
        "    tokens = tokens.lower().split() # Make text lowercase and split it\n",
        "    \n",
        "    return tokens\n",
        "\n",
        "# Apply tokenizer\n",
        "df['lemma_tokens'] = df['lemmas_back_to_text'].apply(tokenize)"
      ],
      "metadata": {
        "id": "-yi3HUVDxyNn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d0749ce-d109-40ac-868a-e65565a0d184"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "<>:19: DeprecationWarning: invalid escape sequence \\w\n",
            "<>:19: DeprecationWarning: invalid escape sequence \\w\n",
            "<ipython-input-42-0214d658a4f1>:19: DeprecationWarning: invalid escape sequence \\w\n",
            "  tokens = re.sub('\\w*\\d\\w*', '', text) # Remove words containing numbers\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "V8T5hMUTzJSV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "outputId": "25f9d9aa-7eb0-44c2-ae5b-9956596424eb"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                     original_tweets  \\\n",
              "0  just had a real good moment. i missssssssss hi...   \n",
              "1         is reading manga  http://plurk.com/p/mzp1e   \n",
              "2  @comeagainjen http://twitpic.com/2y2lx - http:...   \n",
              "3  @lapcat Need to send 'em to my accountant tomo...   \n",
              "4      ADD ME ON MYSPACE!!!  myspace.com/LookThunder   \n",
              "\n",
              "   label (depression result)  \\\n",
              "0                          0   \n",
              "1                          0   \n",
              "2                          0   \n",
              "3                          0   \n",
              "4                          0   \n",
              "\n",
              "                                     url_free_tweets  \\\n",
              "0  just had a real good moment. i missssssssss hi...   \n",
              "1                                 is reading manga     \n",
              "2                                 @comeagainjen  -     \n",
              "3  @lapcat Need to send 'em to my accountant tomo...   \n",
              "4      ADD ME ON MYSPACE!!!  myspace.com/LookThunder   \n",
              "\n",
              "                                username_free_tweets  \\\n",
              "0  just had a real good moment. i missssssssss hi...   \n",
              "1                                 is reading manga     \n",
              "2                                                -     \n",
              "3   Need to send 'em to my accountant tomorrow. O...   \n",
              "4      ADD ME ON MYSPACE!!!  myspace.com/LookThunder   \n",
              "\n",
              "                                   slang_free_tweets  \\\n",
              "0  just had a real good moment. i missssssssss hi...   \n",
              "1                                 is reading manga     \n",
              "2                                                -     \n",
              "3   Need to send 'em to my accountant tomorrow. O...   \n",
              "4      ADD ME ON MYSPACE!!!  myspace.com/LookThunder   \n",
              "\n",
              "                                    punc_free_tweets  \\\n",
              "0  just had a real good moment i missssssssss him...   \n",
              "1                                   is reading manga   \n",
              "2                                                      \n",
              "3  need to send em to my accountant tomorrow oddl...   \n",
              "4            add me on myspace myspacecomlookthunder   \n",
              "\n",
              "                                   emoji_free_tweets  \\\n",
              "0  just had a real good moment. i missssssssss hi...   \n",
              "1         is reading manga  http://plurk.com/p/mzp1e   \n",
              "2  @comeagainjen http://twitpic.com/2y2lx - http:...   \n",
              "3  @lapcat Need to send 'em to my accountant tomo...   \n",
              "4      ADD ME ON MYSPACE!!!  myspace.com/LookThunder   \n",
              "\n",
              "                                              tokens  \\\n",
              "0                 [real, good, moment, missssssssss]   \n",
              "1                                   [reading, manga]   \n",
              "2                                                 []   \n",
              "3  [need, send, em, accountant, tomorrow, oddly, ...   \n",
              "4              [add, myspace, myspacecomlookthunder]   \n",
              "\n",
              "                                 tokens_back_to_text  \\\n",
              "0                      real good moment missssssssss   \n",
              "1                                      reading manga   \n",
              "2                                                      \n",
              "3  need send em accountant tomorrow oddly referri...   \n",
              "4                  add myspace myspacecomlookthunder   \n",
              "\n",
              "                                              lemmas  \\\n",
              "0                 [real, good, moment, missssssssss]   \n",
              "1                                      [read, manga]   \n",
              "2                                                 []   \n",
              "3  [need, send, accountant, tomorrow, oddly, refe...   \n",
              "4              [add, myspace, myspacecomlookthunder]   \n",
              "\n",
              "                                 lemmas_back_to_text  \\\n",
              "0                      real good moment missssssssss   \n",
              "1                                         read manga   \n",
              "2                                                      \n",
              "3  need send accountant tomorrow oddly refer taxi...   \n",
              "4                  add myspace myspacecomlookthunder   \n",
              "\n",
              "                                        lemma_tokens  \n",
              "0                 [real, good, moment, missssssssss]  \n",
              "1                                      [read, manga]  \n",
              "2                                                 []  \n",
              "3  [need, send, accountant, tomorrow, oddly, refe...  \n",
              "4              [add, myspace, myspacecomlookthunder]  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-22999cd2-5abe-46df-a0be-0ce7a234cbd3\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>original_tweets</th>\n",
              "      <th>label (depression result)</th>\n",
              "      <th>url_free_tweets</th>\n",
              "      <th>username_free_tweets</th>\n",
              "      <th>slang_free_tweets</th>\n",
              "      <th>punc_free_tweets</th>\n",
              "      <th>emoji_free_tweets</th>\n",
              "      <th>tokens</th>\n",
              "      <th>tokens_back_to_text</th>\n",
              "      <th>lemmas</th>\n",
              "      <th>lemmas_back_to_text</th>\n",
              "      <th>lemma_tokens</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>just had a real good moment. i missssssssss hi...</td>\n",
              "      <td>0</td>\n",
              "      <td>just had a real good moment. i missssssssss hi...</td>\n",
              "      <td>just had a real good moment. i missssssssss hi...</td>\n",
              "      <td>just had a real good moment. i missssssssss hi...</td>\n",
              "      <td>just had a real good moment i missssssssss him...</td>\n",
              "      <td>just had a real good moment. i missssssssss hi...</td>\n",
              "      <td>[real, good, moment, missssssssss]</td>\n",
              "      <td>real good moment missssssssss</td>\n",
              "      <td>[real, good, moment, missssssssss]</td>\n",
              "      <td>real good moment missssssssss</td>\n",
              "      <td>[real, good, moment, missssssssss]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>is reading manga  http://plurk.com/p/mzp1e</td>\n",
              "      <td>0</td>\n",
              "      <td>is reading manga</td>\n",
              "      <td>is reading manga</td>\n",
              "      <td>is reading manga</td>\n",
              "      <td>is reading manga</td>\n",
              "      <td>is reading manga  http://plurk.com/p/mzp1e</td>\n",
              "      <td>[reading, manga]</td>\n",
              "      <td>reading manga</td>\n",
              "      <td>[read, manga]</td>\n",
              "      <td>read manga</td>\n",
              "      <td>[read, manga]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>@comeagainjen http://twitpic.com/2y2lx - http:...</td>\n",
              "      <td>0</td>\n",
              "      <td>@comeagainjen  -</td>\n",
              "      <td>-</td>\n",
              "      <td>-</td>\n",
              "      <td></td>\n",
              "      <td>@comeagainjen http://twitpic.com/2y2lx - http:...</td>\n",
              "      <td>[]</td>\n",
              "      <td></td>\n",
              "      <td>[]</td>\n",
              "      <td></td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>@lapcat Need to send 'em to my accountant tomo...</td>\n",
              "      <td>0</td>\n",
              "      <td>@lapcat Need to send 'em to my accountant tomo...</td>\n",
              "      <td>Need to send 'em to my accountant tomorrow. O...</td>\n",
              "      <td>Need to send 'em to my accountant tomorrow. O...</td>\n",
              "      <td>need to send em to my accountant tomorrow oddl...</td>\n",
              "      <td>@lapcat Need to send 'em to my accountant tomo...</td>\n",
              "      <td>[need, send, em, accountant, tomorrow, oddly, ...</td>\n",
              "      <td>need send em accountant tomorrow oddly referri...</td>\n",
              "      <td>[need, send, accountant, tomorrow, oddly, refe...</td>\n",
              "      <td>need send accountant tomorrow oddly refer taxi...</td>\n",
              "      <td>[need, send, accountant, tomorrow, oddly, refe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ADD ME ON MYSPACE!!!  myspace.com/LookThunder</td>\n",
              "      <td>0</td>\n",
              "      <td>ADD ME ON MYSPACE!!!  myspace.com/LookThunder</td>\n",
              "      <td>ADD ME ON MYSPACE!!!  myspace.com/LookThunder</td>\n",
              "      <td>ADD ME ON MYSPACE!!!  myspace.com/LookThunder</td>\n",
              "      <td>add me on myspace myspacecomlookthunder</td>\n",
              "      <td>ADD ME ON MYSPACE!!!  myspace.com/LookThunder</td>\n",
              "      <td>[add, myspace, myspacecomlookthunder]</td>\n",
              "      <td>add myspace myspacecomlookthunder</td>\n",
              "      <td>[add, myspace, myspacecomlookthunder]</td>\n",
              "      <td>add myspace myspacecomlookthunder</td>\n",
              "      <td>[add, myspace, myspacecomlookthunder]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-22999cd2-5abe-46df-a0be-0ce7a234cbd3')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-22999cd2-5abe-46df-a0be-0ce7a234cbd3 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-22999cd2-5abe-46df-a0be-0ce7a234cbd3');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['label (depression result)'].unique()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7JJW22Km3eU",
        "outputId": "51582f3d-cead-48f0-c4ea-3f50efedd7e8"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df0 = df[df['label (depression result)'] == 0]\n",
        "df1 = df[df['label (depression result)'] == 1]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vh6R63z6lz7-",
        "outputId": "19ca9965-fc73-427a-9dff-4acdc06b711a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df0.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQUk8reJm-uE",
        "outputId": "5edd3f1b-4393-4fa2-aa7d-40218a8eab98"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "original_tweets              8000\n",
              "label (depression result)    8000\n",
              "url_free_tweets              8000\n",
              "username_free_tweets         8000\n",
              "slang_free_tweets            8000\n",
              "punc_free_tweets             8000\n",
              "emoji_free_tweets            8000\n",
              "tokens                       8000\n",
              "tokens_back_to_text          8000\n",
              "lemmas                       8000\n",
              "lemmas_back_to_text          8000\n",
              "lemma_tokens                 8000\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df1.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bmj0jrz4nDNQ",
        "outputId": "f713e006-16d7-43d7-bd1e-3900ca5081f7"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "original_tweets              2314\n",
              "label (depression result)    2314\n",
              "url_free_tweets              2314\n",
              "username_free_tweets         2314\n",
              "slang_free_tweets            2314\n",
              "punc_free_tweets             2314\n",
              "emoji_free_tweets            2314\n",
              "tokens                       2314\n",
              "tokens_back_to_text          2314\n",
              "lemmas                       2314\n",
              "lemmas_back_to_text          2314\n",
              "lemma_tokens                 2314\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Topic Modelling"
      ],
      "metadata": {
        "id": "EkLRzdjK7Zp-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a id2word dictionary\n",
        "id2word = Dictionary(df0['lemma_tokens'])\n",
        "print(len(id2word))\n",
        "     \n",
        "\n",
        "# Filtering Extremes\n",
        "id2word.filter_extremes(no_below=2, no_above=.99)\n",
        "print(len(id2word))\n",
        "     \n",
        "\n",
        "# Creating a corpus object \n",
        "corpus = [id2word.doc2bow(d) for d in df0['lemma_tokens']]\n",
        "     \n",
        "\n",
        "# Instantiating a Base LDA model \n",
        "base_model = LdaMulticore(corpus=corpus, num_topics=5, id2word=id2word, workers=12, passes=5)\n",
        "     \n",
        "\n",
        "# Filtering for words \n",
        "words = [re.findall(r'\"([^\"]*)\"',t[1]) for t in base_model.print_topics()]\n",
        "     \n",
        "\n",
        "# Create Topics\n",
        "topics = [' '.join(t[0:10]) for t in words]\n",
        "     \n",
        "\n",
        "# Getting the topics\n",
        "for id, t in enumerate(topics): \n",
        "    print(f\"------ Topic {id} ------\")\n",
        "    print(t, end=\"\\n\\n\")\n",
        "     "
      ],
      "metadata": {
        "id": "mv6gGB2KzN3z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d049b8b-7a28-4869-d58a-1feac1f6e6da"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "WARNING:gensim.models.ldamulticore:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10730\n",
            "3541\n",
            "------ Topic 0 ------\n",
            "day go great lol watch time thank happy today to\n",
            "\n",
            "------ Topic 1 ------\n",
            "good love today hope tomorrow go know day right thank\n",
            "\n",
            "------ Topic 2 ------\n",
            "love go work day guy time haha home yeah know\n",
            "\n",
            "------ Topic 3 ------\n",
            "thank good morning not hey know think come new lol\n",
            "\n",
            "------ Topic 4 ------\n",
            "night day good u look work twitter try today need\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute Perplexity\n",
        "# a measure of how good the model is. lower the better\n",
        "base_perplexity = base_model.log_perplexity(corpus)\n",
        "print('\\nPerplexity: ', base_perplexity) \n",
        "\n",
        "# Compute Coherence Score\n",
        "coherence_model = CoherenceModel(model=base_model, texts=df['lemma_tokens'], \n",
        "                                   dictionary=id2word, coherence='c_v')\n",
        "coherence_lda_model_base = coherence_model.get_coherence()\n",
        "print('\\nCoherence Score: ', coherence_lda_model_base)"
      ],
      "metadata": {
        "id": "VMhdAqZg7eIb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2a531bf-7ef6-4032-f593-cde0f0b7ec68"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Perplexity:  -7.61440212346485\n",
            "\n",
            "Coherence Score:  0.24383095782954137\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating Topic Distance Visualization \n",
        "pyLDAvis.enable_notebook()\n",
        "pyLDAvis.gensim.prepare(base_model, corpus, id2word)"
      ],
      "metadata": {
        "id": "GO2EJ9uw7nHQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 951
        },
        "outputId": "cfb646dd-f748-44c2-b376-6dbb6a258805"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "/usr/local/lib/python3.9/dist-packages/pyLDAvis/_prepare.py:243: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
            "  default_term_info = default_term_info.sort_values(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
              "topic                                                \n",
              "0      0.012713  0.007482       1        1  21.436638\n",
              "3      0.060186 -0.088405       2        1  20.751528\n",
              "1      0.047445  0.000410       3        1  20.609124\n",
              "2      0.009891  0.111984       4        1  20.372365\n",
              "4     -0.130236 -0.031470       5        1  16.830345, topic_info=      Term        Freq       Total Category  logprob  loglift\n",
              "45    love  499.000000  499.000000  Default  30.0000  30.0000\n",
              "25   night  228.000000  228.000000  Default  29.0000  29.0000\n",
              "0     good  645.000000  645.000000  Default  28.0000  28.0000\n",
              "143  great  233.000000  233.000000  Default  27.0000  27.0000\n",
              "239     go  435.000000  435.000000  Default  26.0000  26.0000\n",
              "..     ...         ...         ...      ...      ...      ...\n",
              "203   find   24.676790   86.421646   Topic5  -5.6007   0.5286\n",
              "45    love   31.492262  499.091292   Topic5  -5.3568  -0.9811\n",
              "64     new   27.648521  246.072576   Topic5  -5.4870  -0.4041\n",
              "117    fun   25.901198  155.093117   Topic5  -5.5523  -0.0077\n",
              "76   thank   27.054034  456.996996   Topic5  -5.5087  -1.0449\n",
              "\n",
              "[414 rows x 6 columns], token_table=      Topic      Freq    Term\n",
              "term                         \n",
              "371       2  0.901608  accord\n",
              "9         1  0.645092     add\n",
              "9         2  0.200201     add\n",
              "9         3  0.022245     add\n",
              "9         4  0.088978     add\n",
              "...     ...       ...     ...\n",
              "3148      2  0.049775       °\n",
              "3148      3  0.746624       °\n",
              "3148      5  0.199100       °\n",
              "3333      3  0.822892      ðº\n",
              "3149      3  0.731557    ð½ðµ\n",
              "\n",
              "[938 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[1, 4, 2, 3, 5])"
            ],
            "text/html": [
              "\n",
              "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
              "\n",
              "\n",
              "<div id=\"ldavis_el1471401863204894889789152218\" style=\"background-color:white;\"></div>\n",
              "<script type=\"text/javascript\">\n",
              "\n",
              "var ldavis_el1471401863204894889789152218_data = {\"mdsDat\": {\"x\": [0.012713204027603672, 0.06018629463023994, 0.04744470275522375, 0.009891449004314573, -0.13023565041738192], \"y\": [0.007481879613060944, -0.08840538326352249, 0.00040963688628690856, 0.11198415261623866, -0.03147028585206403], \"topics\": [1, 2, 3, 4, 5], \"cluster\": [1, 1, 1, 1, 1], \"Freq\": [21.436638155045102, 20.75152763472074, 20.609124000124414, 20.3723647169611, 16.830345493148638]}, \"tinfo\": {\"Term\": [\"love\", \"night\", \"good\", \"great\", \"go\", \"morning\", \"tomorrow\", \"day\", \"to\", \"watch\", \"look\", \"hey\", \"thank\", \"yeah\", \"welcome\", \"not\", \"bad\", \"u\", \"luck\", \"try\", \"work\", \"guy\", \"twitter\", \"home\", \"let\", \"right\", \"tweet\", \"enjoy\", \"feel\", \"lol\", \"wwwtweeteraddercom\", \"sway\", \"dance\", \"nom\", \"star\", \"writer\", \"attention\", \"surprised\", \"feedback\", \"uncle\", \"normal\", \"wwwtweeterfollowcom\", \"ignore\", \"sugar\", \"vip\", \"owner\", \"lb\", \"ebay\", \"dc\", \"artist\", \"kevin\", \"contact\", \"train\", \"musical\", \"ridiculous\", \"available\", \"spray\", \"jumper\", \"poker\", \"skip\", \"mother\", \"na\", \"appreciate\", \"follower\", \"minute\", \"boy\", \"la\", \"dad\", \"great\", \"pay\", \"vacation\", \"r\", \"add\", \"to\", \"watch\", \"mom\", \"day\", \"hear\", \"follow\", \"go\", \"lol\", \"online\", \"happy\", \"sleep\", \"time\", \"ask\", \"school\", \"twitter\", \"weekend\", \"thank\", \"see\", \"today\", \"long\", \"look\", \"movie\", \"new\", \"week\", \"cool\", \"get\", \"amp\", \"u\", \"know\", \"haha\", \"good\", \"love\", \"work\", \"welcome\", \"drag\", \"muscle\", \"poke\", \"accord\", \"pass\", \"football\", \"microsoft\", \"toast\", \"tweep\", \"cheer\", \"ily\", \"hmmm\", \"woop\", \"gooood\", \"pressure\", \"bad\", \"hospital\", \"susan\", \"talent\", \"awsome\", \"normally\", \"atlanta\", \"shoulder\", \"purpose\", \"advertise\", \"nah\", \"pig\", \"uni\", \"window\", \"shout\", \"lmao\", \"ampamp\", \"tweeps\", \"paint\", \"moon\", \"tip\", \"morning\", \"hey\", \"nite\", \"sweet\", \"not\", \"ok\", \"sun\", \"okay\", \"thank\", \"sorry\", \"come\", \"feel\", \"x\", \"rain\", \"cool\", \"good\", \"birthday\", \"think\", \"need\", \"know\", \"sit\", \"s\", \"new\", \"beautiful\", \"well\", \"head\", \"watch\", \"lol\", \"happy\", \"u\", \"time\", \"friend\", \"haha\", \"twitter\", \"wait\", \"day\", \"oh\", \"night\", \"topic\", \"suck\", \"response\", \"luck\", \"cheese\", \"zomg\", \"cell\", \"lazy\", \"nail\", \"bb\", \"dave\", \"sky\", \"hoo\", \"\\u00f0\\u00bd\\u00f0\\u00b5\", \"goodnight\", \"woo\", \"gnight\", \"skinny\", \"gray\", \"careful\", \"devil\", \"wicked\", \"vote\", \"\\u00f0\\u00ba\", \"stream\", \"weapon\", \"quothow\", \"avoid\", \"jeff\", \"meow\", \"test\", \"forum\", \"sarah\", \"message\", \"band\", \"involve\", \"tomorrow\", \"july\", \"excited\", \"\\u00b0\", \"stay\", \"plan\", \"ago\", \"begin\", \"right\", \"album\", \"tweet\", \"good\", \"let\", \"hope\", \"song\", \"june\", \"wish\", \"yes\", \"today\", \"listen\", \"love\", \"rock\", \"bed\", \"make\", \"s\", \"use\", \"know\", \"go\", \"new\", \"fun\", \"way\", \"time\", \"thank\", \"day\", \"ready\", \"lol\", \"wait\", \"haha\", \"think\", \"happy\", \"oh\", \"awww\", \"bank\", \"classic\", \"respect\", \"recover\", \"church\", \"taylor\", \"kool\", \"grand\", \"natural\", \"compliment\", \"town\", \"cutie\", \"connect\", \"nearly\", \"jealous\", \"holiday\", \"afraid\", \"hee\", \"gorgeous\", \"beat\", \"spammer\", \"wwwmeasia\", \"shareholder\", \"dividend\", \"edit\", \"chile\", \"bein\", \"hater\", \"calm\", \"agree\", \"hand\", \"stage\", \"chance\", \"hug\", \"answer\", \"laker\", \"ha\", \"having\", \"love\", \"yeah\", \"guy\", \"play\", \"tea\", \"home\", \"pack\", \"support\", \"thing\", \"lot\", \"work\", \"write\", \"friend\", \"say\", \"glad\", \"go\", \"wait\", \"haha\", \"ur\", \"to\", \"amp\", \"pretty\", \"game\", \"lt\", \"life\", \"time\", \"think\", \"get\", \"know\", \"well\", \"nice\", \"day\", \"awesome\", \"thank\", \"way\", \"fun\", \"not\", \"good\", \"come\", \"\\u00a9\", \"bowie\", \"limit\", \"lead\", \"steal\", \"nighty\", \"patron\", \"el\", \"booty\", \"brilliant\", \"corner\", \"forgive\", \"james\", \"sms\", \"heh\", \"loveee\", \"attack\", \"bob\", \"swing\", \"halfway\", \"wwwiamsoannoyedcom\", \"click\", \"hollywood\", \"senior\", \"obviously\", \"mouse\", \"dye\", \"roast\", \"vine\", \"ah\", \"email\", \"night\", \"blog\", \"museum\", \"site\", \"hat\", \"hehe\", \"glass\", \"try\", \"look\", \"big\", \"update\", \"enjoy\", \"count\", \"u\", \"check\", \"hair\", \"free\", \"proud\", \"luv\", \"lt\", \"world\", \"excite\", \"garden\", \"tonight\", \"twitter\", \"need\", \"post\", \"day\", \"work\", \"read\", \"nice\", \"good\", \"today\", \"oh\", \"awesome\", \"help\", \"feel\", \"great\", \"morning\", \"amp\", \"lol\", \"sure\", \"find\", \"love\", \"new\", \"fun\", \"thank\"], \"Freq\": [499.0, 228.0, 645.0, 233.0, 435.0, 183.0, 132.0, 525.0, 129.0, 228.0, 183.0, 157.0, 456.0, 104.0, 48.0, 184.0, 62.0, 221.0, 48.0, 112.0, 240.0, 116.0, 189.0, 131.0, 94.0, 116.0, 112.0, 96.0, 147.0, 333.0, 11.690493699939571, 7.497112992703151, 17.29120256288045, 7.3272459882137255, 17.017789511965784, 5.6678571001997895, 4.753578433105426, 5.519365188009717, 5.520218211380459, 5.430953746601068, 4.642000698842959, 6.146870566996822, 3.837718727047357, 4.5843337445631995, 18.234180045270907, 3.7919861786730573, 4.5473457996696975, 3.7841621770250473, 3.7538840555463255, 4.445380049587703, 3.693552826588883, 8.124727289358022, 24.303890365939452, 3.6705699609891913, 3.669433751682254, 4.389058228504871, 2.924743853944201, 2.9243373930113052, 2.9241183199750003, 2.923982420430445, 36.17853611757645, 7.997432240556411, 20.82511154580065, 36.707372981333066, 16.388885036028178, 27.299805479725645, 21.39222405833196, 13.899593522737744, 148.69270488108333, 29.693611844166245, 13.460855334079557, 19.767816059642755, 29.139341714825324, 73.25143283129445, 115.68715033962194, 30.511745915440475, 232.5517996838711, 41.84649272891816, 69.70820247633459, 182.96789881346072, 144.9098318931804, 14.140306519306785, 84.51398703425467, 44.15972180274443, 111.84543804726982, 22.54627803989195, 33.20226777073427, 66.90363194096095, 39.5324636053141, 105.60598449218887, 35.668690534436, 74.68608050855683, 35.985803333980186, 55.30862537019244, 39.416175181920686, 57.53599959732075, 34.81362347218247, 35.57181321975998, 38.941325835594526, 44.11568341918422, 44.35132976388084, 46.14066089510345, 40.652505569712176, 46.77707276883208, 39.76612566813612, 36.862249944255844, 44.22125615753441, 7.258096778723267, 4.810900374933904, 4.810434057246469, 4.808311904399642, 20.738791413949638, 5.573072432535284, 4.661576633076593, 4.662716307753592, 4.62028475300052, 13.820869314520179, 5.338042087718709, 9.058691230172103, 6.768959238577943, 3.7356503859625407, 3.7103007717792527, 50.380863264879935, 2.95978473933317, 2.959656298284627, 2.9596797819091742, 2.959547267170658, 2.959506530270934, 2.9593965406416776, 2.959029429286513, 2.9586407513373767, 2.9580884069028763, 6.591749411457436, 6.583636058805241, 7.262195869350843, 3.6033510737577554, 6.494374322644136, 24.486112682054436, 7.101363234434887, 5.7144903403001726, 12.980146929097026, 10.29948831150166, 8.323085382282843, 112.25719270625652, 96.75583809469295, 21.22240071717352, 33.11053034937257, 97.56004775726419, 50.981568629702586, 36.93112627715599, 22.60963688860225, 194.4107021842101, 27.206674291074087, 82.66058706947994, 66.56479961713602, 33.50034318060954, 24.545153389711665, 45.064768780959874, 188.75717964655294, 33.7657368934502, 85.03377890419915, 55.39504843756155, 94.3386772466993, 22.60080896280312, 41.92256396503143, 74.37943673103118, 29.12197953882841, 48.279774750014845, 27.78957619294419, 60.607427733298245, 73.5622449374042, 54.424187473020766, 52.68371541373126, 58.5697200833128, 40.74306872892263, 47.21611800344744, 44.345725118725674, 41.444663578860705, 47.32332985540011, 38.494246169804775, 35.971196220955406, 7.450799791691107, 9.902322913939171, 7.377710970978676, 41.99822868536797, 5.409459462639509, 3.8169924805690854, 3.8061289681246633, 10.637269231258948, 8.352153377403535, 8.300813476145864, 4.4994418164194645, 8.247712490238507, 4.446074763939359, 4.437785766865244, 33.9028671522047, 13.950003680520807, 3.671143019648534, 3.6704531028378002, 3.6702236858645176, 3.669610638018898, 3.667172725701148, 3.6660532470265386, 27.034661768820484, 2.9081588067993347, 2.9075321938422243, 2.9073249094192684, 2.907238283093257, 2.9068905878671507, 2.9069222397939627, 3.632381546509015, 15.166461003068623, 3.6315083817009493, 5.7403972543508415, 11.78277007342692, 20.172961927325957, 3.6027084600839197, 90.75183339239675, 12.948408204610782, 29.994527210856567, 14.828579302180472, 27.734394960585437, 23.50080838675739, 14.548743903283517, 7.589300531949826, 68.90176485086131, 22.26156262884268, 59.51061436733908, 260.1627620705415, 50.4150557546714, 91.68100069232239, 48.48504786539098, 21.957134872734965, 34.22676203322377, 47.682031342641544, 100.2637999945131, 38.04392157885017, 131.0349788630069, 21.687449033046075, 41.65615560301357, 35.68069067349836, 41.31934483197996, 25.98008429920916, 70.88454435562271, 86.95736423255946, 60.19679767861561, 46.6204647468151, 38.86271742006076, 58.38867207809562, 68.70226156624405, 69.85160823651186, 32.48052761085718, 52.66830359186874, 39.50240048108448, 42.56871349484858, 40.33809511943084, 38.83004311221443, 35.82265503889107, 19.548642451196287, 8.116345843553006, 8.92665559159828, 6.379052486219482, 4.764351420395231, 13.449604785355499, 5.442892228237867, 4.623920559435579, 3.8515065970770346, 3.85085848214804, 4.618654243355553, 14.599214343625583, 4.601208542119845, 4.589315691985402, 9.756483597149474, 11.966306610068, 11.904538316704473, 5.201592570786441, 7.380705522339883, 6.642719283859271, 12.553989468129727, 2.9344220905463736, 2.9343301076290462, 2.9343294018777626, 2.934278352534898, 2.9341607273209243, 2.933970409724715, 2.933953706944331, 2.933713516257397, 2.9336172988323663, 16.267241505956047, 19.57014434031066, 5.769568970999888, 9.892779226086672, 17.577689899810206, 10.992594737775741, 14.883235949293075, 29.056570797195587, 6.998735518993154, 291.5362721935222, 66.6976367305608, 70.60602335235905, 52.1494830218203, 15.73001798268557, 69.70836794342212, 14.471938216095882, 12.991904375746577, 51.9844990736958, 33.195005989478254, 99.11263887312366, 23.244287405993326, 59.61046351818946, 34.34563653460563, 38.146410200643714, 121.1265608949429, 59.88615443570249, 69.99051036867117, 30.550185174249428, 47.05890089347444, 62.28425589412085, 30.892579939104312, 24.49101304592857, 33.314968651703936, 32.729654294950784, 70.43215634807527, 58.433718293556524, 41.32068751101392, 63.1665240430666, 40.33562433507235, 39.64217382560728, 73.42034115992212, 38.04468791559935, 61.22401420942105, 35.905640464429595, 38.64155400936642, 39.675669722540114, 52.71940635457544, 35.02776867548131, 18.91861671413835, 4.813069489202135, 5.58706462530234, 4.674545002799346, 3.887062444098917, 3.886781415638732, 3.8861664958268194, 3.885544968153464, 3.876043252096446, 4.611436427740994, 4.5976489304917205, 5.286129399474226, 6.014433117186111, 3.7405871455895086, 3.739750668070396, 3.737527317043759, 3.717683131735559, 3.71083427769896, 4.4434976291721675, 2.961803447780797, 2.9612207898806346, 2.960915856509936, 2.960591488252641, 2.960536681872439, 3.6998551215628774, 2.959563577100553, 3.6964015422146836, 2.955844711550317, 2.9512005510492223, 17.552742212883434, 29.582403722815183, 141.4993456243391, 24.343866862336146, 7.4438111103950195, 21.163666402962683, 5.5399031520918625, 15.236671229271858, 10.443580414793171, 54.54947898749956, 79.96466987420719, 34.45269923560065, 20.37230891657265, 43.07681044388638, 7.524075637247546, 82.06775471300342, 35.408538054234, 20.72601607867391, 22.77622714239123, 11.481771844494972, 13.369370434664097, 32.471550430101644, 26.999342486304904, 17.51957385802242, 13.138625467872359, 34.199056862816406, 56.006535757501446, 43.817655882616705, 22.750720797222012, 102.60378777863758, 59.2366636631458, 24.47069426789198, 37.31594001644947, 96.62966533713669, 53.04217137471383, 37.35502435079045, 32.69831668690935, 22.777819248301217, 31.385379707161388, 36.77020529123851, 33.6456912237322, 33.6856120355923, 33.065073209266956, 24.648890410908695, 24.67678957931567, 31.49226225469645, 27.648520561518815, 25.901198318430286, 27.05403381197832], \"Total\": [499.0, 228.0, 645.0, 233.0, 435.0, 183.0, 132.0, 525.0, 129.0, 228.0, 183.0, 157.0, 456.0, 104.0, 48.0, 184.0, 62.0, 221.0, 48.0, 112.0, 240.0, 116.0, 189.0, 131.0, 94.0, 116.0, 112.0, 96.0, 147.0, 333.0, 12.813237103287808, 8.235371900547632, 19.214850470375083, 8.234149993594224, 19.220505065157, 6.405985834285116, 5.49123528606162, 6.40444619784014, 6.407254555279775, 6.4061779254618845, 5.491406311015462, 7.32560389430915, 4.5764059505508365, 5.489716238506918, 21.98648035121507, 4.576855130000218, 5.493265357031143, 4.5769927690138985, 4.577172429902544, 5.493084512086492, 4.578051695787956, 10.07814573563584, 30.223960891484722, 4.578365601069079, 4.578308721817701, 5.489844624317857, 3.66175792301736, 3.6617564824065236, 3.6617035522826464, 3.6616058149329755, 45.76114034893651, 10.072007097750125, 26.538083558524153, 47.669324944853315, 21.041817802852496, 35.685607025815905, 28.41515556779899, 18.288215123471186, 233.98941908329593, 42.17074210720467, 18.301787138076968, 28.39868512990345, 44.95482443516512, 129.19092433884316, 228.47330782674788, 48.577232992146804, 525.7508667143427, 70.40292185944948, 128.52461289912605, 435.7881063632518, 333.9966795213368, 20.1527133240831, 199.01223916115322, 87.14242692504456, 314.481807730635, 37.645763232881365, 68.6521243850127, 189.58438248490228, 88.71238976958088, 456.9969962640424, 79.90735809709987, 280.35222406149217, 82.25790193165304, 183.00593664023648, 100.85897823317195, 246.07257581467138, 92.87668764786437, 100.32383897531402, 126.59286596525781, 200.24181531380765, 221.17218881419512, 288.2646895154735, 209.1766077884312, 645.0460861776386, 499.09129219123224, 240.839025771207, 48.98834993023299, 8.319921065998578, 5.545846359613213, 5.54590512431726, 5.545645674072854, 24.022447121083427, 6.468015993224498, 5.542711553788951, 5.544321911564568, 5.541893866337756, 16.621627863461768, 6.467407314916306, 11.081900531665399, 8.316585826438915, 4.618449387355923, 4.61842937146992, 62.810194158058096, 3.694550437535278, 3.6945306954858017, 3.694564057712743, 3.694540730044657, 3.694545793405176, 3.694529148981772, 3.6944832946934247, 3.694457043528333, 3.6943785917133094, 8.310912543016249, 8.315203804848245, 9.235001215750136, 4.614483172701114, 8.318404946882975, 33.24186388359837, 9.227246579782223, 7.381387852330299, 17.51770765531544, 13.825032173481691, 11.078136804980353, 183.62940161175814, 157.51566787792137, 31.425633829593565, 52.58194075155904, 184.4151654489703, 88.39767599522443, 62.79826891959411, 35.97792950823905, 456.9969962640424, 45.12360038822722, 178.4438945601513, 147.32972809236767, 67.23905934683206, 45.01493183254501, 100.32383897531402, 645.0460861776386, 70.05503986853878, 236.22041521309563, 140.22072349740716, 288.2646895154735, 42.317460518928435, 103.7712904875745, 246.07257581467138, 62.57754929027095, 134.47385417193362, 58.776048437848004, 228.47330782674788, 333.9966795213368, 199.01223916115322, 221.17218881419512, 314.481807730635, 141.66332686441365, 209.1766077884312, 189.58438248490228, 163.38501747555273, 525.7508667143427, 155.34020310734584, 228.98897691139695, 8.19205157755186, 10.923656111509128, 8.193212540880962, 48.27778034401319, 6.375163897029906, 4.554943426174589, 4.554883129650185, 12.761908633289556, 10.024130160169143, 10.02030951430422, 5.464779033196193, 10.023063975794468, 5.46874160581471, 5.467786575485627, 41.90185743435461, 17.312105980424157, 4.557160056066067, 4.557281256679228, 4.557043235129058, 4.557004539534949, 4.554641046467776, 4.555176988390491, 33.74764141044448, 3.6456800611480467, 3.6455904101846293, 3.6455573228246747, 3.645570248310252, 3.6454790523088945, 3.645552872080787, 4.55562929068277, 19.130018728014186, 4.557666388152654, 7.289546782507101, 15.508507731047677, 27.391666223017086, 4.555725768548012, 132.39848754898495, 17.31750197384523, 42.022213059115785, 20.090428539877337, 39.22419326165041, 33.75092676873434, 20.064942525897536, 10.02937173019378, 116.0754753092917, 33.73463667151644, 112.46991817792653, 645.0460861776386, 94.02476215006597, 190.36498140417575, 94.06973020960561, 36.51147486049653, 63.958533098323294, 101.6158502658313, 280.35222406149217, 83.31244018842887, 499.09129219123224, 38.36341765922002, 102.64321474609298, 81.32763081757884, 103.7712904875745, 51.16738934315567, 288.2646895154735, 435.7881063632518, 246.07257581467138, 155.09311679366962, 118.10101867551724, 314.481807730635, 456.9969962640424, 525.7508667143427, 87.94871018841185, 333.9966795213368, 163.38501747555273, 209.1766077884312, 236.22041521309563, 199.01223916115322, 155.34020310734584, 22.02029411116229, 9.173639630124976, 10.093796683688542, 7.338745257797046, 5.505303837085075, 15.605588025394656, 6.4221913690817605, 5.50691463805359, 4.5882372553775355, 4.5881635004243, 5.506357229619563, 17.44048484211223, 5.503935785560826, 5.50465098759924, 11.93845254660317, 14.68872508546761, 14.672511305964488, 6.41825147261228, 9.169324918818127, 8.253952077382067, 15.602027142296524, 3.670618901098468, 3.670582115528187, 3.6706038801967917, 3.6705808197610033, 3.670571129848549, 3.6705622411145797, 3.67057240537448, 3.670533880020724, 3.670546907401179, 21.12991214246101, 25.673253154477173, 7.342393880609646, 12.86500378842726, 23.899599496005536, 14.681884666730426, 20.200448789477388, 41.3660267587145, 9.18157336189205, 499.09129219123224, 104.86698957193838, 116.48814428249985, 85.35915062559046, 22.97956298450355, 131.4190372098462, 21.081034512090856, 19.30782201592284, 106.60075862962692, 61.43801838703206, 240.839025771207, 40.40851181840401, 141.66332686441365, 71.71207680731244, 82.70870166012739, 435.7881063632518, 163.38501747555273, 209.1766077884312, 65.28414058883232, 129.19092433884316, 200.24181531380765, 68.79976161739683, 47.769226961131324, 80.05324807335431, 81.79462794022741, 314.481807730635, 236.22041521309563, 126.59286596525781, 288.2646895154735, 134.47385417193362, 133.1365661059651, 525.7508667143427, 127.56256454686039, 456.9969962640424, 118.10101867551724, 155.09311679366962, 184.4151654489703, 645.0460861776386, 178.4438945601513, 20.358021781451463, 5.548411899976417, 6.472584088124652, 5.545019270156288, 4.6222836012657105, 4.622258160876451, 4.622225316104053, 4.6220264121550025, 4.622079052087614, 5.5454075446416065, 5.545290079923104, 6.468057835267995, 7.396525187070257, 4.62033972843695, 4.620196925934806, 4.61905543487827, 4.618888402374581, 4.621287723712231, 5.544826906847059, 3.696222948316751, 3.6961778486793238, 3.696126606419762, 3.696058458056932, 3.6960143069825424, 4.619596166146639, 3.6960127082289205, 4.618499673153415, 3.695721702642811, 3.6957874483517608, 22.162410141604944, 40.62719005046931, 228.98897691139695, 35.08119777109256, 10.141664519191657, 32.27135182359662, 7.379612002916635, 23.963676603009212, 15.694520305388608, 112.32517005200229, 183.00593664023648, 68.1492432568548, 36.93518350976049, 96.64214281041642, 11.060185945920338, 221.17218881419512, 80.20011566162043, 40.50630221174286, 47.962977367299914, 19.377494250962826, 23.982811231515996, 80.05324807335431, 63.454897983040205, 35.98024618823373, 23.999572500490018, 92.76149455016993, 189.58438248490228, 140.22072349740716, 55.299015159507945, 525.7508667143427, 240.839025771207, 63.467164825207234, 133.1365661059651, 645.0460861776386, 280.35222406149217, 155.34020310734584, 127.56256454686039, 64.35228305262804, 147.32972809236767, 233.98941908329593, 183.62940161175814, 200.24181531380765, 333.9966795213368, 81.71678906406326, 86.42164595075953, 499.09129219123224, 246.07257581467138, 155.09311679366962, 456.9969962640424], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -6.5897, -7.0339, -6.1983, -7.0569, -6.2142, -7.3136, -7.4896, -7.3402, -7.34, -7.3563, -7.5133, -7.2325, -7.7036, -7.5258, -6.1452, -7.7156, -7.5339, -7.7176, -7.7257, -7.5566, -7.7419, -6.9535, -5.8578, -7.7481, -7.7484, -7.5693, -7.9753, -7.9754, -7.9755, -7.9755, -5.46, -6.9693, -6.0123, -5.4455, -6.2519, -5.7416, -5.9854, -6.4166, -4.0466, -5.6575, -6.4487, -6.0644, -5.6764, -4.7546, -4.2976, -5.6303, -3.5993, -5.3145, -4.8041, -3.8391, -4.0723, -6.3994, -4.6115, -5.2606, -4.3313, -5.9329, -5.5458, -4.8452, -5.3713, -4.3887, -5.4742, -4.7352, -5.4653, -5.0355, -5.3743, -4.996, -5.4984, -5.4769, -5.3864, -5.2616, -5.2563, -5.2168, -5.3434, -5.2031, -5.3654, -5.4413, -5.2268, -7.0339, -7.4451, -7.4452, -7.4456, -5.984, -7.298, -7.4766, -7.4764, -7.4855, -6.3898, -7.3411, -6.8123, -7.1036, -7.6981, -7.7049, -5.0964, -7.9309, -7.9309, -7.9309, -7.9309, -7.931, -7.931, -7.9311, -7.9312, -7.9314, -7.1302, -7.1314, -7.0333, -7.7341, -7.145, -5.8179, -7.0557, -7.273, -6.4526, -6.6839, -6.8969, -4.2952, -4.4438, -5.9609, -5.5161, -4.4355, -5.0845, -5.4069, -5.8976, -3.746, -5.7125, -4.6012, -4.8178, -5.5044, -5.8155, -5.2079, -3.7755, -5.4965, -4.5729, -5.0015, -4.4691, -5.898, -5.2802, -4.7068, -5.6445, -5.139, -5.6913, -4.9116, -4.7178, -5.0192, -5.0517, -4.9458, -5.3087, -5.1612, -5.224, -5.2916, -5.159, -5.3655, -5.4333, -7.0008, -6.7163, -7.0106, -5.2715, -7.3209, -7.6696, -7.6725, -6.6447, -6.8866, -6.8927, -7.5051, -6.8992, -7.5171, -7.5189, -5.4856, -6.3736, -7.7086, -7.7088, -7.7088, -7.709, -7.7097, -7.71, -5.712, -7.9416, -7.9418, -7.9419, -7.9419, -7.942, -7.942, -7.7192, -6.29, -7.7194, -7.2616, -6.5425, -6.0047, -7.7274, -4.501, -6.4481, -5.6081, -6.3125, -5.6864, -5.8521, -6.3316, -6.9824, -4.7764, -5.9062, -4.9229, -3.4478, -5.0888, -4.4908, -5.1278, -5.92, -5.4761, -5.1445, -4.4013, -5.3703, -4.1336, -5.9324, -5.2796, -5.4345, -5.2878, -5.7518, -4.748, -4.5437, -4.9115, -5.1671, -5.3491, -4.942, -4.7793, -4.7627, -5.5284, -5.0451, -5.3327, -5.258, -5.3118, -5.3499, -5.4305, -6.0246, -6.9037, -6.8085, -7.1445, -7.4364, -6.3986, -7.3032, -7.4663, -7.6491, -7.6492, -7.4674, -6.3166, -7.4712, -7.4738, -6.7196, -6.5154, -6.5206, -7.3486, -6.9987, -7.104, -6.4675, -7.921, -7.9211, -7.9211, -7.9211, -7.9211, -7.9212, -7.9212, -7.9213, -7.9213, -6.2084, -6.0235, -7.2449, -6.7057, -6.1309, -6.6003, -6.2973, -5.6283, -7.0518, -3.3224, -4.7974, -4.7404, -5.0434, -6.242, -4.7532, -6.3253, -6.4332, -5.0466, -5.4951, -4.4013, -5.8515, -4.9097, -5.4611, -5.3561, -4.2007, -4.9051, -4.7492, -5.5782, -5.1461, -4.8658, -5.567, -5.7992, -5.4915, -5.5093, -4.7429, -4.9296, -5.2762, -4.8518, -5.3003, -5.3176, -4.7013, -5.3588, -4.883, -5.4166, -5.3432, -5.3168, -5.0326, -5.4414, -5.8664, -7.2352, -7.0861, -7.2644, -7.4489, -7.449, -7.4491, -7.4493, -7.4517, -7.278, -7.281, -7.1415, -7.0124, -7.4873, -7.4875, -7.4881, -7.4934, -7.4953, -7.3151, -7.7207, -7.7209, -7.721, -7.7212, -7.7212, -7.4982, -7.7215, -7.4992, -7.7228, -7.7243, -5.9413, -5.4194, -3.8542, -5.6143, -6.7992, -5.7543, -7.0946, -6.0828, -6.4606, -4.8074, -4.425, -5.267, -5.7924, -5.0436, -6.7884, -4.399, -5.2396, -5.7752, -5.6808, -6.3658, -6.2136, -5.3262, -5.5107, -5.9432, -6.231, -5.2743, -4.7811, -5.0265, -5.6819, -4.1757, -4.725, -5.6091, -5.1871, -4.2357, -4.8355, -5.1861, -5.3192, -5.6808, -5.3602, -5.2019, -5.2907, -5.2895, -5.3081, -5.6018, -5.6007, -5.3568, -5.487, -5.5523, -5.5087], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 1.4484, 1.4461, 1.4346, 1.4234, 1.4184, 1.4176, 1.3958, 1.3913, 1.3911, 1.3749, 1.372, 1.3646, 1.364, 1.3598, 1.3529, 1.3519, 1.3511, 1.3499, 1.3418, 1.3284, 1.3254, 1.3246, 1.3221, 1.3191, 1.3188, 1.3163, 1.3153, 1.3152, 1.3151, 1.3151, 1.3051, 1.3094, 1.2976, 1.2788, 1.2902, 1.2722, 1.2562, 1.2657, 1.0867, 1.1893, 1.2329, 1.1778, 1.1065, 0.9727, 0.8595, 1.075, 0.7244, 1.0198, 0.9283, 0.6722, 0.705, 1.1858, 0.6836, 0.8603, 0.5063, 1.0274, 0.8136, 0.4985, 0.7318, 0.0751, 0.7335, 0.2173, 0.7133, 0.3435, 0.6005, 0.0869, 0.5588, 0.5032, 0.3611, 0.0274, -0.0667, -0.2921, -0.098, -1.0839, -0.9897, -0.3369, 1.4702, 1.436, 1.4304, 1.4303, 1.4299, 1.4256, 1.4236, 1.3994, 1.3994, 1.3907, 1.388, 1.3806, 1.371, 1.3666, 1.3604, 1.3536, 1.352, 1.3508, 1.3508, 1.3508, 1.3507, 1.3507, 1.3507, 1.3506, 1.3504, 1.3503, 1.3408, 1.3391, 1.3322, 1.3252, 1.325, 1.2668, 1.3107, 1.3166, 1.2728, 1.2782, 1.2866, 1.0804, 1.0852, 1.18, 1.11, 0.9358, 1.0222, 1.0417, 1.108, 0.7178, 1.0666, 0.803, 0.7781, 0.8759, 0.9661, 0.7722, 0.3437, 0.8427, 0.5508, 0.6438, 0.4556, 0.9453, 0.6662, 0.3761, 0.8076, 0.5482, 0.8235, 0.2455, 0.0596, 0.276, 0.1379, -0.1082, 0.3264, 0.0841, 0.1197, 0.2008, -0.8353, 0.1774, -0.2784, 1.4846, 1.4813, 1.4746, 1.4401, 1.4152, 1.4027, 1.3998, 1.3973, 1.397, 1.3912, 1.3851, 1.3845, 1.3724, 1.3707, 1.3676, 1.3635, 1.3632, 1.363, 1.363, 1.3629, 1.3627, 1.3623, 1.3576, 1.3534, 1.3532, 1.3532, 1.3531, 1.353, 1.353, 1.353, 1.3473, 1.3523, 1.3405, 1.3047, 1.2735, 1.3447, 1.2017, 1.2887, 1.2423, 1.2757, 1.2328, 1.2175, 1.258, 1.3007, 1.0579, 1.1638, 0.9429, 0.6714, 0.9562, 0.8488, 0.9167, 1.0709, 0.9542, 0.8228, 0.5512, 0.7956, 0.2421, 1.0091, 0.6776, 0.7556, 0.6586, 0.9017, 0.1766, -0.0323, 0.1714, 0.3775, 0.4679, -0.1044, -0.3155, -0.439, 0.5833, -0.2677, 0.1597, -0.0126, -0.188, -0.0547, 0.1124, 1.4719, 1.4685, 1.4681, 1.4508, 1.4464, 1.4423, 1.4255, 1.4162, 1.416, 1.4158, 1.4152, 1.4132, 1.4118, 1.4091, 1.3892, 1.386, 1.3819, 1.3808, 1.374, 1.3738, 1.3736, 1.3671, 1.3671, 1.3671, 1.3671, 1.3671, 1.367, 1.367, 1.3669, 1.3669, 1.3295, 1.3195, 1.3499, 1.3283, 1.2838, 1.3016, 1.2855, 1.2378, 1.3195, 1.0534, 1.1385, 1.0903, 1.0982, 1.212, 0.9569, 1.2148, 1.1948, 0.8728, 0.9754, 0.7031, 1.038, 0.7254, 0.8548, 0.8171, 0.3107, 0.5873, 0.4962, 0.8316, 0.5811, 0.4232, 0.7903, 0.9229, 0.7143, 0.6751, 0.0947, 0.1941, 0.4714, 0.0729, 0.3869, 0.3795, -0.3776, 0.3811, -0.4191, 0.4003, 0.2013, 0.0545, -0.9133, -0.0371, 1.7087, 1.6398, 1.6349, 1.6112, 1.6088, 1.6087, 1.6085, 1.6084, 1.606, 1.5976, 1.5946, 1.5802, 1.5751, 1.5708, 1.5706, 1.5702, 1.5649, 1.5626, 1.5606, 1.5605, 1.5603, 1.5602, 1.5601, 1.5601, 1.56, 1.5598, 1.5593, 1.5586, 1.557, 1.5488, 1.4647, 1.3006, 1.4166, 1.4727, 1.3601, 1.4952, 1.3292, 1.3747, 1.0597, 0.9541, 1.0999, 1.187, 0.974, 1.3967, 0.7906, 0.9644, 1.1119, 1.0373, 1.2586, 1.1976, 0.8797, 0.9275, 1.0623, 1.1795, 0.7842, 0.5626, 0.6188, 0.8938, 0.148, 0.3794, 0.8289, 0.51, -0.1164, 0.117, 0.3568, 0.4207, 0.7434, 0.2357, -0.0686, 0.085, -0.0005, -0.5307, 0.5835, 0.5286, -0.9811, -0.4041, -0.0077, -1.0449]}, \"token.table\": {\"Topic\": [2, 1, 2, 3, 4, 5, 2, 3, 4, 1, 2, 3, 4, 5, 3, 4, 5, 2, 3, 4, 5, 1, 3, 4, 5, 1, 2, 3, 4, 5, 2, 3, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 1, 2, 3, 4, 5, 2, 5, 1, 1, 3, 1, 2, 3, 4, 5, 2, 2, 3, 4, 5, 1, 2, 3, 4, 5, 2, 3, 4, 5, 4, 1, 2, 3, 1, 2, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 5, 5, 5, 1, 2, 3, 4, 5, 5, 4, 3, 3, 2, 4, 1, 2, 3, 4, 5, 1, 2, 3, 3, 4, 4, 5, 4, 5, 1, 2, 3, 4, 5, 4, 4, 1, 5, 1, 2, 3, 4, 5, 5, 3, 4, 5, 4, 1, 2, 3, 1, 2, 3, 3, 1, 2, 3, 4, 5, 1, 3, 4, 2, 5, 1, 4, 5, 1, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 2, 3, 4, 5, 1, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 2, 5, 3, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 4, 5, 3, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 2, 3, 4, 4, 3, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 4, 5, 1, 2, 3, 4, 5, 1, 3, 4, 5, 5, 2, 3, 4, 5, 1, 2, 3, 4, 5, 3, 5, 4, 2, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 3, 4, 5, 1, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 1, 3, 4, 5, 1, 2, 3, 4, 5, 3, 1, 2, 3, 4, 5, 2, 2, 4, 1, 2, 4, 3, 2, 5, 3, 4, 5, 3, 3, 4, 1, 1, 2, 3, 4, 5, 1, 1, 2, 3, 4, 5, 4, 1, 2, 3, 5, 1, 3, 4, 5, 3, 5, 1, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 1, 2, 3, 4, 5, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 1, 2, 3, 5, 1, 2, 3, 4, 5, 3, 2, 3, 4, 5, 2, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 2, 3, 1, 2, 3, 4, 5, 1, 2, 3, 5, 5, 1, 2, 3, 4, 5, 2, 3, 5, 1, 1, 3, 5, 1, 2, 3, 5, 4, 2, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 5, 1, 2, 4, 5, 1, 1, 2, 1, 2, 3, 4, 5, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 4, 5, 1, 1, 2, 3, 4, 1, 2, 3, 2, 3, 5, 5, 1, 2, 3, 5, 2, 4, 1, 3, 4, 5, 1, 2, 3, 4, 5, 2, 1, 1, 2, 3, 4, 5, 2, 1, 2, 3, 4, 5, 1, 2, 4, 5, 2, 3, 1, 3, 4, 5, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 4, 4, 3, 1, 1, 2, 3, 4, 5, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 3, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 5, 4, 2, 2, 5, 1, 2, 3, 4, 5, 1, 2, 3, 5, 3, 1, 1, 2, 3, 1, 2, 3, 4, 5, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 4, 1, 2, 4, 1, 2, 1, 3, 4, 5, 5, 3, 3, 1, 1, 2, 3, 4, 5, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 1, 1, 2, 4, 5, 5, 2, 4, 1, 2, 3, 4, 1, 2, 3, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 4, 1, 2, 3, 4, 5, 2, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 3, 1, 2, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 2, 2, 3, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 4, 1, 2, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 3, 4, 5, 1, 4, 5, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 3, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 2, 3, 4, 1, 2, 3, 4, 5, 3, 2, 1, 2, 3, 4, 5, 3, 4, 5, 1, 2, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 5, 4, 1, 1, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 3, 4, 5, 2, 3, 5, 3, 3], \"Freq\": [0.9016082695971958, 0.645092053286171, 0.20020098205432893, 0.022244553561592103, 0.08897821424636841, 0.044489107123184206, 0.8120445497191766, 0.1558056745310093, 0.7790283726550465, 0.09967633834079656, 0.04983816917039828, 0.7475725375559742, 0.04983816917039828, 0.09967633834079656, 0.04732627344864718, 0.7572203751783548, 0.14197882034594153, 0.045121446341376235, 0.1353643390241287, 0.045121446341376235, 0.8121860341447723, 0.2075018642756094, 0.6521487162947723, 0.0889293704038326, 0.0296431234679442, 0.21973432437698234, 0.16480074328273675, 0.1348369717767846, 0.30962563889483874, 0.16979470520039544, 0.7586228393785062, 0.1083746913397866, 0.06811114667492443, 0.06811114667492443, 0.06811114667492443, 0.7492226134241687, 0.06811114667492443, 0.7913156183146731, 0.037681696110222526, 0.07536339222044505, 0.037681696110222526, 0.037681696110222526, 0.7281883231905057, 0.6109585256040406, 0.02656341415669742, 0.02656341415669742, 0.05312682831339484, 0.2921975557236716, 0.8120114577595937, 0.866009232425618, 0.9105419344699142, 0.728618070952604, 0.8229371111321913, 0.16462510043285938, 0.054875033477619795, 0.22733942440728203, 0.2978930388785075, 0.25869658639449333, 0.8120089123942986, 0.04541265411587263, 0.04541265411587263, 0.9082530823174526, 0.04541265411587263, 0.015920982467966263, 0.7960491233983132, 0.04776294740389879, 0.1273678597437301, 0.015920982467966263, 0.18253727098202313, 0.7301490839280925, 0.03650745419640462, 0.03650745419640462, 0.8720639051188687, 0.09979731649730751, 0.09979731649730751, 0.7983785319784601, 0.06409423537592987, 0.06409423537592987, 0.8332250598870884, 0.1757818918247441, 0.4634249875379617, 0.19176206380881175, 0.04794051595220294, 0.12784137587254116, 0.14613727792046732, 0.2240771594780499, 0.4091843781773085, 0.10716733714167603, 0.11690982233637386, 0.09970714287012261, 0.09970714287012261, 0.7976571429609809, 0.09970714287012261, 0.09970714287012261, 0.8173112170754014, 0.16141044968820784, 0.07336838622191266, 0.11738941795506025, 0.14673677244382533, 0.49890502630900607, 0.14274490484575297, 0.4853326764755601, 0.07137245242287649, 0.14274490484575297, 0.15701939533032827, 0.0285052980951527, 0.0285052980951527, 0.0285052980951527, 0.2280423847612216, 0.6841271542836649, 0.865559610035889, 0.8654114209044856, 0.90115876220748, 0.7566075583488742, 0.028022502161069412, 0.056045004322138825, 0.14011251080534706, 0.028022502161069412, 0.9016469862222082, 0.8173168946433818, 0.8777695886184497, 0.8781784046141267, 0.15546050610564952, 0.7773025305282475, 0.062344049740476096, 0.21196976911761872, 0.11221928953285697, 0.16209452932523785, 0.4364083481833327, 0.060162579033443186, 0.8422761064682046, 0.060162579033443186, 0.7842935618219048, 0.8173134803154949, 0.8330349345917222, 0.0640796103532094, 0.8916367430447549, 0.8116605082708295, 0.16812006975048036, 0.46513219297632896, 0.1457040604504163, 0.1961400813755604, 0.028020011625080058, 0.9080413404899, 0.9083228003489945, 0.7937968163838298, 0.09922460204797873, 0.3588379428827307, 0.44854742860341335, 0.03987088254252563, 0.07974176508505126, 0.07974176508505126, 0.9016660856215, 0.1808287862228682, 0.0904143931114341, 0.7233151448914727, 0.9084408312170238, 0.7655203039487616, 0.054680021710625824, 0.16404006513187747, 0.8847323598073336, 0.052043079988666675, 0.052043079988666675, 0.7319600620083103, 0.44317568405759067, 0.0893959534365097, 0.1331429093735251, 0.13884903406096188, 0.19591028093532978, 0.8739019692306342, 0.8782250805696507, 0.8173093434829571, 0.8413541359914143, 0.8660821225670639, 0.873936272541193, 0.8173115010915979, 0.8654212770140824, 0.07384217309327169, 0.02461405769775723, 0.1722984038843006, 0.7384217309327169, 0.17590669562603783, 0.020694905367769155, 0.062084716103307465, 0.28972867514876816, 0.44494046540703686, 0.1111721131387945, 0.222344226277589, 0.05558605656939725, 0.1111721131387945, 0.5002745091245753, 0.07139081408634229, 0.7139081408634229, 0.047593876057561525, 0.16657856620146533, 0.9364385242124984, 0.0746624604716816, 0.4547622592366061, 0.2307748778215613, 0.033937482032582546, 0.21041238860201178, 0.06942705075784626, 0.17356762689461563, 0.20828115227353877, 0.2661370279050773, 0.2892793781576927, 0.5446427607989783, 0.140050995634023, 0.0700254978170115, 0.06224488694845467, 0.17895404997680717, 0.7761804901328848, 0.06293355325401769, 0.020977851084672563, 0.020977851084672563, 0.12586710650803537, 0.9276414910360823, 0.7730295750815332, 0.8776421219415553, 0.10424707293940615, 0.22934356046669352, 0.0833976583515249, 0.10424707293940615, 0.47953653552126824, 0.07058989945627231, 0.2894185877707165, 0.091766869293154, 0.42353939673763386, 0.12706181902129016, 0.0902683516163074, 0.19343218203494444, 0.3030437518547463, 0.25146183664542776, 0.16764122443028517, 0.08373591649818207, 0.08373591649818207, 0.1256038747472731, 0.5024154989890924, 0.18840581212090965, 0.16666963546614544, 0.04166740886653636, 0.2083370443326818, 0.5416763152649727, 0.3080742323244595, 0.1263894286459321, 0.14218810722667363, 0.32387291090520104, 0.10269141077481984, 0.1813593938596581, 0.08463438380117377, 0.0604531312865527, 0.4594437977778005, 0.2176312726315897, 0.19114951853418358, 0.12743301235612237, 0.06371650617806118, 0.6371650617806119, 0.8777396340678385, 0.41992885378900197, 0.04818855699218055, 0.19963830753903372, 0.27765787600256414, 0.05507263656249206, 0.07286301088734413, 0.2930023203767668, 0.4030719751214782, 0.08216467185168594, 0.15037685225685915, 0.09546116198468255, 0.023865290496170637, 0.8114198768698017, 0.023865290496170637, 0.04773058099234127, 0.8660915524918229, 0.1211540835983595, 0.8480785851885165, 0.8717944991427578, 0.8777621351417172, 0.6367809304529225, 0.1367583206341847, 0.06410546279727408, 0.004273697519818272, 0.15812680823327604, 0.06867651681873217, 0.051507387614049126, 0.17169129204683042, 0.609504086766248, 0.10301477522809825, 0.04834885428242547, 0.19339541712970187, 0.7010583870951693, 0.04834885428242547, 0.1960066205943491, 0.2246905162910831, 0.2055679191599271, 0.33464544979523014, 0.04302584354510102, 0.1481250983769283, 0.09875006558461887, 0.22218764756539244, 0.518437844319249, 0.8116393523735334, 0.03895104348416435, 0.1558041739366574, 0.7790208696832871, 0.03895104348416435, 0.4271094097442416, 0.2713400956022241, 0.1959678468238285, 0.07537224877839559, 0.030148899511358233, 0.13550847925402734, 0.8130508755241641, 0.8173197954470487, 0.10891379511821812, 0.7623965658275269, 0.08506866543243695, 0.4763845264216469, 0.35728839481623514, 0.05104119925946217, 0.03402746617297478, 0.5965661493971469, 0.07101977969013654, 0.2556712068844916, 0.05681582375210923, 0.014203955938027308, 0.10905928286472961, 0.10905928286472961, 0.7634149800531073, 0.8657639629052561, 0.041729823706368414, 0.12518947111910525, 0.16691929482547366, 0.6259473555955263, 0.0776973211022046, 0.1398551779839683, 0.24863142752705472, 0.18647357064529105, 0.35740767707014115, 0.13966864564260717, 0.6158117557878588, 0.13332007084067046, 0.08253147242517696, 0.025394299207746756, 0.09023722935814142, 0.8121350642232729, 0.06815465867751572, 0.06815465867751572, 0.8178559041301886, 0.8116754737632426, 0.1826219435900864, 0.19784043888926026, 0.015218495299173867, 0.5326473354710853, 0.0684832288462824, 0.7314296941268807, 0.15233894273037693, 0.1575920097210796, 0.48328216314464406, 0.10506133981405306, 0.10506133981405306, 0.8120067788278379, 0.20920852673015194, 0.753150696228547, 0.8740483347021569, 0.7731073298055767, 0.15462146596111534, 0.8780159744502947, 0.13519862025861595, 0.8111917215516957, 0.06807942787283539, 0.8169531344740247, 0.13615885574567077, 0.8229204472592597, 0.7506856369720074, 0.17323514699354015, 0.8192789483445895, 0.24649784853631101, 0.08216594951210367, 0.6025502964220936, 0.02738864983736789, 0.05477729967473578, 0.8737341266111536, 0.15957556257521027, 0.3260891930884732, 0.24630141180086804, 0.21854914004865755, 0.048566475566368346, 0.9079494287870862, 0.7390422322303913, 0.035192487249066255, 0.035192487249066255, 0.17596243624533128, 0.04950385065310578, 0.04950385065310578, 0.7425577597965867, 0.14851155195931734, 0.8619400370338336, 0.07835818518489396, 0.9102054379368758, 0.9017101215338199, 0.15953244291178967, 0.02127099238823862, 0.5317748097059656, 0.2765229010471021, 0.01063549619411931, 0.19561186844314796, 0.29341780266472195, 0.07335445066618049, 0.40344947866399267, 0.036677225333090244, 0.92698679821067, 0.16804213114315236, 0.18004514051052037, 0.45611435595998495, 0.08402106557157618, 0.10802708430631222, 0.030082549026181498, 0.7219811766283559, 0.030082549026181498, 0.1504127451309075, 0.060165098052362996, 0.434136052513471, 0.22155908886894382, 0.15868421229802732, 0.08982125224416641, 0.09880337746858305, 0.43764792384216056, 0.024313773546786696, 0.31607905610822706, 0.20666707514768692, 0.012156886773393348, 0.3005366984794714, 0.049178732478458956, 0.13660759021794155, 0.07650025052204727, 0.43714428869741295, 0.3418078992670204, 0.08138283315881438, 0.03255313326352575, 0.5371266988481749, 0.0801456579704732, 0.01001820724630915, 0.26247702985329974, 0.5850633031844543, 0.06211288492711673, 0.8659779161332831, 0.06245842761331064, 0.02498337104532426, 0.0874417986586349, 0.41222562224785025, 0.39973393672518814, 0.0207134626504014, 0.0828538506016056, 0.8699654313168588, 0.0207134626504014, 0.0416965296664593, 0.2084826483322965, 0.1667861186658372, 0.5420548856639709, 0.3442864340018109, 0.11066349664343922, 0.4426539865737569, 0.11066349664343922, 0.012295944071493247, 0.8780345688313248, 0.06448073646686348, 0.7737688376023616, 0.06448073646686348, 0.06448073646686348, 0.9020855499114042, 0.760390577939088, 0.047524411121193, 0.095048822242386, 0.047524411121193, 0.047524411121193, 0.6381590323395239, 0.10292887618379418, 0.08234310094703534, 0.10292887618379418, 0.08234310094703534, 0.7233256222854492, 0.21699768668563474, 0.07079476318005704, 0.6099241135512605, 0.1198065223047119, 0.010891502027701082, 0.18515553447091837, 0.7866936821393403, 0.06555780684494503, 0.10926301140824171, 0.043705204563296685, 0.8116855208102246, 0.3866785157176331, 0.2379560096723896, 0.2082115084633409, 0.1189780048361948, 0.03965933494539827, 0.9015756434241928, 0.19720628662240647, 0.6902220031784226, 0.8736742210071589, 0.7942806158056651, 0.09928507697570814, 0.09928507697570814, 0.12032373037547013, 0.842266112628291, 0.7980742340904531, 0.09975927926130664, 0.8718085132820772, 0.08376294968685272, 0.8376294968685272, 0.10697420200001583, 0.3922387406667247, 0.0641845212000095, 0.12123742893335128, 0.3137909925333798, 0.235702819820452, 0.3007242873571284, 0.24383050326253655, 0.10565988474709917, 0.11378756818918372, 0.1427106057766103, 0.06008867611646749, 0.21782145092219465, 0.30044338058233744, 0.27791012703866214, 0.13537769554730522, 0.15721280773235444, 0.03930320193308861, 0.0480372468071083, 0.6157501636183883, 0.8653778869074544, 0.09546346833504105, 0.6682442783452873, 0.03182115611168035, 0.1909269366700821, 0.8501181063553209, 0.9105135764531341, 0.812007799539269, 0.07591566542760256, 0.5314096579932179, 0.05964802283597344, 0.2169019012217216, 0.11929604567194688, 0.8658765520053098, 0.14806212133060073, 0.24462437437229684, 0.2317494073000707, 0.1287496707222615, 0.23818689083618377, 0.09050011677266481, 0.5769382444257382, 0.21493777733507893, 0.022625029193166203, 0.09050011677266481, 0.2223585434000024, 0.6392808122750069, 0.0277948179250003, 0.0277948179250003, 0.1111792717000012, 0.6946955367677254, 0.04962110976912324, 0.19848443907649296, 0.04962110976912324, 0.873962554283384, 0.14230800667203378, 0.04743600222401126, 0.09487200444802252, 0.6641040311361577, 0.057085094675419334, 0.7421062307804513, 0.171255284026258, 0.874182380094376, 0.08325546477089295, 0.041627732385446474, 0.8653840361403, 0.71139369384905, 0.11856561564150835, 0.071139369384905, 0.11856561564150835, 0.8418314408503847, 0.12026163440719782, 0.11851526411136828, 0.7110915846682098, 0.08888644808352622, 0.08888644808352622, 0.11715205606792935, 0.14058246728151522, 0.09372164485434348, 0.6091906915532326, 0.035145616820378804, 0.9015660902809864, 0.8192907910663191, 0.2170020562823126, 0.2350855609725053, 0.07233401876077086, 0.07233401876077086, 0.41592060787443247, 0.8660953060600577, 0.08720960449494973, 0.23255894531986596, 0.21802401123737433, 0.4505829565572403, 0.014534934082491622, 0.15481877899929897, 0.10321251933286597, 0.10321251933286597, 0.5676688563307629, 0.8120273059488323, 0.8229165248949795, 0.7042579580186358, 0.07042579580186357, 0.07042579580186357, 0.14085159160372715, 0.04442970184737756, 0.5553712730922196, 0.3776524657027093, 0.04442970184737756, 0.2048303250318942, 0.07878089424303623, 0.12604943078885797, 0.2048303250318942, 0.3781482923665739, 0.19329447769706945, 0.1819242143031242, 0.3638484286062484, 0.15918368751523368, 0.10233237054550735, 0.9082150863897421, 0.8175784537043718, 0.8543657283480328, 0.8736850752196329, 0.12061117960271937, 0.06892067405869678, 0.5944408137562598, 0.06892067405869678, 0.14645643237473066, 0.8117494339075098, 0.1303324965573898, 0.052132998622955916, 0.5734629848525151, 0.18246549518034572, 0.052132998622955916, 0.019273153399200316, 0.40473622138320664, 0.3950996446836065, 0.057819460197600946, 0.11563892039520189, 0.13718273986521684, 0.8230964391913009, 0.0139446526236712, 0.278893052473424, 0.139446526236712, 0.4741181892048208, 0.0836679157420272, 0.480684323982903, 0.14566191635845546, 0.2767576410810654, 0.04369857490753664, 0.04369857490753664, 0.4505217148620331, 0.2753188257490202, 0.08760144455650644, 0.08760144455650644, 0.10011593663600735, 0.8116851697062898, 0.8173042087666407, 0.8120215360857237, 0.721292127314418, 0.12021535455240302, 0.07089272284328384, 0.5435108751318428, 0.28357089137313535, 0.02363090761442795, 0.0945236304577118, 0.12394894462013903, 0.061974472310069514, 0.12394894462013903, 0.6507319592557299, 0.8777162906365572, 0.8193126599715415, 0.09976989096497671, 0.09976989096497671, 0.7981591277198137, 0.5049205255420134, 0.09180373191672972, 0.13770559787509457, 0.06885279893754728, 0.19508293032305063, 0.8657372044269979, 0.2976515393167448, 0.0744128848291862, 0.5102597816858482, 0.0744128848291862, 0.042521648473820686, 0.19945216965329093, 0.5983565089598728, 0.11080676091849496, 0.08864540873479597, 0.8173008641954688, 0.8192786260234105, 0.13619536301925675, 0.8171721781155404, 0.8844720751286428, 0.05202776912521428, 0.05098893906265256, 0.7138451468771358, 0.1274723476566314, 0.07648340859397884, 0.8653731239910697, 0.8229119737694467, 0.9154444169534074, 0.9107938885671603, 0.015924005823797212, 0.5891882154804968, 0.07962002911898605, 0.07962002911898605, 0.23886008735695818, 0.25896240372821866, 0.051792480745643736, 0.6733022496933686, 0.051792480745643736, 0.2325103594697612, 0.07342432404308248, 0.2080355847887337, 0.1835608101077062, 0.3059346835128437, 0.9368491536432085, 0.8120111178574262, 0.8499919717693037, 0.2091972993536541, 0.6275918980609623, 0.07607174521951059, 0.07607174521951059, 0.7213931232119398, 0.8120037853281291, 0.7785504530542969, 0.04351692852794276, 0.2611015711676566, 0.04351692852794276, 0.6962708564470842, 0.10454772828168644, 0.05227386414084322, 0.7841079621126483, 0.23194900812598696, 0.42451044883435346, 0.1509856751008783, 0.13348008958193588, 0.059081351126430634, 0.037523185120075714, 0.2908046846805868, 0.14071194420028393, 0.4878014065609843, 0.04690398140009464, 0.15240003692112816, 0.35983342050821926, 0.16933337435680906, 0.24553339281737316, 0.07196668410164385, 0.356141427729047, 0.18761021639298012, 0.1844303822168279, 0.22258839233065436, 0.04769751264228308, 0.09026788688423075, 0.722143095073846, 0.09026788688423075, 0.5650551722080331, 0.007740481811068947, 0.015480963622137894, 0.3638026451202405, 0.046442890866413684, 0.9018235376215801, 0.26752061714891046, 0.09987436373559323, 0.3566941561985472, 0.08560659748765134, 0.18904790278523004, 0.10574138918936113, 0.015105912741337303, 0.6873190297308474, 0.022658869112005955, 0.16616504015471034, 0.08624268117708271, 0.032341005441406015, 0.26950837867838345, 0.24794770838411279, 0.3665313950026015, 0.8544868075759727, 0.05733785551565487, 0.05733785551565487, 0.8600678327348231, 0.7940719644976031, 0.09925899556220039, 0.03308633185406679, 0.03308633185406679, 0.03308633185406679, 0.13354086170584537, 0.1513463099332914, 0.21366537872935257, 0.017805448227446046, 0.4896498262547663, 0.9022186495433816, 0.8128552678756481, 0.13547587797927468, 0.05334759815960786, 0.02667379907980393, 0.5334759815960786, 0.24895545807817002, 0.15115152811888893, 0.353404637670171, 0.23208662772369437, 0.06857104910018243, 0.04747226476166476, 0.2953829807392474, 0.19894002150950374, 0.2396322986364477, 0.054256369502591925, 0.13564092375647982, 0.3707518582677115, 0.780496585979463, 0.7579858233328243, 0.10828368904754632, 0.18952119185085894, 0.24367010380824722, 0.5414891195738827, 0.1991295264477115, 0.16849421468652512, 0.015317655880593192, 0.47484733229838894, 0.15317655880593192, 0.2931554686013398, 0.09771848953377993, 0.5081361455756557, 0.019543697906755985, 0.05863109372026796, 0.7103131460289706, 0.05463947277145927, 0.1639184183143778, 0.8117349934011853, 0.8186849242109476, 0.04548249578949709, 0.09096499157899418, 0.029631700415381098, 0.8000559112152896, 0.029631700415381098, 0.11852680166152439, 0.07344614693201938, 0.2509410020177329, 0.24482048977339793, 0.3672307346600969, 0.06732563468768443, 0.5077179522780981, 0.2669896128358964, 0.09191445687793155, 0.12255260917057541, 0.013130636696847364, 0.20321585934783545, 0.05080396483695886, 0.33022577144023263, 0.30482378902175317, 0.11007525714674421, 0.8229194425821071, 0.37684375795894137, 0.3445428644196035, 0.09690268061801349, 0.03230089353933783, 0.13997053867046394, 0.45089530452166715, 0.022544765226083358, 0.23672003487387525, 0.28180956532604196, 0.011272382613041679, 0.8981727301013981, 0.06123904977964078, 0.020413016593213592, 0.08923667782033833, 0.3569467112813533, 0.07436389818361527, 0.2974555927344611, 0.17847335564067665, 0.8781217525015083, 0.8668359706377643, 0.1719864335082502, 0.09381078191359103, 0.5315944308436825, 0.1563513031893184, 0.046905390956795516, 0.8086826649415527, 0.1155260949916504, 0.0577630474958252, 0.12024164974296798, 0.8416915482007759, 0.12024164974296798, 0.15362958673960664, 0.09965162383109619, 0.09134732184517151, 0.4110629483032718, 0.24497690858477814, 0.03151844953772594, 0.15759224768862967, 0.31518449537725934, 0.06303689907545187, 0.42549906875930016, 0.14848356769395568, 0.07424178384697784, 0.07424178384697784, 0.5691870094934968, 0.14848356769395568, 0.9366239881280627, 0.8116492557499434, 0.8173090549612477, 0.9365314871853003, 0.8190451035253302, 0.17846769595781642, 0.5056584718804799, 0.07436153998242351, 0.14872307996484702, 0.08923384797890821, 0.047679446319664, 0.190717785278656, 0.0095358892639328, 0.6389045806834975, 0.11443067116719359, 0.15745575083162058, 0.2361836262474309, 0.4723672524948618, 0.08856885984278658, 0.05904590656185772, 0.878166779638655, 0.049120686220658086, 0.9332930381925036, 0.049774946214567184, 0.7466241932185078, 0.19909978485826874, 0.8228917375309346, 0.7315574492123873], \"Term\": [\"accord\", \"add\", \"add\", \"add\", \"add\", \"add\", \"advertise\", \"afraid\", \"afraid\", \"ago\", \"ago\", \"ago\", \"ago\", \"ago\", \"agree\", \"agree\", \"agree\", \"ah\", \"ah\", \"ah\", \"ah\", \"album\", \"album\", \"album\", \"album\", \"amp\", \"amp\", \"amp\", \"amp\", \"amp\", \"ampamp\", \"ampamp\", \"answer\", \"answer\", \"answer\", \"answer\", \"answer\", \"appreciate\", \"appreciate\", \"appreciate\", \"appreciate\", \"appreciate\", \"artist\", \"ask\", \"ask\", \"ask\", \"ask\", \"ask\", \"atlanta\", \"attack\", \"attention\", \"available\", \"avoid\", \"awesome\", \"awesome\", \"awesome\", \"awesome\", \"awesome\", \"awsome\", \"awww\", \"awww\", \"awww\", \"awww\", \"bad\", \"bad\", \"bad\", \"bad\", \"bad\", \"band\", \"band\", \"band\", \"band\", \"bank\", \"bb\", \"bb\", \"bb\", \"beat\", \"beat\", \"beat\", \"beautiful\", \"beautiful\", \"beautiful\", \"beautiful\", \"beautiful\", \"bed\", \"bed\", \"bed\", \"bed\", \"bed\", \"begin\", \"begin\", \"begin\", \"begin\", \"begin\", \"bein\", \"big\", \"big\", \"big\", \"big\", \"big\", \"birthday\", \"birthday\", \"birthday\", \"birthday\", \"birthday\", \"blog\", \"blog\", \"blog\", \"blog\", \"blog\", \"bob\", \"booty\", \"bowie\", \"boy\", \"boy\", \"boy\", \"boy\", \"boy\", \"brilliant\", \"calm\", \"careful\", \"cell\", \"chance\", \"chance\", \"check\", \"check\", \"check\", \"check\", \"check\", \"cheer\", \"cheer\", \"cheer\", \"cheese\", \"chile\", \"church\", \"church\", \"classic\", \"click\", \"come\", \"come\", \"come\", \"come\", \"come\", \"compliment\", \"connect\", \"contact\", \"contact\", \"cool\", \"cool\", \"cool\", \"cool\", \"cool\", \"corner\", \"count\", \"count\", \"count\", \"cutie\", \"dad\", \"dad\", \"dad\", \"dance\", \"dance\", \"dance\", \"dave\", \"day\", \"day\", \"day\", \"day\", \"day\", \"dc\", \"devil\", \"dividend\", \"drag\", \"dye\", \"ebay\", \"edit\", \"el\", \"email\", \"email\", \"email\", \"email\", \"enjoy\", \"enjoy\", \"enjoy\", \"enjoy\", \"enjoy\", \"excite\", \"excite\", \"excite\", \"excite\", \"excite\", \"excited\", \"excited\", \"excited\", \"excited\", \"feedback\", \"feel\", \"feel\", \"feel\", \"feel\", \"feel\", \"find\", \"find\", \"find\", \"find\", \"find\", \"follow\", \"follow\", \"follow\", \"follow\", \"follow\", \"follower\", \"follower\", \"follower\", \"follower\", \"follower\", \"football\", \"forgive\", \"forum\", \"free\", \"free\", \"free\", \"free\", \"free\", \"friend\", \"friend\", \"friend\", \"friend\", \"friend\", \"fun\", \"fun\", \"fun\", \"fun\", \"fun\", \"game\", \"game\", \"game\", \"game\", \"game\", \"garden\", \"garden\", \"garden\", \"garden\", \"get\", \"get\", \"get\", \"get\", \"get\", \"glad\", \"glad\", \"glad\", \"glad\", \"glad\", \"glass\", \"glass\", \"glass\", \"glass\", \"gnight\", \"go\", \"go\", \"go\", \"go\", \"go\", \"good\", \"good\", \"good\", \"good\", \"good\", \"goodnight\", \"goodnight\", \"goodnight\", \"goodnight\", \"goodnight\", \"gooood\", \"gorgeous\", \"gorgeous\", \"grand\", \"gray\", \"great\", \"great\", \"great\", \"great\", \"great\", \"guy\", \"guy\", \"guy\", \"guy\", \"guy\", \"ha\", \"ha\", \"ha\", \"ha\", \"haha\", \"haha\", \"haha\", \"haha\", \"haha\", \"hair\", \"hair\", \"hair\", \"hair\", \"halfway\", \"hand\", \"hand\", \"hand\", \"hand\", \"happy\", \"happy\", \"happy\", \"happy\", \"happy\", \"hat\", \"hat\", \"hater\", \"having\", \"having\", \"head\", \"head\", \"head\", \"head\", \"head\", \"hear\", \"hear\", \"hear\", \"hear\", \"hear\", \"hee\", \"hee\", \"hee\", \"heh\", \"hehe\", \"hehe\", \"hehe\", \"hehe\", \"help\", \"help\", \"help\", \"help\", \"help\", \"hey\", \"hey\", \"hey\", \"hey\", \"hey\", \"hmmm\", \"hmmm\", \"holiday\", \"holiday\", \"holiday\", \"hollywood\", \"home\", \"home\", \"home\", \"home\", \"home\", \"hoo\", \"hope\", \"hope\", \"hope\", \"hope\", \"hope\", \"hospital\", \"hug\", \"hug\", \"ignore\", \"ily\", \"ily\", \"involve\", \"james\", \"james\", \"jealous\", \"jealous\", \"jealous\", \"jeff\", \"july\", \"july\", \"jumper\", \"june\", \"june\", \"june\", \"june\", \"june\", \"kevin\", \"know\", \"know\", \"know\", \"know\", \"know\", \"kool\", \"la\", \"la\", \"la\", \"la\", \"laker\", \"laker\", \"laker\", \"laker\", \"lazy\", \"lazy\", \"lb\", \"lead\", \"let\", \"let\", \"let\", \"let\", \"let\", \"life\", \"life\", \"life\", \"life\", \"life\", \"limit\", \"listen\", \"listen\", \"listen\", \"listen\", \"listen\", \"lmao\", \"lmao\", \"lmao\", \"lmao\", \"lmao\", \"lol\", \"lol\", \"lol\", \"lol\", \"lol\", \"long\", \"long\", \"long\", \"long\", \"long\", \"look\", \"look\", \"look\", \"look\", \"look\", \"lot\", \"lot\", \"lot\", \"lot\", \"love\", \"love\", \"love\", \"love\", \"love\", \"loveee\", \"lt\", \"lt\", \"lt\", \"lt\", \"lt\", \"luck\", \"luck\", \"luck\", \"luck\", \"luv\", \"luv\", \"luv\", \"luv\", \"make\", \"make\", \"make\", \"make\", \"make\", \"meow\", \"message\", \"message\", \"message\", \"message\", \"microsoft\", \"minute\", \"minute\", \"minute\", \"minute\", \"minute\", \"mom\", \"mom\", \"mom\", \"mom\", \"mom\", \"moon\", \"moon\", \"morning\", \"morning\", \"morning\", \"morning\", \"morning\", \"mother\", \"mother\", \"mother\", \"mother\", \"mouse\", \"movie\", \"movie\", \"movie\", \"movie\", \"movie\", \"muscle\", \"museum\", \"museum\", \"musical\", \"na\", \"na\", \"na\", \"nah\", \"nah\", \"nail\", \"nail\", \"natural\", \"nearly\", \"nearly\", \"need\", \"need\", \"need\", \"need\", \"need\", \"new\", \"new\", \"new\", \"new\", \"new\", \"nice\", \"nice\", \"nice\", \"nice\", \"nice\", \"night\", \"night\", \"night\", \"night\", \"night\", \"nighty\", \"nite\", \"nite\", \"nite\", \"nite\", \"nom\", \"normal\", \"normally\", \"not\", \"not\", \"not\", \"not\", \"not\", \"obviously\", \"oh\", \"oh\", \"oh\", \"oh\", \"oh\", \"ok\", \"ok\", \"ok\", \"ok\", \"ok\", \"okay\", \"okay\", \"okay\", \"okay\", \"okay\", \"online\", \"online\", \"online\", \"online\", \"owner\", \"pack\", \"pack\", \"pack\", \"pack\", \"paint\", \"paint\", \"paint\", \"pass\", \"pass\", \"pass\", \"patron\", \"pay\", \"pay\", \"pay\", \"pay\", \"pig\", \"pig\", \"plan\", \"plan\", \"plan\", \"plan\", \"play\", \"play\", \"play\", \"play\", \"play\", \"poke\", \"poker\", \"post\", \"post\", \"post\", \"post\", \"post\", \"pressure\", \"pretty\", \"pretty\", \"pretty\", \"pretty\", \"pretty\", \"proud\", \"proud\", \"proud\", \"proud\", \"purpose\", \"quothow\", \"r\", \"r\", \"r\", \"r\", \"rain\", \"rain\", \"rain\", \"rain\", \"read\", \"read\", \"read\", \"read\", \"read\", \"ready\", \"ready\", \"ready\", \"ready\", \"ready\", \"recover\", \"respect\", \"response\", \"ridiculous\", \"right\", \"right\", \"right\", \"right\", \"right\", \"roast\", \"rock\", \"rock\", \"rock\", \"rock\", \"rock\", \"s\", \"s\", \"s\", \"s\", \"s\", \"sarah\", \"sarah\", \"say\", \"say\", \"say\", \"say\", \"say\", \"school\", \"school\", \"school\", \"school\", \"school\", \"see\", \"see\", \"see\", \"see\", \"see\", \"senior\", \"shareholder\", \"shoulder\", \"shout\", \"shout\", \"sit\", \"sit\", \"sit\", \"sit\", \"sit\", \"site\", \"site\", \"site\", \"site\", \"skinny\", \"skip\", \"sky\", \"sky\", \"sky\", \"sleep\", \"sleep\", \"sleep\", \"sleep\", \"sleep\", \"sms\", \"song\", \"song\", \"song\", \"song\", \"song\", \"sorry\", \"sorry\", \"sorry\", \"sorry\", \"spammer\", \"spray\", \"stage\", \"stage\", \"star\", \"star\", \"stay\", \"stay\", \"stay\", \"stay\", \"steal\", \"stream\", \"suck\", \"sugar\", \"sun\", \"sun\", \"sun\", \"sun\", \"sun\", \"support\", \"support\", \"support\", \"support\", \"sure\", \"sure\", \"sure\", \"sure\", \"sure\", \"surprised\", \"susan\", \"sway\", \"sweet\", \"sweet\", \"sweet\", \"sweet\", \"swing\", \"talent\", \"taylor\", \"tea\", \"tea\", \"tea\", \"tea\", \"test\", \"test\", \"test\", \"thank\", \"thank\", \"thank\", \"thank\", \"thank\", \"thing\", \"thing\", \"thing\", \"thing\", \"thing\", \"think\", \"think\", \"think\", \"think\", \"think\", \"time\", \"time\", \"time\", \"time\", \"time\", \"tip\", \"tip\", \"tip\", \"to\", \"to\", \"to\", \"to\", \"to\", \"toast\", \"today\", \"today\", \"today\", \"today\", \"today\", \"tomorrow\", \"tomorrow\", \"tomorrow\", \"tomorrow\", \"tomorrow\", \"tonight\", \"tonight\", \"tonight\", \"tonight\", \"tonight\", \"topic\", \"town\", \"town\", \"town\", \"train\", \"train\", \"train\", \"train\", \"train\", \"try\", \"try\", \"try\", \"try\", \"try\", \"tweep\", \"tweeps\", \"tweeps\", \"tweet\", \"tweet\", \"tweet\", \"tweet\", \"tweet\", \"twitter\", \"twitter\", \"twitter\", \"twitter\", \"twitter\", \"u\", \"u\", \"u\", \"u\", \"u\", \"uncle\", \"uni\", \"uni\", \"update\", \"update\", \"update\", \"ur\", \"ur\", \"ur\", \"ur\", \"ur\", \"use\", \"use\", \"use\", \"use\", \"use\", \"vacation\", \"vacation\", \"vacation\", \"vine\", \"vip\", \"vip\", \"vip\", \"vote\", \"vote\", \"vote\", \"vote\", \"wait\", \"wait\", \"wait\", \"wait\", \"wait\", \"watch\", \"watch\", \"watch\", \"watch\", \"watch\", \"way\", \"way\", \"way\", \"way\", \"way\", \"weapon\", \"week\", \"week\", \"week\", \"week\", \"week\", \"weekend\", \"weekend\", \"weekend\", \"weekend\", \"weekend\", \"welcome\", \"welcome\", \"welcome\", \"well\", \"well\", \"well\", \"well\", \"well\", \"wicked\", \"window\", \"wish\", \"wish\", \"wish\", \"wish\", \"wish\", \"woo\", \"woo\", \"woo\", \"woop\", \"woop\", \"woop\", \"work\", \"work\", \"work\", \"work\", \"work\", \"world\", \"world\", \"world\", \"world\", \"world\", \"write\", \"write\", \"write\", \"write\", \"write\", \"writer\", \"wwwiamsoannoyedcom\", \"wwwmeasia\", \"wwwtweeteraddercom\", \"wwwtweeterfollowcom\", \"x\", \"x\", \"x\", \"x\", \"x\", \"yeah\", \"yeah\", \"yeah\", \"yeah\", \"yeah\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"zomg\", \"\\u00a9\", \"\\u00a9\", \"\\u00b0\", \"\\u00b0\", \"\\u00b0\", \"\\u00f0\\u00ba\", \"\\u00f0\\u00bd\\u00f0\\u00b5\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [1, 4, 2, 3, 5]};\n",
              "\n",
              "function LDAvis_load_lib(url, callback){\n",
              "  var s = document.createElement('script');\n",
              "  s.src = url;\n",
              "  s.async = true;\n",
              "  s.onreadystatechange = s.onload = callback;\n",
              "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
              "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
              "}\n",
              "\n",
              "if(typeof(LDAvis) !== \"undefined\"){\n",
              "   // already loaded: just create the visualization\n",
              "   !function(LDAvis){\n",
              "       new LDAvis(\"#\" + \"ldavis_el1471401863204894889789152218\", ldavis_el1471401863204894889789152218_data);\n",
              "   }(LDAvis);\n",
              "}else if(typeof define === \"function\" && define.amd){\n",
              "   // require.js is available: use it to load d3/LDAvis\n",
              "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
              "   require([\"d3\"], function(d3){\n",
              "      window.d3 = d3;\n",
              "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
              "        new LDAvis(\"#\" + \"ldavis_el1471401863204894889789152218\", ldavis_el1471401863204894889789152218_data);\n",
              "      });\n",
              "    });\n",
              "}else{\n",
              "    // require.js not available: dynamically load d3 & LDAvis\n",
              "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
              "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
              "                 new LDAvis(\"#\" + \"ldavis_el1471401863204894889789152218\", ldavis_el1471401863204894889789152218_data);\n",
              "            })\n",
              "         });\n",
              "}\n",
              "</script>"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = CountVectorizer()\n",
        "data_vectorized = vectorizer.fit_transform(df['lemmas_back_to_text'])\n",
        "     \n",
        "\n",
        "# Define Search Param\n",
        "search_params = {'n_components': [10, 15, 20, 25, 30], 'learning_decay': [.5, .7, .9]}\n",
        "\n",
        "# Init the Model\n",
        "lda = LatentDirichletAllocation()\n",
        "\n",
        "# Init Grid Search Class\n",
        "model = GridSearchCV(lda, param_grid=search_params)\n",
        "\n",
        "# Do the Grid Search\n",
        "model.fit(data_vectorized)\n",
        "GridSearchCV(cv=None, error_score='raise',\n",
        "             estimator=LatentDirichletAllocation(batch_size=128, \n",
        "                                                 doc_topic_prior=None,\n",
        "                                                 evaluate_every=-1, \n",
        "                                                 learning_decay=0.7, \n",
        "                                                 learning_method=None,\n",
        "                                                 learning_offset=10.0, \n",
        "                                                 max_doc_update_iter=100, \n",
        "                                                 max_iter=10,\n",
        "                                                 mean_change_tol=0.001, \n",
        "                                                 n_components=10, \n",
        "                                                 n_jobs=1,\n",
        "                                                 perp_tol=0.1, \n",
        "                                                 random_state=None,\n",
        "                                                 topic_word_prior=None, \n",
        "                                                 total_samples=1000000.0, \n",
        "                                                 verbose=0),\n",
        "             iid=True, n_jobs=1,\n",
        "             param_grid={'n_topics': [10, 15, 20, 30], \n",
        "                         'learning_decay': [0.5, 0.7, 0.9]},\n",
        "             pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
        "             scoring=None, verbose=0)\n",
        "     \n",
        "\n",
        "# Best Model\n",
        "best_lda_model = model.best_estimator_\n",
        "\n",
        "# Model Parameters\n",
        "print(\"Best Model's Params: \", model.best_params_)\n",
        "\n",
        "# Log Likelihood Score\n",
        "print(\"Best Log Likelihood Score: \", model.best_score_)\n",
        "\n",
        "# Perplexity\n",
        "print(\"Model Perplexity: \", best_lda_model.perplexity(data_vectorized))\n",
        "     "
      ],
      "metadata": {
        "id": "gvoJelGe8CFK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348
        },
        "outputId": "05c54ac6-7321-4a29-90bf-0c2b65b70fd3"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-d00c56fed8bf>\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Do the Grid Search\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_vectorized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m GridSearchCV(cv=None, error_score='raise',\n\u001b[1;32m     17\u001b[0m              estimator=LatentDirichletAllocation(batch_size=128, \n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    872\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 874\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    875\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1386\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1387\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1388\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    819\u001b[0m                     )\n\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 821\u001b[0;31m                 out = parallel(\n\u001b[0m\u001b[1;32m    822\u001b[0m                     delayed(_fit_and_score)(\n\u001b[1;32m    823\u001b[0m                         \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         )\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1086\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1087\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1088\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1089\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1090\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    899\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    900\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 901\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    902\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    595\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 597\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    598\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    289\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    289\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    682\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 684\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    685\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/decomposition/_lda.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    666\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m                     \u001b[0;31m# batch update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 668\u001b[0;31m                     self._em_step(\n\u001b[0m\u001b[1;32m    669\u001b[0m                         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_update\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparallel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparallel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m                     )\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/decomposition/_lda.py\u001b[0m in \u001b[0;36m_em_step\u001b[0;34m(self, X, total_samples, batch_update, parallel)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m         \u001b[0;31m# E-step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m         _, suff_stats = self._e_step(\n\u001b[0m\u001b[1;32m    518\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcal_sstats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_init\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparallel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparallel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/decomposition/_lda.py\u001b[0m in \u001b[0;36m_e_step\u001b[0;34m(self, X, cal_sstats, random_init, parallel)\u001b[0m\n\u001b[1;32m    458\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mparallel\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m             \u001b[0mparallel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m         results = parallel(\n\u001b[0m\u001b[1;32m    461\u001b[0m             delayed(_update_doc_distribution)(\n\u001b[1;32m    462\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx_slice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         )\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1083\u001b[0m             \u001b[0;31m# remaining jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1085\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1086\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1087\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    899\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    900\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 901\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    902\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    595\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 597\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    598\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    289\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    289\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/decomposition/_lda.py\u001b[0m in \u001b[0;36m_update_doc_distribution\u001b[0;34m(X, exp_topic_word_distr, doc_topic_prior, max_doc_update_iter, mean_change_tol, cal_sstats, random_state)\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0;31m# The optimal phi_{dwk} is proportional to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m             \u001b[0;31m# exp(E[log(theta_{dk})]) * exp(E[log(beta_{dw})]).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m             \u001b[0mnorm_phi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_doc_topic_d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_topic_word_d\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0mdoc_topic_d\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp_doc_topic_d\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnts\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnorm_phi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_topic_word_d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/numpy/core/overrides.py\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Defining a function to loop over number of topics to be used to find an \n",
        "#optimal number of tipics\n",
        "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
        "    \"\"\"\n",
        "    Compute c_v coherence for various number of topics\n",
        "\n",
        "    Parameters:\n",
        "    ----------\n",
        "    dictionary : Gensim dictionary\n",
        "    corpus : Gensim corpus\n",
        "    texts : List of input texts\n",
        "    limit : Max num of topics\n",
        "\n",
        "    Returns:\n",
        "    -------\n",
        "    model_list : List of LDA topic models\n",
        "    coherence_values : Coherence values corresponding to the \n",
        "    LDA model with respective number of topics\n",
        "    \"\"\"\n",
        "    coherence_values_topic = []\n",
        "    model_list_topic = []\n",
        "    for num_topics in range(start, limit, step):\n",
        "        model = LdaMulticore(corpus=corpus, num_topics=num_topics, id2word=id2word)\n",
        "        model_list_topic.append(model)\n",
        "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
        "        coherence_values_topic.append(coherencemodel.get_coherence())\n",
        "\n",
        "    return model_list_topic, coherence_values_topic    \n",
        "     \n",
        "\n",
        "\n",
        "     "
      ],
      "metadata": {
        "id": "QHeCr3HD8jZr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b14ab48e-3c62-456d-aaf0-cfbb4f423d00"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_list_topic, coherence_values_topic = compute_coherence_values(dictionary=id2word,\n",
        "                                                        corpus=corpus,\n",
        "                                                        texts=df1['lemma_tokens'],\n",
        "                                                        start=2, limit=200, step=6)\n",
        "     "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Bb7gpebsrzg",
        "outputId": "ad1bb239-18f5-40b7-c013-5d6f0308b26d"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "WARNING:gensim.models.ldamulticore:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "/usr/local/lib/python3.9/dist-packages/gensim/topic_coherence/direct_confirmation_measure.py:204: RuntimeWarning: divide by zero encountered in double_scalars\n",
            "  m_lr_i = np.log(numerator / denominator)\n",
            "/usr/local/lib/python3.9/dist-packages/gensim/topic_coherence/indirect_confirmation_measure.py:323: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  return cv1.T.dot(cv2)[0, 0] / (_magnitude(cv1) * _magnitude(cv2))\n",
            "WARNING:gensim.models.ldamulticore:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamulticore:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamulticore:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamulticore:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamulticore:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamulticore:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamulticore:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamulticore:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamulticore:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamulticore:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamulticore:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamulticore:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamulticore:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamulticore:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamulticore:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamulticore:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamulticore:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamulticore:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamulticore:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamulticore:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamulticore:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamulticore:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamulticore:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamulticore:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamulticore:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamulticore:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamulticore:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamulticore:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamulticore:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamulticore:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamulticore:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamulticore:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_5_2 = LdaMulticore(corpus=corpus,\n",
        "                       id2word=id2word,\n",
        "                       num_topics=68,\n",
        "                       random_state=42,\n",
        "                       chunksize=2000,\n",
        "                       passes=25,\n",
        "                       decay=0.5,\n",
        "                       iterations=70)"
      ],
      "metadata": {
        "id": "v7L-Aipi8wH6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a id2word dictionary\n",
        "id2word = Dictionary(df1['lemma_tokens'])\n",
        "print(len(id2word))\n",
        "     \n",
        "\n",
        "# Filtering Extremes\n",
        "id2word.filter_extremes(no_below=2, no_above=.99)\n",
        "print(len(id2word))\n",
        "     \n",
        "\n",
        "# Creating a corpus object \n",
        "corpus = [id2word.doc2bow(d) for d in df1['lemma_tokens']]\n",
        "     \n",
        "\n",
        "# Instantiating a Base LDA model \n",
        "base_model = LdaMulticore(corpus=corpus, num_topics=5, id2word=id2word, workers=12, passes=5)\n",
        "     \n",
        "\n",
        "# Filtering for words \n",
        "words = [re.findall(r'\"([^\"]*)\"',t[1]) for t in base_model.print_topics()]\n",
        "     \n",
        "\n",
        "# Create Topics\n",
        "topics = [' '.join(t[0:10]) for t in words]\n",
        "     \n",
        "\n",
        "# Getting the topics\n",
        "for id, t in enumerate(topics): \n",
        "    print(f\"------ Topic {id} ------\")\n",
        "    print(t, end=\"\\n\\n\")\n",
        "     "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ce7WhWCyo6Xd",
        "outputId": "7616efba-e119-4392-f34b-5e76036585e2"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "WARNING:gensim.models.ldamulticore:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5820\n",
            "2107\n",
            "------ Topic 0 ------\n",
            "depression emoji anxiety people ¦ face â go good help\n",
            "\n",
            "------ Topic 1 ------\n",
            "depression anxiety day face think bad thing feel emoji lose\n",
            "\n",
            "------ Topic 2 ------\n",
            "depression feel kid u life people mom cure anxiety emotional\n",
            "\n",
            "------ Topic 3 ------\n",
            "depression ¦ â exercise life risk thing study find time\n",
            "\n",
            "------ Topic 4 ------\n",
            "depression anxiety face cry emoji mental know help go °\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute Perplexity\n",
        "# a measure of how good the model is. lower the better\n",
        "base_perplexity = base_model.log_perplexity(corpus)\n",
        "print('\\nPerplexity: ', base_perplexity) \n",
        "\n",
        "# Compute Coherence Score\n",
        "coherence_model = CoherenceModel(model=base_model, texts=df['lemma_tokens'], \n",
        "                                   dictionary=id2word, coherence='c_v')\n",
        "coherence_lda_model_base = coherence_model.get_coherence()\n",
        "print('\\nCoherence Score: ', coherence_lda_model_base)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gEa1eznFo__o",
        "outputId": "6e6ba839-c7bc-4edc-f90f-64822abd5dc4"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Perplexity:  -6.916992636798057\n",
            "\n",
            "Coherence Score:  0.41482318772818705\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating Topic Distance Visualization \n",
        "pyLDAvis.enable_notebook()\n",
        "pyLDAvis.gensim.prepare(base_model, corpus, id2word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 951
        },
        "id": "2OawjbvfpJK9",
        "outputId": "bccd637a-0033-460c-8ac8-5e39be555486"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "/usr/local/lib/python3.9/dist-packages/pyLDAvis/_prepare.py:243: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
            "  default_term_info = default_term_info.sort_values(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
              "topic                                                \n",
              "4     -0.026003 -0.024697       1        1  24.801446\n",
              "2     -0.015469  0.092750       2        1  19.968720\n",
              "0     -0.034642 -0.093406       3        1  19.643356\n",
              "3      0.148938 -0.002692       4        1  18.097949\n",
              "1     -0.072824  0.028045       5        1  17.488529, topic_info=       Term        Freq       Total Category  logprob  loglift\n",
              "52       â  347.000000  347.000000  Default  30.0000  30.0000\n",
              "51        ¦  360.000000  360.000000  Default  29.0000  29.0000\n",
              "35    emoji  179.000000  179.000000  Default  28.0000  28.0000\n",
              "849       °   40.000000   40.000000  Default  27.0000  27.0000\n",
              "142       u   45.000000   45.000000  Default  26.0000  26.0000\n",
              "..      ...         ...         ...      ...      ...      ...\n",
              "117    help   21.823379  130.196213   Topic5  -5.1373  -0.0424\n",
              "19   people   22.796480  149.864584   Topic5  -5.0936  -0.1395\n",
              "368    year   17.366752   68.642819   Topic5  -5.3657   0.3693\n",
              "21     time   16.127433  100.201375   Topic5  -5.4397  -0.0830\n",
              "20     tell   15.143704   49.606351   Topic5  -5.5027   0.5571\n",
              "\n",
              "[378 rows x 6 columns], token_table=      Topic      Freq      Term\n",
              "term                           \n",
              "775       4  0.889131      acne\n",
              "1035      2  0.215665       act\n",
              "1035      3  0.754828       act\n",
              "195       1  0.157418  activity\n",
              "195       4  0.629673  activity\n",
              "...     ...       ...       ...\n",
              "52        1  0.060445        â\n",
              "52        2  0.014392        â\n",
              "52        3  0.100741        â\n",
              "52        4  0.817442        â\n",
              "52        5  0.002878        â\n",
              "\n",
              "[784 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[5, 3, 1, 4, 2])"
            ],
            "text/html": [
              "\n",
              "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
              "\n",
              "\n",
              "<div id=\"ldavis_el147140186278043424809616255\" style=\"background-color:white;\"></div>\n",
              "<script type=\"text/javascript\">\n",
              "\n",
              "var ldavis_el147140186278043424809616255_data = {\"mdsDat\": {\"x\": [-0.02600260907790838, -0.01546924386313212, -0.034642145426214285, 0.14893759881730326, -0.0728236004500484], \"y\": [-0.024696749271087663, 0.09274993805655199, -0.0934060259637583, -0.0026916735607947612, 0.028044510739088698], \"topics\": [1, 2, 3, 4, 5], \"cluster\": [1, 1, 1, 1, 1], \"Freq\": [24.801445725615945, 19.96872033715766, 19.643356198782868, 18.09794860244722, 17.488529135996302]}, \"tinfo\": {\"Term\": [\"\\u00e2\\u0080\", \"\\u00a6\", \"emoji\", \"\\u00b0\", \"u\", \"kid\", \"anxiety\", \"red\", \"heavy\", \"exercise\", \"bad\", \"cry\", \"intellectual\", \"skin\", \"emotional\", \"lose\", \"development\", \"day\", \"tie\", \"mom\", \"face\", \"risk\", \"cure\", \"people\", \"loudly\", \"think\", \"puff\", \"cannabis\", \"say\", \"depression\", \"\\u00b0\", \"\\u00a4\\u00eb\\u0091\\u0090\\u00ec\\u0084\\u00b8\\u00ec\\u009a\\u0094\", \"\\u0095\\u00eb\\u00b4\\u0084\\u00ec\", \"\\u009c\\u00eb\\u0082\\u00b4\\u00eb\\u00b2\\u0084\\u00eb\", \"\\u009c\\u00eb\", \"you\", \"app\", \"bom\", \"phrase\", \"concert\", \"st\", \"loudly\", \"harmful\", \"obsession\", \"asshole\", \"deepika\", \"potentially\", \"blend\", \"pls\", \"program\", \"govt\", \"dead\", \"hurt\", \"drink\", \"padukone\", \"plz\", \"speech\", \"election\", \"strength\", \"divorce\", \"student\", \"cry\", \"rain\", \"summer\", \"treat\", \"fight\", \"problem\", \"suck\", \"depression\", \"child\", \"faceemoji\", \"health\", \"mental\", \"face\", \"friend\", \"severe\", \"anxiety\", \"sure\", \"time\", \"talk\", \"help\", \"know\", \"go\", \"emoji\", \"love\", \"way\", \"good\", \"post\", \"struggle\", \"stop\", \"feel\", \"thing\", \"people\", \"year\", \"\\u00a6\", \"\\u00e2\\u0080\", \"intellectual\", \"vaastu\", \"tie\", \"development\", \"ketamine\", \"finish\", \"playlist\", \"hock\", \"shape\", \"stage\", \"villainous\", \"nighttime\", \"sm\", \"emotional\", \"worst\", \"productivity\", \"diet\", \"silent\", \"depressionme\", \"dish\", \"anchor\", \"zicutake\", \"farright\", \"turkey\", \"kid\", \"overall\", \"ride\", \"particular\", \"poor\", \"shoutout\", \"u\", \"mom\", \"bowl\", \"second\", \"say\", \"wake\", \"fall\", \"set\", \"video\", \"character\", \"state\", \"cure\", \"depression\", \"war\", \"bout\", \"feel\", \"great\", \"sleep\", \"show\", \"condition\", \"maybe\", \"life\", \"hour\", \"crippling\", \"people\", \"fuck\", \"big\", \"know\", \"come\", \"love\", \"work\", \"anxiety\", \"go\", \"cause\", \"think\", \"help\", \"time\", \"live\", \"good\", \"hard\", \"red\", \"attempt\", \"toneemoji\", \"skype\", \"heartemoji\", \"tone\", \"index\", \"backhand\", \"incredible\", \"nude\", \"sort\", \"skin\", \"jesus\", \"son\", \"form\", \"encourage\", \"yo\", \"heavy\", \"judge\", \"education\", \"airplane\", \"feed\", \"insurance\", \"bts\", \"steve\", \"follower\", \"cough\", \"blossomemoji\", \"tale\", \"quiet\", \"porn\", \"sign\", \"light\", \"panic\", \"act\", \"practice\", \"therapy\", \"hand\", \"medium\", \"point\", \"emoji\", \"people\", \"understand\", \"let\", \"ago\", \"support\", \"person\", \"raise\", \"heart\", \"depression\", \"anxiety\", \"bring\", \"smile\", \"thought\", \"good\", \"go\", \"happy\", \"need\", \"face\", \"help\", \"know\", \"mental\", \"love\", \"\\u00a6\", \"\\u00e2\\u0080\", \"feel\", \"come\", \"try\", \"work\", \"health\", \"cannabis\", \"puff\", \"ease\\u00e2\", \"researcher\", \"\\u00e2\\u0080\", \"journal\", \"acne\", \"properly\", \"\\u00a6\", \"minutesaday\", \"diary\", \"bateman\", \"fever\", \"sleepy\", \"site\", \"adolescent\", \"crying\", \"reserve\", \"tms\", \"metaanalysis\", \"contain\", \"hotlineshare\", \"inflammation\", \"bubble\", \"ease\", \"seizure\", \"hotline\", \"identify\", \"tour\", \"essential\", \"engage\", \"risk\", \"exercise\", \"age\", \"find\", \"develop\", \"millennial\", \"study\", \"regularly\", \"check\", \"evidence\", \"activity\", \"cut\", \"depression\", \"disorder\", \"teen\", \"bipolar\", \"life\", \"great\", \"rt\", \"depressionanxiety\", \"include\", \"thing\", \"cure\", \"time\", \"real\", \"suffer\", \"day\", \"cause\", \"go\", \"eat\", \"come\", \"help\", \"anxiety\", \"know\", \"mental\", \"good\", \"feel\", \"marry\", \"cold\", \"creep\", \"weary\", \"cole\", \"j\", \"realize\", \"t\", \"completely\", \"queen\", \"effort\", \"sweat\", \"lazy\", \"dear\", \"girlfriend\", \"variety\", \"silence\", \"stream\", \"attract\", \"differently\", \"smoke\", \"rule\", \"uncertainty\", \"heck\", \"technically\", \"endorphin\", \"nightmare\", \"anime\", \"bus\", \"um\", \"etc\", \"tired\", \"adhd\", \"lose\", \"sweatemoji\", \"involve\", \"believe\", \"depressionemoji\", \"alive\", \"lately\", \"bad\", \"seriously\", \"day\", \"think\", \"sad\", \"brain\", \"thank\", \"anxiety\", \"thing\", \"take\", \"not\", \"depression\", \"tear\", \"sadness\", \"face\", \"shit\", \"nap\", \"man\", \"have\", \"get\", \"break\", \"try\", \"suffer\", \"feel\", \"cause\", \"emoji\", \"know\", \"good\", \"life\", \"help\", \"people\", \"year\", \"time\", \"tell\"], \"Freq\": [347.0, 360.0, 179.0, 40.0, 45.0, 46.0, 286.0, 30.0, 34.0, 43.0, 71.0, 64.0, 28.0, 34.0, 36.0, 44.0, 32.0, 104.0, 29.0, 43.0, 157.0, 40.0, 71.0, 149.0, 30.0, 100.0, 19.0, 17.0, 44.0, 1926.0, 38.499579270526944, 18.127520088577747, 19.713885732277788, 19.691995934660497, 19.689926322567217, 11.997043223818041, 7.95321932179864, 15.772901431769544, 6.31168814935025, 8.666751521264446, 6.277309524756325, 26.587976553073094, 5.435410463295103, 6.204465803794493, 5.41189024808921, 6.17657940634795, 6.130660884803566, 6.115348576953976, 5.324287506209661, 3.6814887423591482, 4.413798040400357, 3.6672051065547357, 11.024693580064762, 3.667399554401081, 4.393968960574223, 3.632970630995169, 3.621596045384109, 3.5981190748885243, 3.57770910512394, 3.556984154384281, 7.7886716655797645, 49.685740624676505, 6.303671622839562, 5.596761691352129, 14.138872143109179, 23.410477919736746, 18.65578029448517, 14.877408372734537, 563.3044923400004, 22.643865138619525, 22.97605812179558, 34.92981860449746, 44.215109338386746, 61.41282767095223, 24.37575137940346, 16.275736201616976, 84.98341038964892, 15.712339208534928, 36.72388889850257, 26.480219322589853, 41.61485704663335, 44.09809712649764, 40.738283701707736, 49.033789506088155, 30.264463937340203, 23.47038503161522, 29.282510900898014, 20.075439726851858, 20.849662350182836, 18.386635031936127, 26.336477585929934, 21.97695687086623, 23.181873499650354, 20.43913203433558, 22.453728268501624, 21.12294438331785, 25.945193905401297, 7.269952289037936, 25.658902423938297, 27.370693203759735, 4.516989377185307, 4.507925781730181, 5.990770891045272, 3.7285595455434235, 4.465884171730562, 4.445255997418788, 3.661400189823747, 3.6628635416836826, 3.6573575753622207, 29.938618409030273, 3.6486350006493447, 4.3440234591662925, 4.32212158102503, 3.5650029355350377, 2.840848229149779, 2.840816374207165, 2.840672409621552, 2.8400711167125166, 2.8392910410247785, 2.837975110876011, 36.22072324609809, 2.8028998558073135, 2.7997607858033478, 2.798419421087997, 3.490400388495385, 2.774331404908258, 34.98469220765805, 33.252628843581924, 5.5084692803636, 7.908351020530402, 27.975745489898557, 9.422524107750078, 13.388030840896963, 8.55185888971812, 13.712837664293453, 6.233349983115414, 12.811450453079196, 32.689409752159264, 435.5998112378777, 12.595250710619156, 6.694689559931713, 49.78403430363562, 27.32130767133108, 18.18018568175903, 11.358394900796704, 8.646074450844067, 13.01311347645542, 33.476942953594694, 11.207369877050217, 7.943153903492871, 33.46802159416241, 12.767761275815067, 10.649747378026065, 29.141428962044564, 19.82505441283639, 20.642654729098982, 18.973455736115493, 32.63237705800288, 22.626928243166482, 17.47793482901535, 19.674001732085465, 20.48931836998142, 15.78746463636685, 13.407774978816809, 14.424071116979254, 13.295605083058364, 28.136718805376272, 7.705028051719311, 8.33042038812148, 6.584391364542274, 17.297048130462393, 16.259643015875806, 5.690215155340855, 4.8169587011913855, 4.794621785770497, 3.931462781435233, 6.269663706425962, 28.8719417991133, 7.778526059691421, 4.647024314722834, 3.8731573447614753, 3.871281319816443, 3.863736896309668, 28.434224345591133, 3.8570319836419498, 3.764290813155845, 3.0103019829461246, 3.009223201801377, 3.0073238571108147, 2.9792896363602885, 2.958631256789481, 2.9452151156896305, 2.940707846124037, 2.938029719098401, 2.9288818201724682, 2.923868803625707, 7.048920209658773, 19.693015951753562, 21.053145939688928, 10.658417952787056, 6.708303261552603, 6.605483102717502, 15.706028370384928, 21.203255141661316, 17.401578889797676, 16.866693886591115, 82.98472142415065, 61.18737827532016, 15.662955828736166, 25.84322766250168, 8.130230089312699, 17.955955313642516, 20.355590164918173, 10.520478743012795, 18.57703203963766, 268.1746172409261, 69.97628332201374, 12.453796416186671, 19.64662326133506, 16.045619249687807, 32.22770953683759, 35.00849979885399, 15.68262700574623, 23.290125565897412, 35.77173175028389, 32.055209492532775, 31.825230162122388, 26.120921135577234, 23.33300669130162, 40.301471834007046, 35.079912632913356, 24.45639267931096, 20.096299857689523, 19.793384136511857, 19.215139953117735, 19.039682187754952, 16.427414036758737, 17.66654235919025, 3.765710785665131, 8.865289356223524, 284.39703602719766, 3.6740378742306308, 3.659454859711908, 3.6563798041642217, 288.74914468719106, 3.599598702053339, 2.8693123988356235, 2.8692660715795557, 2.8672160345477646, 3.57657383198449, 2.8544702202467245, 4.990627159232803, 2.8459912133704766, 3.5410403789741753, 2.8263618849879086, 2.8026318111322506, 2.802400398655081, 2.7983021150168543, 2.7956428857579305, 2.7792264756754057, 13.811811177254459, 2.6832948137302117, 11.381795753163988, 5.360433249876307, 3.947860156790469, 1.9724664056672965, 3.3003217184143727, 28.79228669665819, 30.640225885621675, 11.568006256601432, 23.79206375273379, 12.817841322657495, 5.096195114243261, 23.81010049776284, 7.532028525525701, 12.51874987184597, 6.528878712525898, 7.965431420001285, 11.286769822645395, 350.47447418901197, 17.279210689956628, 11.206974145411456, 10.746205494593157, 30.035273385730324, 19.89009667386243, 7.584435170887306, 7.0063806787146055, 8.921995708075825, 24.196565601124508, 19.064128696839752, 21.597421226698604, 13.921105902641925, 15.72567472301188, 19.104359554572802, 15.375370840550719, 18.905450459303065, 12.186882894480762, 13.562623357157863, 14.213449240289416, 15.699872903216999, 14.327342673201906, 13.005599648814574, 12.738033112446068, 12.881489153612199, 6.780865704478292, 13.35779512998819, 4.881049167969756, 11.27309154050459, 3.9884548869844227, 3.988292254233725, 10.269801450770206, 4.71045000769099, 3.916078555050773, 3.8849218398086687, 3.8140727851162817, 4.538982751851982, 3.027609998779624, 3.7399221990779425, 2.990920309742495, 2.984726424130804, 2.983086039271702, 2.9688584850560487, 2.906912059595689, 2.892912667018315, 5.703856697549177, 2.1269571784904207, 2.126900949081935, 2.1268654813011976, 2.1268535866186333, 2.1268384479317333, 2.1266494306124377, 2.1264837701243597, 2.126269665838201, 2.1257681427678974, 17.39519901466143, 7.545509966453683, 4.840325235140711, 29.971094807345512, 6.095280423368158, 4.783328511495811, 10.438669610335426, 4.7428095997407835, 5.343354388524482, 5.387527346763277, 39.73997354488062, 6.435643953476629, 48.808912065853015, 43.5336246739026, 18.475201588791556, 15.055027921069376, 16.662718406873953, 83.64299948050169, 36.57607794981691, 24.64456425784005, 18.56425859101954, 308.9651599001855, 14.47288918035271, 7.921653566783544, 43.64316648217595, 17.40257804319044, 16.00938285420011, 14.684977858483755, 13.496727617543275, 17.640139508433474, 12.26026974203817, 20.66476091415207, 20.223496738252813, 31.48945146921879, 19.232648053946825, 29.977357898383033, 26.217079355831416, 22.16539752674184, 22.190979312399513, 21.823379121888237, 22.796480074740387, 17.36675212437444, 16.12743346957913, 15.1437042934553], \"Total\": [347.0, 360.0, 179.0, 40.0, 45.0, 46.0, 286.0, 30.0, 34.0, 43.0, 71.0, 64.0, 28.0, 34.0, 36.0, 44.0, 32.0, 104.0, 29.0, 43.0, 157.0, 40.0, 71.0, 149.0, 30.0, 100.0, 19.0, 17.0, 44.0, 1926.0, 40.410215056450575, 19.343890660338136, 21.101621421530606, 21.10248425444136, 21.102054055915115, 13.219150407993427, 8.808526416182875, 17.616190930412735, 7.052161823310547, 9.69099070760048, 7.054008109478713, 30.05724877746104, 6.175209439071549, 7.059591669997404, 6.175620884210593, 7.053784717954305, 7.056075851806872, 7.054501522413445, 6.176379298344022, 4.421237808221095, 5.301361352287487, 4.421548846249649, 13.294728113358104, 4.4225480903674645, 5.300288233936846, 4.422973537226261, 4.4234803730179095, 4.427162115807438, 4.422326356580883, 4.423666261017088, 9.755876292375314, 64.04180594458606, 7.9398340492352935, 7.067678741101547, 20.59871493225178, 36.78648503520251, 28.73764224754321, 23.107857746962853, 1926.5185549080015, 38.418328185471026, 39.964087980997604, 69.40897871818548, 100.65806311809753, 157.59626498800097, 47.605234337946385, 27.901362447458776, 286.9349431533842, 27.89984101613746, 100.20137484552575, 64.12476588548299, 130.1962132713252, 145.60917827969791, 129.6474901620266, 179.73034767626206, 86.01777954950022, 56.159624717224915, 110.83772219390276, 46.79177511655962, 51.75075231612162, 38.30207328803689, 144.9478451917075, 100.33779651860598, 149.86458409754408, 68.64281944707149, 360.19615024851834, 347.4253116796743, 28.450543112864906, 8.018194341720363, 29.35406153183245, 32.05689822383231, 5.35980586948171, 5.3556403204568275, 7.12634956445821, 4.465896725997779, 5.353467217178173, 5.357325812490912, 4.464970757418619, 4.469835995067924, 4.467601409125554, 36.62086143991851, 4.46616555554026, 5.350278315630778, 5.360726869198327, 4.465721476474066, 3.577812858717683, 3.5778022939543024, 3.5777973555658398, 3.577758897469279, 3.5777592411617207, 3.577752479498703, 46.20954766631575, 3.5799082631365367, 3.581460899263936, 3.5803743035735254, 4.479993822994402, 3.5768608223146314, 45.241520351923164, 43.729967483696484, 7.208286928058632, 10.774408296100022, 44.54951562218325, 13.45411304444325, 19.87978869172998, 12.587846976678371, 21.766617832393337, 8.951955009924049, 21.57497212599263, 71.29085284329031, 1926.5185549080015, 22.5837590127173, 9.838485392601433, 144.9478451917075, 67.91483984574313, 39.604103119428146, 20.800338726053326, 14.477648846345623, 26.83149291107774, 112.80616458583395, 22.026311335416885, 13.323797630938301, 149.86458409754408, 30.716591726608996, 22.38986063531158, 145.60917827969791, 71.29857081346704, 86.01777954950022, 72.72572308386022, 286.9349431533842, 129.6474901620266, 68.42696695114367, 100.07839634497506, 130.1962132713252, 100.20137484552575, 42.97154669790147, 110.83772219390276, 46.85602668201887, 30.012950172677463, 8.442445478291324, 9.367567177701243, 7.488522182727222, 19.73438937749502, 18.69035108602869, 6.551494063409553, 5.61543232823948, 5.6133705734990205, 4.6786439302046, 7.481998399430091, 34.514327788343635, 9.337438316812808, 5.604967895934364, 4.673476828457583, 4.674045473598049, 4.672991831191759, 34.42526875141907, 4.672069069510819, 4.66475714436495, 3.736901937820238, 3.7367109615896603, 3.7367163342817844, 3.7351649328864207, 3.734710056376639, 3.7325431778367406, 3.7335650508261438, 3.7330268069674246, 3.7321713647795742, 3.730767436690927, 9.38556605763137, 27.824276639968385, 30.629559661758627, 14.905092351233057, 9.273630410939356, 9.255984112850854, 24.921040478892976, 35.22502328617149, 28.546004328763914, 27.651925260829625, 179.73034767626206, 149.86458409754408, 27.440781612830335, 52.089584327078704, 12.019802699292033, 34.80617018279574, 41.87458935744933, 17.431689014951477, 39.67092235947163, 1926.5185549080015, 286.9349431533842, 22.896420020541214, 47.94370420471079, 35.31736006938827, 110.83772219390276, 129.6474901620266, 34.147013681011195, 68.74772805197512, 157.59626498800097, 130.1962132713252, 145.60917827969791, 100.65806311809753, 86.01777954950022, 360.19615024851834, 347.4253116796743, 144.9478451917075, 71.29857081346704, 66.58430849568396, 72.72572308386022, 69.40897871818548, 17.988544776772663, 19.779268386045818, 4.501273142268746, 10.756251955189606, 347.4253116796743, 4.4998276206321215, 4.498772121226917, 4.500046789447079, 360.19615024851834, 4.504524210667913, 3.6043626765053833, 3.6043561770769257, 3.6040904101685483, 4.497159596716037, 3.6040167457587313, 6.306731616422214, 3.6035998419563557, 4.498775531258348, 3.604745131164669, 3.603160628564219, 3.6036124182647846, 3.602705563035744, 3.603301927810396, 3.6021103570843622, 17.988169065326947, 3.599859493677557, 15.321240423536103, 7.215831383047135, 5.381772790440244, 2.7073994264098302, 4.534602954926445, 40.48076847796243, 43.95247032082724, 16.272745159579287, 41.78748637418256, 20.89575071892555, 7.25419962256195, 44.88071410059228, 11.7293948584713, 21.830576626130178, 9.881492790263305, 12.704997467253385, 20.827484363838966, 1926.5185549080015, 38.66684538501568, 24.149692974069957, 23.657321431274852, 112.80616458583395, 67.91483984574313, 14.432455597464534, 12.701329516341357, 18.931090390286087, 100.33779651860598, 71.29085284329031, 100.20137484552575, 50.02728680195806, 64.6607297744707, 104.93864678900522, 68.42696695114367, 129.6474901620266, 42.44913096065605, 71.29857081346704, 130.1962132713252, 286.9349431533842, 145.60917827969791, 100.65806311809753, 110.83772219390276, 144.9478451917075, 7.667003232611264, 15.306982937906138, 5.7413917300786625, 13.435950184983247, 4.775949098422616, 4.775936760977282, 12.39700716705013, 5.725103354190888, 4.776795454105798, 4.766044474822815, 4.763120444090228, 5.702939382840193, 3.809310078869084, 4.753059200021596, 3.8051402125819243, 3.8066172412486536, 3.811642652406325, 3.808161145442564, 3.797145838997547, 3.796261951126227, 7.566597654981708, 2.8477674342457577, 2.847764668569946, 2.847755152660604, 2.8477542269794616, 2.847750873600898, 2.847718788943152, 2.8476846307102823, 2.847651530478103, 2.847567881203098, 23.738429412344914, 10.398407024570632, 6.608651390466583, 44.41228210371198, 8.503584991000501, 6.6074627049209615, 15.065633336776404, 6.615620676321013, 7.547745843280664, 7.616065621029224, 71.30727067832257, 9.40358467990362, 104.93864678900522, 100.07839634497506, 35.36030049848844, 28.913846007569738, 33.54555234319061, 286.9349431533842, 100.33779651860598, 59.19718264220468, 40.80256260055744, 1926.5185549080015, 29.73623451521925, 13.074089288198302, 157.59626498800097, 43.436137450552486, 38.67627740000407, 34.243900862711904, 30.917691477564617, 48.22473158527282, 26.995403140216816, 66.58430849568396, 64.6607297744707, 144.9478451917075, 68.42696695114367, 179.73034767626206, 145.60917827969791, 110.83772219390276, 112.80616458583395, 130.1962132713252, 149.86458409754408, 68.64281944707149, 100.20137484552575, 49.60635084143999], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -4.919, -5.6722, -5.5883, -5.5894, -5.5895, -6.0849, -6.496, -5.8113, -6.7272, -6.4101, -6.7327, -5.2891, -6.8767, -6.7443, -6.881, -6.7488, -6.7563, -6.7588, -6.8973, -7.2663, -7.0849, -7.2702, -6.1695, -7.2701, -7.0894, -7.2796, -7.2827, -7.2892, -7.2949, -7.3007, -6.5169, -4.6639, -6.7285, -6.8474, -5.9207, -5.4164, -5.6435, -5.8698, -2.2358, -5.4497, -5.4352, -5.0163, -4.7805, -4.452, -5.376, -5.7799, -4.1272, -5.8152, -4.9662, -5.2932, -4.8412, -4.7832, -4.8624, -4.6771, -5.1596, -5.4139, -5.1926, -5.5701, -5.5323, -5.658, -5.2987, -5.4796, -5.4262, -5.5522, -5.4582, -5.5192, -5.0969, -6.3691, -5.108, -5.0434, -6.845, -6.847, -6.5627, -7.0369, -6.8564, -6.861, -7.055, -7.0546, -7.0561, -4.9537, -7.0585, -6.8841, -6.8891, -7.0817, -7.3088, -7.3088, -7.3088, -7.309, -7.3093, -7.3098, -4.7632, -7.3222, -7.3233, -7.3238, -7.1029, -7.3325, -4.798, -4.8487, -6.6466, -6.285, -5.0215, -6.1098, -5.7585, -6.2067, -5.7345, -6.523, -5.8025, -4.8658, -2.2761, -5.8196, -6.4516, -4.4452, -5.0452, -5.4525, -5.9229, -6.1958, -5.7869, -4.842, -5.9363, -6.2806, -4.8423, -5.806, -5.9873, -4.9807, -5.3659, -5.3255, -5.4098, -4.8676, -5.2337, -5.4919, -5.3736, -5.333, -5.5937, -5.757, -5.684, -5.7654, -4.9994, -6.2946, -6.2165, -6.4517, -5.4859, -5.5478, -6.5977, -6.7643, -6.769, -6.9674, -6.5007, -4.9736, -6.2851, -6.8002, -6.9824, -6.9829, -6.9848, -4.9889, -6.9865, -7.0109, -7.2344, -7.2348, -7.2354, -7.2448, -7.2517, -7.2563, -7.2578, -7.2587, -7.2618, -7.2635, -6.3836, -5.3562, -5.2894, -5.9701, -6.4331, -6.4485, -5.5824, -5.2823, -5.4799, -5.5111, -3.9178, -4.2225, -5.5851, -5.0844, -6.2409, -5.4485, -5.3231, -5.9831, -5.4145, -2.7448, -4.0883, -5.8144, -5.3585, -5.561, -4.8636, -4.7809, -5.5839, -5.1884, -4.7593, -4.869, -4.8762, -5.0737, -5.1866, -4.6401, -4.7788, -5.1396, -5.3359, -5.3511, -5.3807, -5.3899, -5.4556, -5.3828, -6.9286, -6.0724, -2.6041, -6.9532, -6.9572, -6.958, -2.5889, -6.9737, -7.2004, -7.2004, -7.2012, -6.9801, -7.2056, -6.6469, -7.2086, -6.9901, -7.2155, -7.2239, -7.224, -7.2255, -7.2264, -7.2323, -5.629, -7.2675, -5.8225, -6.5755, -6.8813, -7.5752, -7.0605, -4.8944, -4.8322, -5.8063, -5.0852, -5.7037, -6.626, -5.0844, -6.2353, -5.7273, -6.3783, -6.1794, -5.8309, -2.3952, -5.405, -5.838, -5.88, -4.8521, -5.2643, -6.2284, -6.3077, -6.066, -5.0683, -5.3067, -5.1819, -5.6211, -5.4992, -5.3046, -5.5217, -5.3151, -5.7541, -5.6472, -5.6003, -5.5009, -5.5923, -5.6891, -5.7099, -5.6987, -6.3061, -5.6282, -6.6349, -5.7978, -6.8368, -6.8369, -5.891, -6.6705, -6.8552, -6.8631, -6.8816, -6.7075, -7.1125, -6.9012, -7.1247, -7.1267, -7.1273, -7.1321, -7.1532, -7.158, -6.4791, -7.4656, -7.4656, -7.4656, -7.4656, -7.4656, -7.4657, -7.4658, -7.4659, -7.4661, -5.3641, -6.1993, -6.6433, -4.82, -6.4127, -6.6551, -5.8747, -6.6636, -6.5444, -6.5362, -4.5379, -6.3584, -4.3323, -4.4467, -5.3038, -5.5085, -5.4071, -3.7937, -4.6209, -5.0157, -5.299, -2.487, -5.548, -6.1507, -4.4442, -5.3636, -5.4471, -5.5334, -5.6178, -5.3501, -5.7139, -5.1918, -5.2134, -4.7706, -5.2636, -4.8198, -4.9538, -5.1217, -5.1206, -5.1373, -5.0936, -5.3657, -5.4397, -5.5027], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 1.3458, 1.3293, 1.3262, 1.3251, 1.325, 1.2973, 1.2921, 1.2837, 1.2833, 1.2826, 1.2776, 1.2716, 1.2667, 1.2652, 1.2623, 1.2615, 1.2537, 1.2514, 1.2458, 1.2112, 1.211, 1.2072, 1.207, 1.207, 1.2067, 1.1975, 1.1943, 1.1869, 1.1823, 1.1762, 1.1691, 1.1405, 1.1635, 1.1609, 1.018, 0.9423, 0.9622, 0.9539, 0.1646, 0.8656, 0.8407, 0.7076, 0.5716, 0.4519, 0.7249, 0.8553, 0.1775, 0.8201, 0.3905, 0.5098, 0.2537, 0.1998, 0.2366, 0.0953, 0.3497, 0.5218, 0.0632, 0.5481, 0.4852, 0.6604, -0.3112, -0.1243, -0.4721, 0.1828, -1.3809, -1.4059, 1.5188, 1.513, 1.4765, 1.453, 1.4399, 1.4387, 1.4374, 1.4306, 1.4297, 1.4244, 1.4126, 1.4119, 1.4109, 1.4095, 1.4088, 1.4027, 1.3956, 1.3857, 1.3804, 1.3803, 1.3803, 1.3801, 1.3798, 1.3794, 1.3674, 1.3663, 1.3648, 1.3646, 1.3614, 1.3569, 1.3539, 1.3371, 1.3421, 1.3017, 1.1457, 1.2548, 1.2157, 1.2244, 1.149, 1.249, 1.0898, 0.8313, 0.1243, 1.0271, 1.226, 0.5423, 0.7004, 0.8324, 1.006, 1.0955, 0.8874, 0.3962, 0.9353, 1.0938, 0.1119, 0.7331, 0.8679, 0.0022, 0.3311, 0.1838, 0.2673, -0.5629, -0.1347, 0.2462, -0.0157, -0.2381, -0.237, 0.4463, -0.4282, 0.3514, 1.5629, 1.536, 1.5101, 1.4988, 1.4956, 1.4881, 1.4865, 1.4741, 1.4698, 1.4534, 1.4507, 1.4489, 1.4448, 1.44, 1.4396, 1.439, 1.4373, 1.4362, 1.4357, 1.413, 1.4112, 1.4109, 1.4103, 1.4013, 1.3945, 1.3905, 1.3887, 1.388, 1.3851, 1.3837, 1.3411, 1.2818, 1.2525, 1.2921, 1.3036, 1.2901, 1.1658, 1.1198, 1.1325, 1.1331, 0.8546, 0.7316, 1.0667, 0.9265, 1.2365, 0.9656, 0.9061, 1.1225, 0.8687, -0.3444, 0.2163, 1.0185, 0.7353, 0.8385, 0.3922, 0.3182, 0.8493, 0.545, 0.1446, 0.2258, 0.1068, 0.2784, 0.3227, -0.5628, -0.6655, -0.1521, 0.3611, 0.4143, 0.2964, 0.3339, 1.6186, 1.5964, 1.5309, 1.516, 1.5092, 1.5066, 1.5029, 1.5018, 1.4883, 1.4851, 1.4813, 1.4813, 1.4806, 1.4803, 1.4762, 1.4753, 1.4733, 1.47, 1.4661, 1.4581, 1.4579, 1.4567, 1.4556, 1.45, 1.4452, 1.4155, 1.4121, 1.4121, 1.3995, 1.3927, 1.3917, 1.3687, 1.3486, 1.3681, 1.1461, 1.2207, 1.3563, 1.0755, 1.2664, 1.1533, 1.2949, 1.2425, 1.0967, 0.0052, 0.9039, 0.9416, 0.9203, 0.3861, 0.4813, 1.066, 1.1145, 0.9571, 0.287, 0.3904, 0.1748, 0.4302, 0.2955, 0.0059, 0.2164, -0.216, 0.4614, 0.0498, -0.5055, -1.1962, -0.6094, -0.337, -0.4541, -0.7112, 1.6208, 1.6074, 1.5813, 1.5681, 1.5634, 1.5634, 1.5554, 1.5485, 1.5449, 1.5392, 1.5214, 1.5153, 1.514, 1.5039, 1.5029, 1.5004, 1.4985, 1.4947, 1.4765, 1.4719, 1.461, 1.4518, 1.4518, 1.4517, 1.4517, 1.4517, 1.4517, 1.4516, 1.4515, 1.4513, 1.4327, 1.4229, 1.4322, 1.3503, 1.4107, 1.4206, 1.3767, 1.4108, 1.3982, 1.3975, 1.159, 1.3644, 0.9782, 0.9112, 1.0945, 1.091, 1.0439, 0.5109, 0.7345, 0.8673, 0.9561, -0.0866, 1.0235, 1.2426, 0.4596, 0.829, 0.8616, 0.8969, 0.9147, 0.7379, 0.9543, 0.5736, 0.5813, 0.2169, 0.4745, -0.0474, 0.0291, 0.1341, 0.1176, -0.0424, -0.1395, 0.3693, -0.083, 0.5571]}, \"token.table\": {\"Topic\": [4, 2, 3, 1, 4, 5, 1, 5, 2, 4, 2, 3, 4, 1, 3, 4, 3, 2, 5, 2, 5, 1, 2, 3, 4, 5, 1, 1, 3, 5, 3, 1, 2, 3, 4, 5, 4, 1, 3, 5, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 3, 1, 3, 4, 1, 2, 3, 2, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 3, 4, 5, 4, 5, 1, 2, 3, 4, 5, 1, 2, 5, 2, 3, 4, 5, 1, 3, 4, 5, 1, 5, 5, 1, 2, 3, 4, 5, 5, 1, 2, 3, 4, 5, 4, 3, 5, 1, 2, 4, 1, 2, 3, 4, 5, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 5, 1, 1, 2, 3, 4, 5, 1, 3, 4, 2, 5, 2, 1, 3, 4, 5, 1, 2, 4, 5, 4, 2, 5, 2, 1, 2, 3, 4, 5, 1, 1, 1, 3, 4, 4, 1, 2, 3, 4, 5, 3, 5, 1, 1, 2, 3, 4, 5, 1, 2, 3, 4, 3, 5, 4, 5, 4, 1, 3, 4, 5, 1, 2, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 3, 4, 5, 1, 2, 3, 5, 2, 3, 1, 2, 3, 4, 5, 4, 1, 3, 4, 5, 1, 2, 3, 4, 5, 2, 3, 3, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 5, 1, 2, 3, 4, 5, 1, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 3, 5, 1, 3, 5, 1, 2, 3, 4, 5, 2, 1, 3, 4, 4, 1, 2, 4, 5, 1, 3, 1, 3, 4, 1, 2, 4, 5, 3, 3, 4, 3, 1, 2, 4, 1, 5, 5, 2, 3, 4, 3, 2, 1, 2, 4, 5, 1, 2, 3, 4, 5, 3, 5, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 3, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 4, 1, 4, 5, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 5, 2, 1, 2, 3, 5, 3, 1, 2, 1, 1, 2, 3, 4, 5, 2, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 1, 2, 1, 1, 1, 2, 3, 4, 5, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 1, 2, 3, 1, 2, 3, 4, 5, 2, 1, 4, 2, 3, 4, 5, 3, 1, 2, 1, 2, 3, 4, 1, 2, 3, 4, 5, 2, 4, 5, 1, 3, 1, 3, 4, 5, 1, 4, 4, 2, 1, 2, 3, 4, 5, 1, 3, 4, 5, 1, 2, 3, 5, 1, 4, 5, 1, 2, 3, 4, 5, 2, 3, 4, 1, 2, 5, 1, 2, 3, 4, 5, 1, 2, 3, 5, 2, 1, 2, 3, 4, 5, 2, 2, 3, 4, 5, 1, 2, 3, 4, 5, 2, 4, 1, 2, 3, 4, 3, 1, 2, 3, 4, 5, 4, 2, 1, 2, 3, 4, 5, 1, 5, 3, 3, 1, 1, 2, 1, 2, 3, 4, 5, 3, 1, 2, 3, 4, 5, 1, 1, 2, 3, 4, 5, 1, 3, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 4, 1, 2, 3, 4, 5, 1, 3, 4, 5, 1, 5, 1, 3, 5, 5, 1, 2, 3, 4, 5, 3, 1, 2, 3, 4, 5, 1, 2, 3, 5, 5, 1, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 4, 1, 2, 3, 4, 5, 2, 4, 5, 4, 2, 3, 3, 1, 4, 1, 3, 4, 5, 1, 2, 3, 4, 5, 2, 1, 2, 3, 5, 5, 1, 2, 3, 2, 5, 2, 3, 4, 5, 2, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 5, 3, 5, 1, 2, 3, 4, 5, 2, 1, 2, 3, 4, 5, 3, 1, 2, 1, 4, 1, 4, 1, 4, 1, 4, 1, 2, 3, 4, 5, 1, 4, 1, 2, 3, 4, 5], \"Freq\": [0.8891314990431454, 0.21566526930389213, 0.7548284425636225, 0.1574183706179335, 0.629673482471734, 0.1574183706179335, 0.15131680291724364, 0.7565840145862182, 0.1585607349131651, 0.7928036745658256, 0.06145244641844153, 0.1843573392553246, 0.7374293570212984, 0.08319604115123289, 0.6655683292098631, 0.16639208230246577, 0.8028040472878777, 0.13248988781070895, 0.6624494390535448, 0.8385047284282373, 0.702324961981886, 0.29623439747651203, 0.1150086484320576, 0.243957739098304, 0.055761768936755204, 0.2927492869179648, 0.9082109335907236, 0.8096351919502798, 0.9475927348978426, 0.7900670996592549, 0.8904033933158576, 0.04207144617178569, 0.12621433851535707, 0.12621433851535707, 0.15426196929654754, 0.5609526156238092, 0.8323261777178057, 0.19912870125922702, 0.06637623375307566, 0.6637623375307568, 0.2679784433556168, 0.4912938128186309, 0.17865229557041123, 0.04466307389260281, 0.12681063698251216, 0.1690808493100162, 0.0845404246550081, 0.46497233560254453, 0.21135106163752024, 0.8505207605295568, 0.8036374114433673, 0.908255369347608, 0.0567659605842255, 0.0567659605842255, 0.10164166130205395, 0.7114916291143776, 0.10164166130205395, 0.8323753008006226, 0.13872921680010378, 0.034585506187526795, 0.24209854331268754, 0.06917101237505359, 0.13834202475010718, 0.5187825928129018, 0.03704334381694173, 0.18521671908470866, 0.1111300314508252, 0.2222600629016504, 0.4445201258033008, 0.13102485005553666, 0.21837475009256108, 0.5240994002221466, 0.08734990003702443, 0.8031773841059522, 0.8328451109499806, 0.7023331255928679, 0.8894549391599297, 0.055590933697495605, 0.17536945643912447, 0.24844006328875967, 0.05845648547970816, 0.2192118205489056, 0.27766830602861375, 0.11170744255209168, 0.6702446553125501, 0.11170744255209168, 0.1374219312379106, 0.1832292416505475, 0.5954950353642793, 0.09161462082527375, 0.5986725890039666, 0.05205848600034492, 0.2602924300017246, 0.10411697200068984, 0.0653296605906318, 0.8492855876782134, 0.8375298642360126, 0.23843395184562383, 0.28051053158308686, 0.28051053158308686, 0.1963573721081608, 0.014025526579154343, 0.8373814701573375, 0.9286976194231051, 0.6216478998433324, 0.2762879554859255, 0.06907198887148137, 0.06907198887148137, 0.8324979636529733, 0.803521556249884, 0.8708689870097917, 0.30021470685744034, 0.6004294137148807, 0.07505367671436008, 0.7807400066647696, 0.046844400399886174, 0.031229600266590784, 0.031229600266590784, 0.10930360093306773, 0.8325008690119523, 0.21040567480617195, 0.4628924845735783, 0.05610817994831252, 0.26651385475448447, 0.01402704498707813, 0.0960269596203579, 0.0960269596203579, 0.24006739905089475, 0.5281482779119685, 0.04801347981017895, 0.16199942080615, 0.08576439925031472, 0.10482315463927354, 0.18105817619510883, 0.4669395070294912, 0.9046603665574777, 0.841563260979755, 0.8506071903113146, 0.29223699847878465, 0.2263149757313501, 0.13911104012844455, 0.18167486583938652, 0.16039295298391554, 0.15746383065070688, 0.31492766130141375, 0.551123407277474, 0.15115739685306534, 0.7557869842653268, 0.83850109507271, 0.09571323983055435, 0.14356985974583153, 0.6221360588986034, 0.09571323983055435, 0.031194533950778872, 0.8422524166710296, 0.09358360185233662, 0.031194533950778872, 0.8323246768576174, 0.7461674690018636, 0.7902510518564184, 0.8385035710523577, 0.07758584829272293, 0.02586194943090764, 0.1810336460163535, 0.43965314032542996, 0.25861949430907644, 0.9042273453694767, 0.9044559648118251, 0.1111841896046605, 0.1111841896046605, 0.7782893272326236, 0.8886374751264055, 0.0942304332144608, 0.1413456498216912, 0.1413456498216912, 0.2826912996433824, 0.353364124554228, 0.8574937292999315, 0.8397856083952152, 0.9035133332293771, 0.27263064158903694, 0.0723305783807649, 0.46180292350796054, 0.027819453223371118, 0.1669167193402267, 0.027306839890717364, 0.8192051967215209, 0.0819205196721521, 0.05461367978143473, 0.8557897056403319, 0.7023086248661417, 0.6615794215766488, 0.22052647385888294, 0.7387162679029289, 0.08425157221900795, 0.08425157221900795, 0.08425157221900795, 0.7161383638615676, 0.10119928448314475, 0.10119928448314475, 0.7083949913820132, 0.10119928448314475, 0.04550369945992054, 0.20476664756964244, 0.02275184972996027, 0.7053073416287684, 0.02275184972996027, 0.3870650107389563, 0.07614393653881107, 0.22843180961643322, 0.03172664022450462, 0.27919443397564064, 0.5755166991659161, 0.025022465181126784, 0.05004493036225357, 0.350314512535775, 0.05030234553831054, 0.6539304919980371, 0.20120938215324216, 0.10060469107662108, 0.8385136611444769, 0.8028450770845142, 0.1793748638733642, 0.34495166129493116, 0.16557679742156695, 0.0896874319366821, 0.21387003000285731, 0.8323875537461066, 0.6252296183772478, 0.08155168935355406, 0.10873558580473873, 0.16310337870710812, 0.0957224362380243, 0.04786121811901215, 0.16751426341654255, 0.5743346174281458, 0.0957224362380243, 0.9335951820553753, 0.8037415394987343, 0.8558938338248153, 0.5041462421889492, 0.1680487473963164, 0.21006093424539551, 0.0420121868490791, 0.06301828027361865, 0.26044556216404063, 0.42322403851656604, 0.13022278108202032, 0.03255569527050508, 0.1627784763525254, 0.2073624812678867, 0.10368124063394335, 0.24883497752146405, 0.06220874438036601, 0.3732524662821961, 0.7884072156080661, 0.3162421420481057, 0.1774041284660105, 0.2699628041874073, 0.14655123655887825, 0.09255867572139678, 0.2616437745740259, 0.1263107877253918, 0.2887103719437527, 0.11728858860214954, 0.19848838071133, 0.7545231751225632, 0.20614051408791642, 0.3975567057409817, 0.10307025704395821, 0.29448644869702345, 0.02838890955091508, 0.2555001859582357, 0.5961671005692167, 0.02838890955091508, 0.11355563820366033, 0.11714055107033734, 0.263566239908259, 0.46856220428134937, 0.17571082660550602, 0.32012957696552385, 0.2774456333701207, 0.17073577438161272, 0.12805183078620955, 0.10670985898850796, 0.8096891367544866, 0.06468788271114283, 0.16171970677785705, 0.2910954722001427, 0.06468788271114283, 0.42047123762242833, 0.5042575275758933, 0.04322207379221942, 0.2737398006840563, 0.11525886344591844, 0.05762943172295922, 0.27728117587802176, 0.025207379625274706, 0.47894021288021943, 0.05041475925054941, 0.17645165737692295, 0.05067296387393643, 0.8614403858569193, 0.05067296387393643, 0.14524214861195212, 0.8133560322269319, 0.7023075695715756, 0.32259002734951436, 0.15361429873786397, 0.24578287798058235, 0.10753000911650477, 0.16897572861165036, 0.8956767801445996, 0.13053773354588513, 0.13053773354588513, 0.7179575345023682, 0.832707515923703, 0.045400248129248255, 0.49940272942173075, 0.045400248129248255, 0.40860223316323424, 0.8273956342851091, 0.15043556987001983, 0.1385841695732246, 0.1385841695732246, 0.6929208478661231, 0.15846947735981223, 0.21129263647974963, 0.47540843207943667, 0.15846947735981223, 0.8907304327288188, 0.9158216342605456, 0.8325696985994726, 0.8028439227449721, 0.035148713893894526, 0.9138665612412578, 0.035148713893894526, 0.15134402487890578, 0.7567201243945288, 0.8375320277862086, 0.10709575432476161, 0.8567660345980929, 0.8889229404387924, 0.8561517264595605, 0.9328696079217318, 0.12984329652665424, 0.7790597791599255, 0.06492164826332712, 0.021640549421109042, 0.3021787535637433, 0.19916326939428536, 0.21976636622817694, 0.09614778522482742, 0.17856017256039378, 0.1313013896885071, 0.6565069484425354, 0.7875441846127281, 0.019197695910200444, 0.23037235092240532, 0.49914009366521156, 0.03839539182040089, 0.2111746550122049, 0.12410669267411739, 0.29253720416041956, 0.11524192891168043, 0.2659429128731087, 0.19502480277361306, 0.13059280133870316, 0.03264820033467579, 0.6856122070281915, 0.06529640066935158, 0.06529640066935158, 0.18616970099404595, 0.30252576411532467, 0.18616970099404595, 0.09308485049702298, 0.23271212624255744, 0.022516293976175116, 0.15761405783322582, 0.09006517590470046, 0.06754888192852535, 0.6754888192852535, 0.8982858078563207, 0.06653968947083858, 0.03326984473541929, 0.34876510597133065, 0.24413557417993142, 0.26738658124468684, 0.034876510597133065, 0.0930040282590215, 0.11680912218606467, 0.14601140273258084, 0.0876068416395485, 0.23361824437212933, 0.4380342081977425, 0.9130033974977088, 0.33542673267666856, 0.4845052805329657, 0.03726963696407429, 0.03726963696407429, 0.11180891089222285, 0.1401246897440377, 0.1401246897440377, 0.5955299314121602, 0.07006234487201884, 0.03503117243600942, 0.4371234517832595, 0.04967311952082495, 0.25830022150828974, 0.12915011075414487, 0.12915011075414487, 0.8326023481210814, 0.13785118304296567, 0.6892559152148284, 0.13785118304296567, 0.8879961152227652, 0.09147045447704232, 0.7546312494355991, 0.02286761361926058, 0.06860284085778173, 0.04573522723852116, 0.2327007821078213, 0.2327007821078213, 0.07756692736927377, 0.02585564245642459, 0.41369027930279345, 0.20364309333125347, 0.13091341714152008, 0.3345565104727735, 0.14545935237946675, 0.16000528761741342, 0.7023165376319485, 0.8948874196757225, 0.24508264585968387, 0.14704958751581032, 0.14704958751581032, 0.46565702713339935, 0.8549485833227488, 0.8499075131355588, 0.8380103006805962, 0.7546759390156706, 0.06709116431051652, 0.06709116431051652, 0.7380028074156817, 0.06709116431051652, 0.06709116431051652, 0.8379012208320619, 0.15347188355742358, 0.2201987894519556, 0.4070341259566452, 0.0600542153050788, 0.15347188355742358, 0.14328498719791322, 0.09552332479860881, 0.477616623993044, 0.023880831199652203, 0.238808311996522, 0.8508029382092904, 0.14032429800909244, 0.8419457880545547, 0.8095357746796045, 0.9043689649810756, 0.1808192360147435, 0.07232769440589741, 0.6147854024501279, 0.07232769440589741, 0.036163847202948705, 0.669643780444951, 0.7458260862495688, 0.1065465837499384, 0.1065465837499384, 0.42742554541218925, 0.1709702181648757, 0.12822766362365676, 0.14959894089426623, 0.12822766362365676, 0.8503309950194995, 0.10803821482489546, 0.10803821482489546, 0.7562675037742682, 0.6611537521532169, 0.03479756590280089, 0.17398782951400446, 0.03479756590280089, 0.10439269770840268, 0.7476246587610302, 0.9047240102222455, 0.88887964662507, 0.050557987306825536, 0.050557987306825536, 0.9100437715228596, 0.8392703889211413, 0.8041240980330056, 0.7556833005316875, 0.12594721675528125, 0.28683393764720155, 0.057366787529440307, 0.6310346628238434, 0.057366787529440307, 0.2798472772553406, 0.11993454739514597, 0.05996727369757299, 0.2798472772553406, 0.2598581860228163, 0.08066463030350494, 0.08066463030350494, 0.8066463030350495, 0.03331895046126982, 0.932930612915555, 0.17051178037164838, 0.08525589018582419, 0.6820471214865935, 0.08525589018582419, 0.09296918705195693, 0.8367226834676124, 0.8891308250894581, 0.8376470061746484, 0.0741092650361415, 0.1235154417269025, 0.0741092650361415, 0.7163895620160345, 0.049406176690761, 0.20786483490217939, 0.20786483490217939, 0.5543062264058117, 0.702304540725148, 0.19796211857134052, 0.16968181591829187, 0.08484090795914594, 0.5090454477548756, 0.22946148935268748, 0.152974326235125, 0.6118973049405, 0.1795754653731057, 0.62851412880587, 0.06734079951491465, 0.11223466585819107, 0.022446933171638213, 0.7425001707885652, 0.1856250426971413, 0.8333658592144799, 0.10634242515379244, 0.10634242515379244, 0.6380545509227546, 0.07944170292606113, 0.7149753263345503, 0.07944170292606113, 0.07944170292606113, 0.07944170292606113, 0.573448699149717, 0.07168108739371462, 0.2150432621811439, 0.10752163109057195, 0.7471793209389279, 0.2762676587820625, 0.11511152449252604, 0.16115613428953646, 0.023022304898505208, 0.39137918327458854, 0.8387242750078999, 0.52883754177628, 0.19230456064592, 0.19230456064592, 0.04807614016148, 0.035939838183018304, 0.07187967636603661, 0.718796763660366, 0.1796991909150915, 0.787062238928949, 0.8957119294323347, 0.8324045673568113, 0.05794694922829848, 0.08692042384244772, 0.8402307638103279, 0.02897347461414924, 0.9347638731906235, 0.3029989080629702, 0.4544983620944553, 0.05049981801049503, 0.025249909005247515, 0.1767493630367326, 0.8894503105740169, 0.8953350206734136, 0.3128669394411571, 0.020857795962743807, 0.4171559192548761, 0.04171559192548761, 0.20857795962743805, 0.13215979567006825, 0.7929587740204095, 0.8920657696588796, 0.8019247906357511, 0.9042653437322723, 0.850580252656301, 0.7466411676276569, 0.09270000388971456, 0.6025500252831446, 0.09270000388971456, 0.13905000583457183, 0.09270000388971456, 0.8032752087080506, 0.46994845069188584, 0.3132989671279239, 0.1305412363033016, 0.05221649452132065, 0.787781789011283, 0.9045013138950233, 0.40579120225577836, 0.15458712466886795, 0.21255729641969345, 0.057970171750825486, 0.17391051525247644, 0.8200185980476592, 0.1025023247559574, 0.28965671024892276, 0.06684385621128987, 0.022281285403763288, 0.5347508496903189, 0.08912514161505315, 0.6491298399122049, 0.17310129064325463, 0.08655064532162732, 0.04327532266081366, 0.04327532266081366, 0.27837607250617125, 0.15465337361453957, 0.015465337361453958, 0.24744539778326333, 0.30930674722907914, 0.8489350209294966, 0.1414891701549161, 0.20111376699123346, 0.17238322884962867, 0.517149686548886, 0.08619161442481434, 0.02873053814160478, 0.5734799703964438, 0.17921249074888868, 0.14336999259911096, 0.10752749444933321, 0.1753481727350883, 0.8767408636754415, 0.11759746048970149, 0.11759746048970149, 0.7055847629382089, 0.8733466787704194, 0.23649774153303518, 0.11824887076651759, 0.13514156659030582, 0.08446347911894114, 0.4223173955947057, 0.8038216112772686, 0.40545956996446614, 0.17154058729265875, 0.1871351861374459, 0.09356759306872295, 0.12475679075829726, 0.26903204559762045, 0.06725801139940511, 0.13451602279881023, 0.4708060797958357, 0.7023077978612458, 0.41408393931704285, 0.12422518179511285, 0.4554923332487471, 0.26206321931546117, 0.22174580095923638, 0.20158709178112397, 0.020158709178112397, 0.302380637671686, 0.11924084477958992, 0.1490510559744874, 0.11924084477958992, 0.11924084477958992, 0.5067735903132572, 0.08025347102557423, 0.6420277682045938, 0.2808871485895098, 0.21925934955050028, 0.069764338493341, 0.09966334070477285, 0.23919201769145484, 0.3687543606076596, 0.13989033109344923, 0.1998433301334989, 0.11990599808009933, 0.09992166506674945, 0.4396553262936976, 0.02831468711238022, 0.2831468711238022, 0.45303499379808354, 0.02831468711238022, 0.22651749689904177, 0.034066835995269995, 0.8857377358770199, 0.10220050798580999, 0.3692564104738144, 0.1596784477724603, 0.09979902985778769, 0.2195578656871329, 0.1596784477724603, 0.19233715272677387, 0.09616857636338694, 0.7693486109070955, 0.8322363692410952, 0.10700708567722032, 0.8560566854177626, 0.8540104221556447, 0.185812377991193, 0.743249511964772, 0.6796540486163992, 0.14564015327494267, 0.04854671775831423, 0.09709343551662845, 0.19524119561657188, 0.1802226421076048, 0.30037107017934134, 0.015018553508967067, 0.3153896236883084, 0.8385152458675245, 0.19893230665086215, 0.7736256369755751, 0.022103589627873572, 0.702353757113948, 0.7023052227852572, 0.2915368852416173, 0.1093263319656065, 0.5830737704832346, 0.8730145094610039, 0.7881013009377151, 0.6431867416335594, 0.13782573035004844, 0.04594191011668282, 0.13782573035004844, 0.8958625301977482, 0.07432671307998374, 0.6689404177198537, 0.1486534261599675, 0.07432671307998374, 0.08855921633213344, 0.5756349061588674, 0.3099572571624671, 0.08855921633213344, 0.40954689629444035, 0.17806386795410448, 0.17806386795410448, 0.053419160386231346, 0.17806386795410448, 0.07442718871625877, 0.8186990758788465, 0.19250410179980698, 0.26125556672830946, 0.26125556672830946, 0.165003515828406, 0.110002343885604, 0.8956228671456248, 0.2913633233760368, 0.1456816616880184, 0.20395432636322577, 0.08740899701281105, 0.2476588248696313, 0.8559826647460402, 0.9077739211397259, 0.8385137416951277, 0.9477944656704632, 0.04738972328352316, 0.9477750339850827, 0.047388751699254134, 0.947755712495825, 0.04738778562479125, 0.9305263515010664, 0.05169590841672591, 0.06107783213346683, 0.019433855678830356, 0.1110506038790306, 0.8023406130259961, 0.00555253019395153, 0.9403562922621507, 0.02474621821742502, 0.060444646069316835, 0.014391582397456389, 0.10074107678219472, 0.8174418801755229, 0.002878316479491278], \"Term\": [\"acne\", \"act\", \"act\", \"activity\", \"activity\", \"activity\", \"adhd\", \"adhd\", \"adolescent\", \"adolescent\", \"age\", \"age\", \"age\", \"ago\", \"ago\", \"ago\", \"airplane\", \"alive\", \"alive\", \"anchor\", \"anime\", \"anxiety\", \"anxiety\", \"anxiety\", \"anxiety\", \"anxiety\", \"app\", \"asshole\", \"attempt\", \"attract\", \"backhand\", \"bad\", \"bad\", \"bad\", \"bad\", \"bad\", \"bateman\", \"believe\", \"believe\", \"believe\", \"big\", \"big\", \"big\", \"big\", \"bipolar\", \"bipolar\", \"bipolar\", \"bipolar\", \"bipolar\", \"blend\", \"blossomemoji\", \"bom\", \"bom\", \"bom\", \"bout\", \"bout\", \"bout\", \"bowl\", \"bowl\", \"brain\", \"brain\", \"brain\", \"brain\", \"brain\", \"break\", \"break\", \"break\", \"break\", \"break\", \"bring\", \"bring\", \"bring\", \"bring\", \"bts\", \"bubble\", \"bus\", \"cannabis\", \"cannabis\", \"cause\", \"cause\", \"cause\", \"cause\", \"cause\", \"character\", \"character\", \"character\", \"check\", \"check\", \"check\", \"check\", \"child\", \"child\", \"child\", \"child\", \"cold\", \"cold\", \"cole\", \"come\", \"come\", \"come\", \"come\", \"come\", \"completely\", \"concert\", \"condition\", \"condition\", \"condition\", \"condition\", \"contain\", \"cough\", \"creep\", \"crippling\", \"crippling\", \"crippling\", \"cry\", \"cry\", \"cry\", \"cry\", \"cry\", \"crying\", \"cure\", \"cure\", \"cure\", \"cure\", \"cure\", \"cut\", \"cut\", \"cut\", \"cut\", \"cut\", \"day\", \"day\", \"day\", \"day\", \"day\", \"dead\", \"dear\", \"deepika\", \"depression\", \"depression\", \"depression\", \"depression\", \"depression\", \"depressionanxiety\", \"depressionanxiety\", \"depressionanxiety\", \"depressionemoji\", \"depressionemoji\", \"depressionme\", \"develop\", \"develop\", \"develop\", \"develop\", \"development\", \"development\", \"development\", \"development\", \"diary\", \"diet\", \"differently\", \"dish\", \"disorder\", \"disorder\", \"disorder\", \"disorder\", \"disorder\", \"divorce\", \"drink\", \"ease\", \"ease\", \"ease\", \"ease\\u00e2\", \"eat\", \"eat\", \"eat\", \"eat\", \"eat\", \"education\", \"effort\", \"election\", \"emoji\", \"emoji\", \"emoji\", \"emoji\", \"emoji\", \"emotional\", \"emotional\", \"emotional\", \"emotional\", \"encourage\", \"endorphin\", \"engage\", \"engage\", \"essential\", \"etc\", \"etc\", \"etc\", \"etc\", \"evidence\", \"evidence\", \"evidence\", \"evidence\", \"exercise\", \"exercise\", \"exercise\", \"exercise\", \"exercise\", \"face\", \"face\", \"face\", \"face\", \"face\", \"faceemoji\", \"faceemoji\", \"faceemoji\", \"faceemoji\", \"fall\", \"fall\", \"fall\", \"fall\", \"farright\", \"feed\", \"feel\", \"feel\", \"feel\", \"feel\", \"feel\", \"fever\", \"fight\", \"fight\", \"fight\", \"fight\", \"find\", \"find\", \"find\", \"find\", \"find\", \"finish\", \"follower\", \"form\", \"friend\", \"friend\", \"friend\", \"friend\", \"friend\", \"fuck\", \"fuck\", \"fuck\", \"fuck\", \"fuck\", \"get\", \"get\", \"get\", \"get\", \"get\", \"girlfriend\", \"go\", \"go\", \"go\", \"go\", \"go\", \"good\", \"good\", \"good\", \"good\", \"good\", \"govt\", \"great\", \"great\", \"great\", \"great\", \"hand\", \"hand\", \"hand\", \"hand\", \"hand\", \"happy\", \"happy\", \"happy\", \"happy\", \"hard\", \"hard\", \"hard\", \"hard\", \"hard\", \"harmful\", \"have\", \"have\", \"have\", \"have\", \"have\", \"health\", \"health\", \"health\", \"health\", \"health\", \"heart\", \"heart\", \"heart\", \"heart\", \"heart\", \"heartemoji\", \"heartemoji\", \"heartemoji\", \"heavy\", \"heavy\", \"heck\", \"help\", \"help\", \"help\", \"help\", \"help\", \"hock\", \"hotline\", \"hotline\", \"hotline\", \"hotlineshare\", \"hour\", \"hour\", \"hour\", \"hour\", \"hurt\", \"hurt\", \"identify\", \"identify\", \"identify\", \"include\", \"include\", \"include\", \"include\", \"incredible\", \"index\", \"inflammation\", \"insurance\", \"intellectual\", \"intellectual\", \"intellectual\", \"involve\", \"involve\", \"j\", \"jesus\", \"jesus\", \"journal\", \"judge\", \"ketamine\", \"kid\", \"kid\", \"kid\", \"kid\", \"know\", \"know\", \"know\", \"know\", \"know\", \"lately\", \"lately\", \"lazy\", \"let\", \"let\", \"let\", \"let\", \"let\", \"life\", \"life\", \"life\", \"life\", \"life\", \"light\", \"light\", \"light\", \"light\", \"light\", \"live\", \"live\", \"live\", \"live\", \"live\", \"lose\", \"lose\", \"lose\", \"lose\", \"lose\", \"loudly\", \"loudly\", \"loudly\", \"love\", \"love\", \"love\", \"love\", \"love\", \"man\", \"man\", \"man\", \"man\", \"man\", \"marry\", \"maybe\", \"maybe\", \"maybe\", \"maybe\", \"maybe\", \"medium\", \"medium\", \"medium\", \"medium\", \"medium\", \"mental\", \"mental\", \"mental\", \"mental\", \"mental\", \"metaanalysis\", \"millennial\", \"millennial\", \"millennial\", \"minutesaday\", \"mom\", \"mom\", \"mom\", \"mom\", \"mom\", \"nap\", \"nap\", \"nap\", \"nap\", \"nap\", \"need\", \"need\", \"need\", \"need\", \"need\", \"nightmare\", \"nighttime\", \"not\", \"not\", \"not\", \"not\", \"nude\", \"obsession\", \"overall\", \"padukone\", \"panic\", \"panic\", \"panic\", \"panic\", \"panic\", \"particular\", \"people\", \"people\", \"people\", \"people\", \"people\", \"person\", \"person\", \"person\", \"person\", \"person\", \"phrase\", \"playlist\", \"playlist\", \"pls\", \"plz\", \"point\", \"point\", \"point\", \"point\", \"point\", \"poor\", \"porn\", \"porn\", \"porn\", \"post\", \"post\", \"post\", \"post\", \"post\", \"potentially\", \"practice\", \"practice\", \"practice\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"productivity\", \"program\", \"properly\", \"puff\", \"puff\", \"puff\", \"queen\", \"quiet\", \"rain\", \"rain\", \"raise\", \"raise\", \"raise\", \"raise\", \"real\", \"real\", \"real\", \"real\", \"real\", \"realize\", \"realize\", \"realize\", \"red\", \"red\", \"regularly\", \"regularly\", \"regularly\", \"regularly\", \"researcher\", \"researcher\", \"reserve\", \"ride\", \"risk\", \"risk\", \"risk\", \"risk\", \"risk\", \"rt\", \"rt\", \"rt\", \"rule\", \"sad\", \"sad\", \"sad\", \"sad\", \"sadness\", \"sadness\", \"sadness\", \"say\", \"say\", \"say\", \"say\", \"say\", \"second\", \"second\", \"seizure\", \"seriously\", \"seriously\", \"seriously\", \"set\", \"set\", \"set\", \"set\", \"set\", \"severe\", \"severe\", \"severe\", \"severe\", \"shape\", \"shit\", \"shit\", \"shit\", \"shit\", \"shit\", \"shoutout\", \"show\", \"show\", \"show\", \"show\", \"sign\", \"sign\", \"sign\", \"sign\", \"silence\", \"silent\", \"site\", \"skin\", \"skin\", \"skin\", \"skin\", \"skype\", \"sleep\", \"sleep\", \"sleep\", \"sleep\", \"sleep\", \"sleepy\", \"sm\", \"smile\", \"smile\", \"smile\", \"smile\", \"smile\", \"smoke\", \"smoke\", \"son\", \"sort\", \"speech\", \"st\", \"stage\", \"state\", \"state\", \"state\", \"state\", \"state\", \"steve\", \"stop\", \"stop\", \"stop\", \"stop\", \"stream\", \"strength\", \"struggle\", \"struggle\", \"struggle\", \"struggle\", \"struggle\", \"student\", \"student\", \"study\", \"study\", \"study\", \"study\", \"study\", \"suck\", \"suck\", \"suck\", \"suck\", \"suck\", \"suffer\", \"suffer\", \"suffer\", \"suffer\", \"suffer\", \"summer\", \"summer\", \"support\", \"support\", \"support\", \"support\", \"support\", \"sure\", \"sure\", \"sure\", \"sure\", \"sweat\", \"sweat\", \"sweatemoji\", \"sweatemoji\", \"sweatemoji\", \"t\", \"take\", \"take\", \"take\", \"take\", \"take\", \"tale\", \"talk\", \"talk\", \"talk\", \"talk\", \"talk\", \"tear\", \"tear\", \"tear\", \"tear\", \"technically\", \"teen\", \"teen\", \"teen\", \"tell\", \"tell\", \"tell\", \"tell\", \"tell\", \"thank\", \"thank\", \"thank\", \"thank\", \"thank\", \"therapy\", \"therapy\", \"therapy\", \"thing\", \"thing\", \"thing\", \"thing\", \"thing\", \"think\", \"think\", \"think\", \"think\", \"think\", \"thought\", \"thought\", \"thought\", \"thought\", \"thought\", \"tie\", \"tie\", \"tie\", \"time\", \"time\", \"time\", \"time\", \"time\", \"tired\", \"tired\", \"tired\", \"tms\", \"tone\", \"tone\", \"toneemoji\", \"tour\", \"tour\", \"treat\", \"treat\", \"treat\", \"treat\", \"try\", \"try\", \"try\", \"try\", \"try\", \"turkey\", \"u\", \"u\", \"u\", \"um\", \"uncertainty\", \"understand\", \"understand\", \"understand\", \"vaastu\", \"variety\", \"video\", \"video\", \"video\", \"video\", \"villainous\", \"wake\", \"wake\", \"wake\", \"wake\", \"war\", \"war\", \"war\", \"war\", \"way\", \"way\", \"way\", \"way\", \"way\", \"weary\", \"weary\", \"work\", \"work\", \"work\", \"work\", \"work\", \"worst\", \"year\", \"year\", \"year\", \"year\", \"year\", \"yo\", \"you\", \"zicutake\", \"\\u0095\\u00eb\\u00b4\\u0084\\u00ec\", \"\\u0095\\u00eb\\u00b4\\u0084\\u00ec\", \"\\u009c\\u00eb\", \"\\u009c\\u00eb\", \"\\u009c\\u00eb\\u0082\\u00b4\\u00eb\\u00b2\\u0084\\u00eb\", \"\\u009c\\u00eb\\u0082\\u00b4\\u00eb\\u00b2\\u0084\\u00eb\", \"\\u00a4\\u00eb\\u0091\\u0090\\u00ec\\u0084\\u00b8\\u00ec\\u009a\\u0094\", \"\\u00a4\\u00eb\\u0091\\u0090\\u00ec\\u0084\\u00b8\\u00ec\\u009a\\u0094\", \"\\u00a6\", \"\\u00a6\", \"\\u00a6\", \"\\u00a6\", \"\\u00a6\", \"\\u00b0\", \"\\u00b0\", \"\\u00e2\\u0080\", \"\\u00e2\\u0080\", \"\\u00e2\\u0080\", \"\\u00e2\\u0080\", \"\\u00e2\\u0080\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [5, 3, 1, 4, 2]};\n",
              "\n",
              "function LDAvis_load_lib(url, callback){\n",
              "  var s = document.createElement('script');\n",
              "  s.src = url;\n",
              "  s.async = true;\n",
              "  s.onreadystatechange = s.onload = callback;\n",
              "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
              "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
              "}\n",
              "\n",
              "if(typeof(LDAvis) !== \"undefined\"){\n",
              "   // already loaded: just create the visualization\n",
              "   !function(LDAvis){\n",
              "       new LDAvis(\"#\" + \"ldavis_el147140186278043424809616255\", ldavis_el147140186278043424809616255_data);\n",
              "   }(LDAvis);\n",
              "}else if(typeof define === \"function\" && define.amd){\n",
              "   // require.js is available: use it to load d3/LDAvis\n",
              "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
              "   require([\"d3\"], function(d3){\n",
              "      window.d3 = d3;\n",
              "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
              "        new LDAvis(\"#\" + \"ldavis_el147140186278043424809616255\", ldavis_el147140186278043424809616255_data);\n",
              "      });\n",
              "    });\n",
              "}else{\n",
              "    // require.js not available: dynamically load d3 & LDAvis\n",
              "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
              "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
              "                 new LDAvis(\"#\" + \"ldavis_el147140186278043424809616255\", ldavis_el147140186278043424809616255_data);\n",
              "            })\n",
              "         });\n",
              "}\n",
              "</script>"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models.ldamodel import LdaModel\n",
        "from gensim.models.ldamulticore import LdaMulticore\n",
        "from gensim.corpora import Dictionary\n",
        "from gensim.models import CoherenceModel\n",
        "\n",
        "# Load the corpus and dictionary\n",
        "corpus = ...\n",
        "dictionary = ...\n",
        "\n",
        "# Find the best number of topics\n",
        "coherence_scores = []\n",
        "for num_topics in range(2, 20):\n",
        "    lda_model = LdaModel(corpus=corpus,\n",
        "                         id2word=dictionary,\n",
        "                         num_topics=num_topics,\n",
        "                         random_state=42,\n",
        "                         passes=10)\n",
        "    coherence_model_lda = CoherenceModel(model=lda_model, texts=df1['lemma_tokens'], dictionary=dictionary, coherence='c_v')\n",
        "    coherence_score = coherence_model_lda.get_coherence()\n",
        "    coherence_scores.append(coherence_score)\n",
        "\n",
        "best_num_topics = coherence_scores.index(max(coherence_scores)) + 2\n",
        "\n",
        "# Train the LDA model with the best number of topics\n",
        "lda_model = LdaModel(corpus=corpus,\n",
        "                     id2word=dictionary,\n",
        "                     num_topics=best_num_topics,\n",
        "                     random_state=42,\n",
        "                     passes=10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "id": "Tr2Lt0etpMN9",
        "outputId": "e24e287a-c609-4124-862e-1c405b447ed2"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-f5212e11081d>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mcoherence_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnum_topics\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     lda_model = LdaModel(corpus=corpus,\n\u001b[0m\u001b[1;32m     14\u001b[0m                          \u001b[0mid2word\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                          \u001b[0mnum_topics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_topics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, corpus, num_topics, id2word, distributed, chunksize, passes, update_every, alpha, eta, decay, offset, eval_every, iterations, gamma_threshold, minimum_probability, random_state, ns_conf, minimum_phi_value, per_word_topics, callbacks, dtype)\u001b[0m\n\u001b[1;32m    440\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid2word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdict_from_corpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_terms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid2word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 442\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid2word\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    443\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_terms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid2word\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: object of type 'ellipsis' has no len()"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u58-w-KLvx9n"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}