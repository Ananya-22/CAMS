{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ananya-22/CAMS/blob/main/Basic_code%2B1CAMS_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Importing and installation"
      ],
      "metadata": {
        "id": "t_jTHNmjessZ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ty9n-AC7EIub",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fca8208d-f37f-47d2-adf5-ec1cb30ac7c7"
      },
      "source": [
        "# Installations\n",
        "import sys\n",
        "if 'google.colab' in sys.modules:\n",
        "    !pip install emoji --upgrade\n",
        "    !pip install pandas-profiling==2.*\n",
        "    !pip install plotly==4.*\n",
        "    !python -m spacy download en_core_web_lg\n",
        "    !pip install pyldavis\n",
        "    !pip install gensim\n",
        "    !pip install chart_studio\n",
        "    !pip install --upgrade autopep8"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting emoji\n",
            "  Downloading emoji-2.2.0.tar.gz (240 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.9/240.9 KB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-2.2.0-py3-none-any.whl size=234926 sha256=206a230554c4b8287a98c9871bbd96425a803691369a7c130f0131e122a840e6\n",
            "  Stored in directory: /root/.cache/pip/wheels/9a/b8/0f/f580817231cbf59f6ade9fd132ff60ada1de9f7dc85521f857\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji\n",
            "Successfully installed emoji-2.2.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pandas-profiling==2.*\n",
            "  Downloading pandas_profiling-2.13.0-py2.py3-none-any.whl (245 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.2/245.2 KB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tangled-up-in-unicode>=0.0.6 in /usr/local/lib/python3.9/dist-packages (from pandas-profiling==2.*) (0.2.0)\n",
            "Requirement already satisfied: requests>=2.24.0 in /usr/local/lib/python3.9/dist-packages (from pandas-profiling==2.*) (2.27.1)\n",
            "Requirement already satisfied: matplotlib>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from pandas-profiling==2.*) (3.7.1)\n",
            "Requirement already satisfied: jinja2>=2.11.1 in /usr/local/lib/python3.9/dist-packages (from pandas-profiling==2.*) (3.1.2)\n",
            "Collecting confuse>=1.0.0\n",
            "  Downloading confuse-2.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from pandas-profiling==2.*) (1.1.1)\n",
            "Requirement already satisfied: attrs>=19.3.0 in /usr/local/lib/python3.9/dist-packages (from pandas-profiling==2.*) (22.2.0)\n",
            "Requirement already satisfied: seaborn>=0.10.1 in /usr/local/lib/python3.9/dist-packages (from pandas-profiling==2.*) (0.12.2)\n",
            "Requirement already satisfied: tqdm>=4.48.2 in /usr/local/lib/python3.9/dist-packages (from pandas-profiling==2.*) (4.65.0)\n",
            "Requirement already satisfied: phik>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from pandas-profiling==2.*) (0.12.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.9/dist-packages (from pandas-profiling==2.*) (1.10.1)\n",
            "Requirement already satisfied: missingno>=0.4.2 in /usr/local/lib/python3.9/dist-packages (from pandas-profiling==2.*) (0.5.2)\n",
            "Requirement already satisfied: pandas!=1.0.0,!=1.0.1,!=1.0.2,!=1.1.0,>=0.25.3 in /usr/local/lib/python3.9/dist-packages (from pandas-profiling==2.*) (1.4.4)\n",
            "Collecting visions[type_image_path]==0.7.1\n",
            "  Downloading visions-0.7.1-py3-none-any.whl (102 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.0/103.0 KB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: htmlmin>=0.1.12 in /usr/local/lib/python3.9/dist-packages (from pandas-profiling==2.*) (0.1.12)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.9/dist-packages (from pandas-profiling==2.*) (1.22.4)\n",
            "Requirement already satisfied: networkx>=2.4 in /usr/local/lib/python3.9/dist-packages (from visions[type_image_path]==0.7.1->pandas-profiling==2.*) (3.0)\n",
            "Collecting bottleneck\n",
            "  Downloading Bottleneck-1.3.7-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (353 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m353.1/353.1 KB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multimethod==1.4\n",
            "  Downloading multimethod-1.4-py2.py3-none-any.whl (7.3 kB)\n",
            "Requirement already satisfied: imagehash in /usr/local/lib/python3.9/dist-packages (from visions[type_image_path]==0.7.1->pandas-profiling==2.*) (4.3.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.9/dist-packages (from visions[type_image_path]==0.7.1->pandas-profiling==2.*) (8.4.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.9/dist-packages (from confuse>=1.0.0->pandas-profiling==2.*) (6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2>=2.11.1->pandas-profiling==2.*) (2.1.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.2.0->pandas-profiling==2.*) (1.0.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.2.0->pandas-profiling==2.*) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.2.0->pandas-profiling==2.*) (1.4.4)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.2.0->pandas-profiling==2.*) (5.12.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.2.0->pandas-profiling==2.*) (0.11.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.2.0->pandas-profiling==2.*) (23.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.2.0->pandas-profiling==2.*) (2.8.2)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.2.0->pandas-profiling==2.*) (4.39.3)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas!=1.0.0,!=1.0.1,!=1.0.2,!=1.1.0,>=0.25.3->pandas-profiling==2.*) (2022.7.1)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.24.0->pandas-profiling==2.*) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.24.0->pandas-profiling==2.*) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.24.0->pandas-profiling==2.*) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.24.0->pandas-profiling==2.*) (1.26.15)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from importlib-resources>=3.2.0->matplotlib>=3.2.0->pandas-profiling==2.*) (3.15.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.7->matplotlib>=3.2.0->pandas-profiling==2.*) (1.16.0)\n",
            "Requirement already satisfied: PyWavelets in /usr/local/lib/python3.9/dist-packages (from imagehash->visions[type_image_path]==0.7.1->pandas-profiling==2.*) (1.4.1)\n",
            "Installing collected packages: multimethod, confuse, bottleneck, visions, pandas-profiling\n",
            "  Attempting uninstall: multimethod\n",
            "    Found existing installation: multimethod 1.9.1\n",
            "    Uninstalling multimethod-1.9.1:\n",
            "      Successfully uninstalled multimethod-1.9.1\n",
            "  Attempting uninstall: visions\n",
            "    Found existing installation: visions 0.7.4\n",
            "    Uninstalling visions-0.7.4:\n",
            "      Successfully uninstalled visions-0.7.4\n",
            "  Attempting uninstall: pandas-profiling\n",
            "    Found existing installation: pandas-profiling 3.2.0\n",
            "    Uninstalling pandas-profiling-3.2.0:\n",
            "      Successfully uninstalled pandas-profiling-3.2.0\n",
            "Successfully installed bottleneck-1.3.7 confuse-2.0.0 multimethod-1.4 pandas-profiling-2.13.0 visions-0.7.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting plotly==4.*\n",
            "  Downloading plotly-4.14.3-py2.py3-none-any.whl (13.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m104.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from plotly==4.*) (1.16.0)\n",
            "Collecting retrying>=1.3.3\n",
            "  Downloading retrying-1.3.4-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: retrying, plotly\n",
            "  Attempting uninstall: plotly\n",
            "    Found existing installation: plotly 5.13.1\n",
            "    Uninstalling plotly-5.13.1:\n",
            "      Successfully uninstalled plotly-5.13.1\n",
            "Successfully installed plotly-4.14.3 retrying-1.3.4\n",
            "/usr/local/lib/python3.9/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n",
            "2023-04-01 15:13:33.235190: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-04-01 15:13:34.277106: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-04-01 15:13:36.030415: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:266] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en-core-web-lg==3.5.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.5.0/en_core_web_lg-3.5.0-py3-none-any.whl (587.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m587.7/587.7 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /usr/local/lib/python3.9/dist-packages (from en-core-web-lg==3.5.0) (3.5.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (3.0.8)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (0.7.0)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.0.4)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (0.10.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (67.6.1)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.1.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.22.4)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.0.7)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.27.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.0.8)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (6.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (4.65.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.0.9)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (3.0.12)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.4.6)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (8.1.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (23.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (3.1.2)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.10.7)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.9/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (3.4)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.9/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (0.0.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.9/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (0.7.9)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.9/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.1.2)\n",
            "Installing collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-3.5.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_lg')\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyldavis\n",
            "  Downloading pyLDAvis-3.4.0-py3-none-any.whl (2.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numexpr in /usr/local/lib/python3.9/dist-packages (from pyldavis) (2.8.4)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from pyldavis) (1.2.2)\n",
            "Requirement already satisfied: pandas>=1.3.4 in /usr/local/lib/python3.9/dist-packages (from pyldavis) (1.4.4)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.9/dist-packages (from pyldavis) (4.3.1)\n",
            "Collecting joblib>=1.2.0\n",
            "  Downloading joblib-1.2.0-py3-none-any.whl (297 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 KB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from pyldavis) (3.1.2)\n",
            "Requirement already satisfied: numpy>=1.22.0 in /usr/local/lib/python3.9/dist-packages (from pyldavis) (1.22.4)\n",
            "Collecting funcy\n",
            "  Downloading funcy-2.0-py2.py3-none-any.whl (30 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from pyldavis) (1.10.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from pyldavis) (67.6.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.3.4->pyldavis) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.3.4->pyldavis) (2.8.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=1.0.0->pyldavis) (3.1.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.9/dist-packages (from gensim->pyldavis) (6.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->pyldavis) (2.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas>=1.3.4->pyldavis) (1.16.0)\n",
            "Installing collected packages: funcy, joblib, pyldavis\n",
            "  Attempting uninstall: joblib\n",
            "    Found existing installation: joblib 1.1.1\n",
            "    Uninstalling joblib-1.1.1:\n",
            "      Successfully uninstalled joblib-1.1.1\n",
            "Successfully installed funcy-2.0 joblib-1.2.0 pyldavis-3.4.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.9/dist-packages (4.3.1)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.9/dist-packages (from gensim) (1.10.1)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.9/dist-packages (from gensim) (1.22.4)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.9/dist-packages (from gensim) (6.3.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting chart_studio\n",
            "  Downloading chart_studio-1.1.0-py3-none-any.whl (64 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.4/64.4 KB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: plotly in /usr/local/lib/python3.9/dist-packages (from chart_studio) (4.14.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from chart_studio) (1.16.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from chart_studio) (2.27.1)\n",
            "Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.9/dist-packages (from chart_studio) (1.3.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->chart_studio) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->chart_studio) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->chart_studio) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->chart_studio) (2022.12.7)\n",
            "Installing collected packages: chart_studio\n",
            "Successfully installed chart_studio-1.1.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting autopep8\n",
            "  Downloading autopep8-2.0.2-py2.py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 KB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pycodestyle>=2.10.0\n",
            "  Downloading pycodestyle-2.10.0-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 KB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tomli in /usr/local/lib/python3.9/dist-packages (from autopep8) (2.0.1)\n",
            "Installing collected packages: pycodestyle, autopep8\n",
            "Successfully installed autopep8-2.0.2 pycodestyle-2.10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#installing tweet-preprocessor\n",
        "!pip install tweet-preprocessor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x40CSMZGowhN",
        "outputId": "a4fcc055-ff77-45bf-9e53-d41f4d43d757"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tweet-preprocessor\n",
            "  Downloading tweet_preprocessor-0.6.0-py3-none-any.whl (27 kB)\n",
            "Installing collected packages: tweet-preprocessor\n",
            "Successfully installed tweet-preprocessor-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APMx7pRNEQFa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3bbfa15-29ff-439f-9455-5c80eaefd304"
      },
      "source": [
        "# Required Libraries\n",
        "\n",
        "#Base and Cleaning \n",
        "import json\n",
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import emoji\n",
        "import regex\n",
        "import re\n",
        "import string\n",
        "from collections import Counter\n",
        "\n",
        "#Visualizations\n",
        "import plotly.express as px\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt \n",
        "import pyLDAvis.gensim\n",
        "import chart_studio\n",
        "import chart_studio.plotly as py \n",
        "import chart_studio.tools as tls\n",
        "\n",
        "#Natural Language Processing (NLP)\n",
        "import spacy\n",
        "import gensim\n",
        "from spacy.tokenizer import Tokenizer\n",
        "from gensim.corpora import Dictionary\n",
        "from gensim.models.ldamulticore import LdaMulticore\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "from gensim.parsing.preprocessing import STOPWORDS as SW\n",
        "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from pprint import pprint\n",
        "from wordcloud import STOPWORDS\n",
        "stopwords = set(STOPWORDS)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n",
            "/usr/local/lib/python3.9/dist-packages/google/rpc/__init__.py:20: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google.rpc')`.\n",
            "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
            "  pkg_resources.declare_namespace(__name__)\n",
            "/usr/local/lib/python3.9/dist-packages/pkg_resources/__init__.py:2349: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.\n",
            "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
            "  declare_namespace(parent)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Dataset upload"
      ],
      "metadata": {
        "id": "oScqC398hta9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('text,category,explanation.txt')\n",
        "df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5adDW_q0hsnD",
        "outputId": "e9800205-6eba-4e1a-be79-948e3e5f5b4c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['text', 'category', 'explanation'], dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# df=df.drop(['Unnamed: 0' , 'post_id' , 'post_created','user_id','followers', 'friends', 'favourites', 'statuses', 'retweets'],axis=1)\n",
        "# df.rename(columns = {'post_text':'original_tweets'}, inplace = True)"
      ],
      "metadata": {
        "id": "MfM3aB8oiOC3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.rename(columns = {'text':'original_tweets'}, inplace = True)"
      ],
      "metadata": {
        "id": "UKzhwhss4RvS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cac58724-a3d6-4a54-8b08-6929001d220d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(20)"
      ],
      "metadata": {
        "id": "Xwrxh1vDjLIH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        },
        "outputId": "92047836-1fb9-4e71-d535-fb8f57f80eee"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                      original_tweets  category  \\\n",
              "0   in the end we all die, nothing we do means any...       5.0   \n",
              "1   Today would have been my best friend's 18th bi...       4.0   \n",
              "2   So I couldn't feel any worse, and I've had eno...       5.0   \n",
              "3   I am 22 and have never had a girlfriend and ha...       4.0   \n",
              "4   I am almost certain that my depression is caus...       4.0   \n",
              "5   I live in a country that I don't want to live ...       2.0   \n",
              "6   EDIT: Wow that many upvotes and support, thank...       3.0   \n",
              "7   Hey guys,\\r\\nI've been depressed, anxious and ...       4.0   \n",
              "8   Hi guys,\\r\\nNever even used reddit before but ...       3.0   \n",
              "9   i always thought hanging yourself would be ext...       0.0   \n",
              "10  Hello. I am 22 years old. Me and my family are...       4.0   \n",
              "11  My daughter tried to kill herself, again. This...       4.0   \n",
              "12  I **need** this year to be better.\\r\\n\\r\\nI'm ...       2.0   \n",
              "13  My greatest solution has always been to just s...       5.0   \n",
              "14  Normally i feel threatened by ''good-looking''...       1.0   \n",
              "15  I keep hearing them exploding close to my hous...       5.0   \n",
              "16  But he wants to walk through them.\\r\\n\\r\\nIt's...       5.0   \n",
              "17  I hate to be negative but I am just super tire...       5.0   \n",
              "18  I have no reason to feel this way. I am surrou...       4.0   \n",
              "19  This has not been a good year for me, and has ...       5.0   \n",
              "\n",
              "                                          explanation  \n",
              "0   feel more and more empty,fear lonliness,get ti...  \n",
              "1    best friend's birthday, he's gone, never supp...  \n",
              "2   absolute minimal contact with,  be more direct...  \n",
              "3    never had a girlfriend, rejected countless times  \n",
              "4   eating out with family, what after that? Paren...  \n",
              "5                  no job, no education or even goals  \n",
              "6   depression, anxiety, severe OCD, tooth ache, m...  \n",
              "7    she dumped me she laughed and smiled and trea...  \n",
              "8   an unexpected diagnosis  after a collapse trig...  \n",
              "9                                                 NaN  \n",
              "10   me and my family suffering from a severe depr...  \n",
              "11   daughter has mental illness, tried to kill he...  \n",
              "12  I failed two courses last semester, if I donít...  \n",
              "13                                 nothing else to do  \n",
              "14              feel threatned by good looking people  \n",
              "15           alone doing nothing, no sound in my room  \n",
              "16  when I look outside this black bubble,vantalbl...  \n",
              "17  crippling sadness/ lonlinessI need someone to ...  \n",
              "18  I've never had luck with relationships,magnifi...  \n",
              "19  Tired of being told Depression is chemical imb...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-87aafa76-9a9e-47c4-8935-caaea5def5bc\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>original_tweets</th>\n",
              "      <th>category</th>\n",
              "      <th>explanation</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>in the end we all die, nothing we do means any...</td>\n",
              "      <td>5.0</td>\n",
              "      <td>feel more and more empty,fear lonliness,get ti...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Today would have been my best friend's 18th bi...</td>\n",
              "      <td>4.0</td>\n",
              "      <td>best friend's birthday, he's gone, never supp...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>So I couldn't feel any worse, and I've had eno...</td>\n",
              "      <td>5.0</td>\n",
              "      <td>absolute minimal contact with,  be more direct...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>I am 22 and have never had a girlfriend and ha...</td>\n",
              "      <td>4.0</td>\n",
              "      <td>never had a girlfriend, rejected countless times</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>I am almost certain that my depression is caus...</td>\n",
              "      <td>4.0</td>\n",
              "      <td>eating out with family, what after that? Paren...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>I live in a country that I don't want to live ...</td>\n",
              "      <td>2.0</td>\n",
              "      <td>no job, no education or even goals</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>EDIT: Wow that many upvotes and support, thank...</td>\n",
              "      <td>3.0</td>\n",
              "      <td>depression, anxiety, severe OCD, tooth ache, m...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Hey guys,\\r\\nI've been depressed, anxious and ...</td>\n",
              "      <td>4.0</td>\n",
              "      <td>she dumped me she laughed and smiled and trea...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Hi guys,\\r\\nNever even used reddit before but ...</td>\n",
              "      <td>3.0</td>\n",
              "      <td>an unexpected diagnosis  after a collapse trig...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>i always thought hanging yourself would be ext...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Hello. I am 22 years old. Me and my family are...</td>\n",
              "      <td>4.0</td>\n",
              "      <td>me and my family suffering from a severe depr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>My daughter tried to kill herself, again. This...</td>\n",
              "      <td>4.0</td>\n",
              "      <td>daughter has mental illness, tried to kill he...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>I **need** this year to be better.\\r\\n\\r\\nI'm ...</td>\n",
              "      <td>2.0</td>\n",
              "      <td>I failed two courses last semester, if I donít...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>My greatest solution has always been to just s...</td>\n",
              "      <td>5.0</td>\n",
              "      <td>nothing else to do</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>Normally i feel threatened by ''good-looking''...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>feel threatned by good looking people</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>I keep hearing them exploding close to my hous...</td>\n",
              "      <td>5.0</td>\n",
              "      <td>alone doing nothing, no sound in my room</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>But he wants to walk through them.\\r\\n\\r\\nIt's...</td>\n",
              "      <td>5.0</td>\n",
              "      <td>when I look outside this black bubble,vantalbl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>I hate to be negative but I am just super tire...</td>\n",
              "      <td>5.0</td>\n",
              "      <td>crippling sadness/ lonlinessI need someone to ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>I have no reason to feel this way. I am surrou...</td>\n",
              "      <td>4.0</td>\n",
              "      <td>I've never had luck with relationships,magnifi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>This has not been a good year for me, and has ...</td>\n",
              "      <td>5.0</td>\n",
              "      <td>Tired of being told Depression is chemical imb...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-87aafa76-9a9e-47c4-8935-caaea5def5bc')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-87aafa76-9a9e-47c4-8935-caaea5def5bc button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-87aafa76-9a9e-47c4-8935-caaea5def5bc');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data cleaning"
      ],
      "metadata": {
        "id": "W0jfd-ZaepS7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "slang_abbrev_dict = {\n",
        "    'AFAIK': 'As Far As I Know',\n",
        "    'AFK': 'Away From Keyboard',\n",
        "    'ASAP': 'As Soon As Possible',\n",
        "    'ATK': 'At The Keyboard',\n",
        "    'ATM': 'At The Moment',\n",
        "    'A3': 'Anytime, Anywhere, Anyplace',\n",
        "    'BAK': 'Back At Keyboard',\n",
        "    'BBL': 'Be Back Later',\n",
        "    'BBS': 'Be Back Soon',\n",
        "    'BFN': 'Bye For Now',\n",
        "    'B4N': 'Bye For Now',\n",
        "    'BRB': 'Be Right Back',\n",
        "    'BRT': 'Be Right There',\n",
        "    'BTW': 'By The Way',\n",
        "    'B4': 'Before',\n",
        "    'B4N': 'Bye For Now',\n",
        "    'CU': 'See You',\n",
        "    'CUL8R': 'See You Later',\n",
        "    'CYA': 'See You',\n",
        "    'FAQ': 'Frequently Asked Questions',\n",
        "    'FC': 'Fingers Crossed',\n",
        "    'FWIW': 'For What It\\'s Worth',\n",
        "    'FYI': 'For Your Information',\n",
        "    'GAL': 'Get A Life',\n",
        "    'GG': 'Good Game',\n",
        "    'GN': 'Good Night',\n",
        "    'GMTA': 'Great Minds Think Alike',\n",
        "    'GR8': 'Great!',\n",
        "    'G9': 'Genius',\n",
        "    'IC': 'I See',\n",
        "    'ICQ': 'I Seek you',\n",
        "    'ILU': 'I Love You',\n",
        "    'IMHO': 'In My Humble Opinion',\n",
        "    'IMO': 'In My Opinion',\n",
        "    'IOW': 'In Other Words',\n",
        "    'IRL': 'In Real Life',\n",
        "    'KISS': 'Keep It Simple, Stupid',\n",
        "    'LDR': 'Long Distance Relationship',\n",
        "    'LMAO': 'Laugh My Ass Off',\n",
        "    'LOL': 'Laughing Out Loud',\n",
        "    'LTNS': 'Long Time No See',\n",
        "    'L8R': 'Later',\n",
        "    'MTE': 'My Thoughts Exactly',\n",
        "    'M8': 'Mate',\n",
        "    'NRN': 'No Reply Necessary',\n",
        "    'OIC': 'Oh I See',\n",
        "    'OMG': 'Oh My God',\n",
        "    'PITA': 'Pain In The Ass',\n",
        "    'PRT': 'Party',\n",
        "    'PRW': 'Parents Are Watching',\n",
        "    'QPSA?': 'Que Pasa?',\n",
        "    'ROFL': 'Rolling On The Floor Laughing',\n",
        "    'ROFLOL': 'Rolling On The Floor Laughing Out Loud',\n",
        "    'ROTFLMAO': 'Rolling On The Floor Laughing My Ass Off',\n",
        "    'SK8': 'Skate',\n",
        "    'STATS': 'Your sex and age',\n",
        "    'ASL': 'Age, Sex, Location',\n",
        "    'THX': 'Thank You',\n",
        "    'TTFN': 'Ta-Ta For Now!',\n",
        "    'TTYL': 'Talk To You Later',\n",
        "    'U': 'You',\n",
        "    'U2': 'You Too',\n",
        "    'U4E': 'Yours For Ever',\n",
        "    'WB': 'Welcome Back',\n",
        "    'WTF': 'What The Fuck',\n",
        "    'WTG': 'Way To Go!',\n",
        "    'WUF': 'Where Are You From?',\n",
        "    'W8': 'Wait',\n",
        "    '7K': 'Sick:-D Laugher'\n",
        "}"
      ],
      "metadata": {
        "id": "HUwVu1x2k2FO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bffe05a0-3cde-466c-bebd-143ad7b4b21e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import preprocessor as p"
      ],
      "metadata": {
        "id": "PF_HD_6ApcNo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98ea628b-4f5d-4a4c-fcd2-f944d34f1b31"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# spell = SpellChecker()\n",
        "\n",
        "# def correct_spellings(text):\n",
        "#     corrected_text = []\n",
        "#     misspelled_words = spell.unknown(text.split())\n",
        "#     for word in text.split():\n",
        "#         if word in misspelled_words:\n",
        "#             corrected_text.append(spell.correction(word))\n",
        "#         else:\n",
        "#             corrected_text.append(word)\n",
        "#     return \" \".join(corrected_text)"
      ],
      "metadata": {
        "id": "p5-koyDPtl55",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa5ecb85-2b45-4cb4-8400-4a728ea6b28b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_emoji(text):\n",
        "    \"\"\"\n",
        "    Removes emoji's from tweets\n",
        "    Accepts:\n",
        "        Text (tweets)\n",
        "    Returns:\n",
        "        Text (emoji free tweets)\n",
        "    \"\"\"\n",
        "    emoji_list = [c for c in text if c in emoji_data.unicode_emoji]\n",
        "    clean_text = ' '.join([str for str in text.split() if not any(i in str for i in emoji_list)])\n",
        "    return clean_text\n",
        "\n",
        "# def remove_emoji(text):\n",
        "#     emoji_pattern = re.compile(\n",
        "#         \"[\"\n",
        "#         u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "#         u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "#         u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "#         u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "#         u\"\\U00002702-\\U000027B0\"\n",
        "#         u\"\\U000024C2-\\U0001F251\"\n",
        "#         \"]+\",\n",
        "#         flags=re.UNICODE)\n",
        "#     return emoji_pattern.sub(r'', text)\n",
        "\n",
        "\n",
        "def url_free_text(text):    \n",
        "    # Cleans text from urls\n",
        "    text = re.sub(r'http\\S+', '', text)\n",
        "    return text\n",
        "\n",
        "def username_free_text(text):\n",
        "    # remove @username from tweets\n",
        "    text = re.sub('@[\\w]+','',text)\n",
        "    # remove hashtags\n",
        "    text = re.sub(r'#\\w+ ?', '', text)\n",
        "    # #remove reserved word such as RT,FAV\n",
        "    # text= p.OPT.RESERVED(text)\n",
        "    return text\n",
        "\n",
        "def decontracted(phrase):\n",
        "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
        "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
        "    phrase = re.sub(r\"n't\", \" not\", phrase) \n",
        "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
        "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
        "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
        "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
        "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
        "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
        "    return phrase\n",
        "\n",
        "\n",
        "def unslang(text):\n",
        "    # Converts text like \"OMG\" into \"Oh my God\"\n",
        "  \n",
        "    if text.upper() in slang_abbrev_dict.keys():\n",
        "        return slang_abbrev_dict[text.upper()]\n",
        "    else:\n",
        "        return text\n",
        "\n",
        "def remove_punc(text):\n",
        "    # remove numbers\n",
        "    text_nonum = re.sub(r'\\d+', '', text)\n",
        "\n",
        "    # remove punctuations and convert characters to lower case\n",
        "    text_nopunct = \"\".join([char.lower() for char in text_nonum if char not in string.punctuation]) \n",
        "    \n",
        "    # substitute multiple whitespace with single whitespace Also, removes leading and trailing whitespaces\n",
        "    text_no_doublespace = re.sub('\\s+', ' ', text_nopunct).strip()\n",
        "    return text_no_doublespace\n",
        "    \n",
        "\n",
        "# Apply the function above and get tweets free of emoji's\n",
        "call_emoji_free = lambda x: remove_emoji(x)\n",
        "\n",
        "# Apply `call_emoji_free` which calls the function to remove all emoji's\n",
        "# df['emoji_free_tweets'] = df['original_tweets'].apply(remove_emoji)\n",
        "df['original_tweets'] = df['original_tweets'].astype(str)\n",
        "\n",
        "#Create a new column with url free tweets\n",
        "df['url_free_tweets'] = df['original_tweets'].apply(url_free_text)\n",
        "\n",
        "#Create a new column with username free tweets\n",
        "df['username_free_tweets'] = df['url_free_tweets'].apply(username_free_text)\n",
        "\n",
        "#Create a new column with removing can't with cannot tweets\n",
        "df['slang_free_tweets'] = df['username_free_tweets'].apply(decontracted)\n",
        "\n",
        "#Create a new column  removing OMG with oh my god tweets\n",
        "df['slang_free_tweets'] = df['slang_free_tweets'].apply(unslang)\n",
        "\n",
        "#Create a new column with no pun\n",
        "df['punc_free_tweets'] = df['slang_free_tweets'].apply(remove_punc)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "g2G6yYwHenUV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "299375b7-d8c6-4fcd-ee44-e16501e4c8a0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "<>:34: DeprecationWarning: invalid escape sequence \\w\n",
            "<>:71: DeprecationWarning: invalid escape sequence \\s\n",
            "<>:34: DeprecationWarning: invalid escape sequence \\w\n",
            "<>:71: DeprecationWarning: invalid escape sequence \\s\n",
            "<ipython-input-11-34ab5a99d751>:34: DeprecationWarning: invalid escape sequence \\w\n",
            "  text = re.sub('@[\\w]+','',text)\n",
            "<ipython-input-11-34ab5a99d751>:71: DeprecationWarning: invalid escape sequence \\s\n",
            "  text_no_doublespace = re.sub('\\s+', ' ', text_nopunct).strip()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "W3qQoStxvjDz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "outputId": "53b494e1-be2a-4c58-9deb-dd48eae617e1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                     original_tweets  category  \\\n",
              "0  in the end we all die, nothing we do means any...       5.0   \n",
              "1  Today would have been my best friend's 18th bi...       4.0   \n",
              "2  So I couldn't feel any worse, and I've had eno...       5.0   \n",
              "3  I am 22 and have never had a girlfriend and ha...       4.0   \n",
              "4  I am almost certain that my depression is caus...       4.0   \n",
              "\n",
              "                                         explanation  \\\n",
              "0  feel more and more empty,fear lonliness,get ti...   \n",
              "1   best friend's birthday, he's gone, never supp...   \n",
              "2  absolute minimal contact with,  be more direct...   \n",
              "3   never had a girlfriend, rejected countless times   \n",
              "4  eating out with family, what after that? Paren...   \n",
              "\n",
              "                                     url_free_tweets  \\\n",
              "0  in the end we all die, nothing we do means any...   \n",
              "1  Today would have been my best friend's 18th bi...   \n",
              "2  So I couldn't feel any worse, and I've had eno...   \n",
              "3  I am 22 and have never had a girlfriend and ha...   \n",
              "4  I am almost certain that my depression is caus...   \n",
              "\n",
              "                                username_free_tweets  \\\n",
              "0  in the end we all die, nothing we do means any...   \n",
              "1  Today would have been my best friend's 18th bi...   \n",
              "2  So I couldn't feel any worse, and I've had eno...   \n",
              "3  I am 22 and have never had a girlfriend and ha...   \n",
              "4  I am almost certain that my depression is caus...   \n",
              "\n",
              "                                   slang_free_tweets  \\\n",
              "0  in the end we all die, nothing we do means any...   \n",
              "1  Today would have been my best friend is 18th b...   \n",
              "2  So I could not feel any worse, and I have had ...   \n",
              "3  I am 22 and have never had a girlfriend and ha...   \n",
              "4  I am almost certain that my depression is caus...   \n",
              "\n",
              "                                    punc_free_tweets  \n",
              "0  in the end we all die nothing we do means anyt...  \n",
              "1  today would have been my best friend is th bir...  \n",
              "2  so i could not feel any worse and i have had e...  \n",
              "3  i am and have never had a girlfriend and have ...  \n",
              "4  i am almost certain that my depression is caus...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9253f28d-5c70-4dcd-9794-79dc2ab51841\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>original_tweets</th>\n",
              "      <th>category</th>\n",
              "      <th>explanation</th>\n",
              "      <th>url_free_tweets</th>\n",
              "      <th>username_free_tweets</th>\n",
              "      <th>slang_free_tweets</th>\n",
              "      <th>punc_free_tweets</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>in the end we all die, nothing we do means any...</td>\n",
              "      <td>5.0</td>\n",
              "      <td>feel more and more empty,fear lonliness,get ti...</td>\n",
              "      <td>in the end we all die, nothing we do means any...</td>\n",
              "      <td>in the end we all die, nothing we do means any...</td>\n",
              "      <td>in the end we all die, nothing we do means any...</td>\n",
              "      <td>in the end we all die nothing we do means anyt...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Today would have been my best friend's 18th bi...</td>\n",
              "      <td>4.0</td>\n",
              "      <td>best friend's birthday, he's gone, never supp...</td>\n",
              "      <td>Today would have been my best friend's 18th bi...</td>\n",
              "      <td>Today would have been my best friend's 18th bi...</td>\n",
              "      <td>Today would have been my best friend is 18th b...</td>\n",
              "      <td>today would have been my best friend is th bir...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>So I couldn't feel any worse, and I've had eno...</td>\n",
              "      <td>5.0</td>\n",
              "      <td>absolute minimal contact with,  be more direct...</td>\n",
              "      <td>So I couldn't feel any worse, and I've had eno...</td>\n",
              "      <td>So I couldn't feel any worse, and I've had eno...</td>\n",
              "      <td>So I could not feel any worse, and I have had ...</td>\n",
              "      <td>so i could not feel any worse and i have had e...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>I am 22 and have never had a girlfriend and ha...</td>\n",
              "      <td>4.0</td>\n",
              "      <td>never had a girlfriend, rejected countless times</td>\n",
              "      <td>I am 22 and have never had a girlfriend and ha...</td>\n",
              "      <td>I am 22 and have never had a girlfriend and ha...</td>\n",
              "      <td>I am 22 and have never had a girlfriend and ha...</td>\n",
              "      <td>i am and have never had a girlfriend and have ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>I am almost certain that my depression is caus...</td>\n",
              "      <td>4.0</td>\n",
              "      <td>eating out with family, what after that? Paren...</td>\n",
              "      <td>I am almost certain that my depression is caus...</td>\n",
              "      <td>I am almost certain that my depression is caus...</td>\n",
              "      <td>I am almost certain that my depression is caus...</td>\n",
              "      <td>i am almost certain that my depression is caus...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9253f28d-5c70-4dcd-9794-79dc2ab51841')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9253f28d-5c70-4dcd-9794-79dc2ab51841 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9253f28d-5c70-4dcd-9794-79dc2ab51841');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data preprocessing"
      ],
      "metadata": {
        "id": "Rf_HZ9ocxnM2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load spacy\n",
        "# Make sure to restart the runtime after running installations and libraries tab\n",
        "nlp = spacy.load('en_core_web_lg')"
      ],
      "metadata": {
        "id": "OKTGaUYsvpPr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b494da76-28e9-49d5-fee4-bd2cf34dd943"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizer\n",
        "tokenizer = Tokenizer(nlp.vocab)\n",
        "\n",
        "\n",
        "# Custom stopwords\n",
        "custom_stopwords = ['hi','\\n','\\n\\n', '&', ' ', '.', '-', 'got', \"it's\", 'it’s', \"i'm\", 'i’m', 'im', 'want', 'like', '$', '@']\n",
        "\n",
        "# Customize stop words by adding to the default list\n",
        "STOP_WORDS = nlp.Defaults.stop_words.union(custom_stopwords)\n",
        "\n",
        "# ALL_STOP_WORDS = spacy + gensim + wordcloud\n",
        "ALL_STOP_WORDS = STOP_WORDS.union(SW).union(stopwords)\n",
        "\n",
        "\n",
        "tokens = []\n",
        "\n",
        "for doc in tokenizer.pipe(df['punc_free_tweets'], batch_size=500):\n",
        "    doc_tokens = []    \n",
        "    for token in doc: \n",
        "        if token.text.lower() not in STOP_WORDS:\n",
        "            doc_tokens.append(token.text.lower())   \n",
        "    tokens.append(doc_tokens)\n",
        "\n",
        "# Makes tokens column\n",
        "df['tokens'] = tokens\n",
        "     "
      ],
      "metadata": {
        "id": "VdBUWYsHxq7p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d9ddfa1-83b6-4e8a-b935-c8baee41d43f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make tokens a string again\n",
        "df['tokens_back_to_text'] = [' '.join(map(str, l)) for l in df['tokens']]\n",
        "\n",
        "def get_lemmas(text):\n",
        "    '''Used to lemmatize the processed tweets'''\n",
        "    lemmas = []\n",
        "    \n",
        "    doc = nlp(text)\n",
        "    \n",
        "    # Something goes here :P\n",
        "    for token in doc: \n",
        "        if ((token.is_stop == False) and (token.is_punct == False)) and (token.pos_ != 'PRON'):\n",
        "            lemmas.append(token.lemma_)\n",
        "    \n",
        "    return lemmas\n",
        "\n",
        "df['lemmas'] = df['tokens_back_to_text'].apply(get_lemmas)"
      ],
      "metadata": {
        "id": "k6smXp7VxvdJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48e56eaa-83bb-423b-acbd-5869e11fd4e9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make lemmas a string again\n",
        "df['lemmas_back_to_text'] = [' '.join(map(str, l)) for l in df['lemmas']]\n",
        "\n",
        "# Tokenizer function\n",
        "def tokenize(text):\n",
        "    \"\"\"\n",
        "    Parses a string into a list of semantic units (words)\n",
        "    Args:\n",
        "        text (str): The string that the function will tokenize.\n",
        "    Returns:\n",
        "        list: tokens parsed out\n",
        "    \"\"\"\n",
        "    # Removing url's\n",
        "    pattern = r\"http\\S+\"\n",
        "    \n",
        "    tokens = re.sub(pattern, \"\", text) # https://www.youtube.com/watch?v=O2onA4r5UaY\n",
        "    tokens = re.sub('[^a-zA-Z 0-9]', '', text)\n",
        "    tokens = re.sub('[%s]' % re.escape(string.punctuation), '', text) # Remove punctuation\n",
        "    tokens = re.sub('\\w*\\d\\w*', '', text) # Remove words containing numbers\n",
        "    # tokens = re.sub('@*!*$*', '', text) # Remove @ ! $\n",
        "    tokens = tokens.strip(',') # TESTING THIS LINE\n",
        "    tokens = tokens.strip('?') # TESTING THIS LINE\n",
        "    tokens = tokens.strip('!') # TESTING THIS LINE\n",
        "    tokens = tokens.strip(\"'\") # TESTING THIS LINE\n",
        "    tokens = tokens.strip(\".\") # TESTING THIS LINE\n",
        "\n",
        "    tokens = tokens.lower().split() # Make text lowercase and split it\n",
        "    \n",
        "    return tokens\n",
        "\n",
        "# Apply tokenizer\n",
        "df['lemma_tokens'] = df['lemmas_back_to_text'].apply(tokenize)"
      ],
      "metadata": {
        "id": "-yi3HUVDxyNn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4deeffdb-9d75-4e5e-d1f4-753e1533aeb6"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "<>:19: DeprecationWarning: invalid escape sequence \\w\n",
            "<>:19: DeprecationWarning: invalid escape sequence \\w\n",
            "<ipython-input-16-0214d658a4f1>:19: DeprecationWarning: invalid escape sequence \\w\n",
            "  tokens = re.sub('\\w*\\d\\w*', '', text) # Remove words containing numbers\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "V8T5hMUTzJSV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 686
        },
        "outputId": "171ff860-c5d8-4b1b-f53a-afd1123904dd"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                     original_tweets  category  \\\n",
              "0  in the end we all die, nothing we do means any...       5.0   \n",
              "1  Today would have been my best friend's 18th bi...       4.0   \n",
              "2  So I couldn't feel any worse, and I've had eno...       5.0   \n",
              "3  I am 22 and have never had a girlfriend and ha...       4.0   \n",
              "4  I am almost certain that my depression is caus...       4.0   \n",
              "\n",
              "                                         explanation  \\\n",
              "0  feel more and more empty,fear lonliness,get ti...   \n",
              "1   best friend's birthday, he's gone, never supp...   \n",
              "2  absolute minimal contact with,  be more direct...   \n",
              "3   never had a girlfriend, rejected countless times   \n",
              "4  eating out with family, what after that? Paren...   \n",
              "\n",
              "                                     url_free_tweets  \\\n",
              "0  in the end we all die, nothing we do means any...   \n",
              "1  Today would have been my best friend's 18th bi...   \n",
              "2  So I couldn't feel any worse, and I've had eno...   \n",
              "3  I am 22 and have never had a girlfriend and ha...   \n",
              "4  I am almost certain that my depression is caus...   \n",
              "\n",
              "                                username_free_tweets  \\\n",
              "0  in the end we all die, nothing we do means any...   \n",
              "1  Today would have been my best friend's 18th bi...   \n",
              "2  So I couldn't feel any worse, and I've had eno...   \n",
              "3  I am 22 and have never had a girlfriend and ha...   \n",
              "4  I am almost certain that my depression is caus...   \n",
              "\n",
              "                                   slang_free_tweets  \\\n",
              "0  in the end we all die, nothing we do means any...   \n",
              "1  Today would have been my best friend is 18th b...   \n",
              "2  So I could not feel any worse, and I have had ...   \n",
              "3  I am 22 and have never had a girlfriend and ha...   \n",
              "4  I am almost certain that my depression is caus...   \n",
              "\n",
              "                                    punc_free_tweets  \\\n",
              "0  in the end we all die nothing we do means anyt...   \n",
              "1  today would have been my best friend is th bir...   \n",
              "2  so i could not feel any worse and i have had e...   \n",
              "3  i am and have never had a girlfriend and have ...   \n",
              "4  i am almost certain that my depression is caus...   \n",
              "\n",
              "                                              tokens  \\\n",
              "0  [end, die, means, throught, life, endure, bull...   \n",
              "1  [today, best, friend, th, birthday, going, tim...   \n",
              "2  [feel, worse, tonight, midnight, going, messag...   \n",
              "3  [girlfriend, rejected, countless, times, coupl...   \n",
              "4  [certain, depression, caused, bullshit, shapes...   \n",
              "\n",
              "                                 tokens_back_to_text  \\\n",
              "0  end die means throught life endure bullshit ex...   \n",
              "1  today best friend th birthday going time sitti...   \n",
              "2  feel worse tonight midnight going message girl...   \n",
              "3  girlfriend rejected countless times couples fe...   \n",
              "4  certain depression caused bullshit shapes size...   \n",
              "\n",
              "                                              lemmas  \\\n",
              "0  [end, die, mean, throught, life, endure, bulls...   \n",
              "1  [today, good, friend, th, birthday, go, time, ...   \n",
              "2  [feel, bad, tonight, midnight, go, message, gi...   \n",
              "3  [girlfriend, reject, countless, time, couple, ...   \n",
              "4  [certain, depression, cause, bullshit, shape, ...   \n",
              "\n",
              "                                 lemmas_back_to_text  \\\n",
              "0  end die mean throught life endure bullshit exp...   \n",
              "1  today good friend th birthday go time sit make...   \n",
              "2  feel bad tonight midnight go message girl abso...   \n",
              "3  girlfriend reject countless time couple feel d...   \n",
              "4  certain depression cause bullshit shape size g...   \n",
              "\n",
              "                                        lemma_tokens  \n",
              "0  [end, die, mean, throught, life, endure, bulls...  \n",
              "1  [today, good, friend, th, birthday, go, time, ...  \n",
              "2  [feel, bad, tonight, midnight, go, message, gi...  \n",
              "3  [girlfriend, reject, countless, time, couple, ...  \n",
              "4  [certain, depression, cause, bullshit, shape, ...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-95340502-81b7-416c-a946-824d5779ba84\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>original_tweets</th>\n",
              "      <th>category</th>\n",
              "      <th>explanation</th>\n",
              "      <th>url_free_tweets</th>\n",
              "      <th>username_free_tweets</th>\n",
              "      <th>slang_free_tweets</th>\n",
              "      <th>punc_free_tweets</th>\n",
              "      <th>tokens</th>\n",
              "      <th>tokens_back_to_text</th>\n",
              "      <th>lemmas</th>\n",
              "      <th>lemmas_back_to_text</th>\n",
              "      <th>lemma_tokens</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>in the end we all die, nothing we do means any...</td>\n",
              "      <td>5.0</td>\n",
              "      <td>feel more and more empty,fear lonliness,get ti...</td>\n",
              "      <td>in the end we all die, nothing we do means any...</td>\n",
              "      <td>in the end we all die, nothing we do means any...</td>\n",
              "      <td>in the end we all die, nothing we do means any...</td>\n",
              "      <td>in the end we all die nothing we do means anyt...</td>\n",
              "      <td>[end, die, means, throught, life, endure, bull...</td>\n",
              "      <td>end die means throught life endure bullshit ex...</td>\n",
              "      <td>[end, die, mean, throught, life, endure, bulls...</td>\n",
              "      <td>end die mean throught life endure bullshit exp...</td>\n",
              "      <td>[end, die, mean, throught, life, endure, bulls...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Today would have been my best friend's 18th bi...</td>\n",
              "      <td>4.0</td>\n",
              "      <td>best friend's birthday, he's gone, never supp...</td>\n",
              "      <td>Today would have been my best friend's 18th bi...</td>\n",
              "      <td>Today would have been my best friend's 18th bi...</td>\n",
              "      <td>Today would have been my best friend is 18th b...</td>\n",
              "      <td>today would have been my best friend is th bir...</td>\n",
              "      <td>[today, best, friend, th, birthday, going, tim...</td>\n",
              "      <td>today best friend th birthday going time sitti...</td>\n",
              "      <td>[today, good, friend, th, birthday, go, time, ...</td>\n",
              "      <td>today good friend th birthday go time sit make...</td>\n",
              "      <td>[today, good, friend, th, birthday, go, time, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>So I couldn't feel any worse, and I've had eno...</td>\n",
              "      <td>5.0</td>\n",
              "      <td>absolute minimal contact with,  be more direct...</td>\n",
              "      <td>So I couldn't feel any worse, and I've had eno...</td>\n",
              "      <td>So I couldn't feel any worse, and I've had eno...</td>\n",
              "      <td>So I could not feel any worse, and I have had ...</td>\n",
              "      <td>so i could not feel any worse and i have had e...</td>\n",
              "      <td>[feel, worse, tonight, midnight, going, messag...</td>\n",
              "      <td>feel worse tonight midnight going message girl...</td>\n",
              "      <td>[feel, bad, tonight, midnight, go, message, gi...</td>\n",
              "      <td>feel bad tonight midnight go message girl abso...</td>\n",
              "      <td>[feel, bad, tonight, midnight, go, message, gi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>I am 22 and have never had a girlfriend and ha...</td>\n",
              "      <td>4.0</td>\n",
              "      <td>never had a girlfriend, rejected countless times</td>\n",
              "      <td>I am 22 and have never had a girlfriend and ha...</td>\n",
              "      <td>I am 22 and have never had a girlfriend and ha...</td>\n",
              "      <td>I am 22 and have never had a girlfriend and ha...</td>\n",
              "      <td>i am and have never had a girlfriend and have ...</td>\n",
              "      <td>[girlfriend, rejected, countless, times, coupl...</td>\n",
              "      <td>girlfriend rejected countless times couples fe...</td>\n",
              "      <td>[girlfriend, reject, countless, time, couple, ...</td>\n",
              "      <td>girlfriend reject countless time couple feel d...</td>\n",
              "      <td>[girlfriend, reject, countless, time, couple, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>I am almost certain that my depression is caus...</td>\n",
              "      <td>4.0</td>\n",
              "      <td>eating out with family, what after that? Paren...</td>\n",
              "      <td>I am almost certain that my depression is caus...</td>\n",
              "      <td>I am almost certain that my depression is caus...</td>\n",
              "      <td>I am almost certain that my depression is caus...</td>\n",
              "      <td>i am almost certain that my depression is caus...</td>\n",
              "      <td>[certain, depression, caused, bullshit, shapes...</td>\n",
              "      <td>certain depression caused bullshit shapes size...</td>\n",
              "      <td>[certain, depression, cause, bullshit, shape, ...</td>\n",
              "      <td>certain depression cause bullshit shape size g...</td>\n",
              "      <td>[certain, depression, cause, bullshit, shape, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-95340502-81b7-416c-a946-824d5779ba84')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-95340502-81b7-416c-a946-824d5779ba84 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-95340502-81b7-416c-a946-824d5779ba84');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Word cloud"
      ],
      "metadata": {
        "id": "DE2HP5qErmbp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "#Frequency of words\n",
        "fdist = FreqDist(tweets['Segmented#'])\n",
        "#WordCloud\n",
        "wc = WordCloud(width=800, height=400, max_words=50).generate_from_frequencies(fdist)\n",
        "plt.figure(figsize=(12,10))\n",
        "plt.imshow(wc, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Ij4dzus2rlVl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Topic Modelling"
      ],
      "metadata": {
        "id": "EkLRzdjK7Zp-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a id2word dictionary\n",
        "id2word = Dictionary(df['lemma_tokens'])\n",
        "print(len(id2word))\n",
        "     \n",
        "\n",
        "# Filtering Extremes\n",
        "id2word.filter_extremes(no_below=2, no_above=.99)\n",
        "print(len(id2word))\n",
        "     \n",
        "\n",
        "# Creating a corpus object \n",
        "corpus = [id2word.doc2bow(d) for d in df['lemma_tokens']]\n",
        "     \n",
        "\n",
        "# Instantiating a Base LDA model \n",
        "base_model = LdaMulticore(corpus=corpus, num_topics=5, id2word=id2word, workers=12, passes=5)\n",
        "     \n",
        "\n",
        "# Filtering for words \n",
        "words = [re.findall(r'\"([^\"]*)\"',t[1]) for t in base_model.print_topics()]\n",
        "     \n",
        "\n",
        "# Create Topics\n",
        "topics = [' '.join(t[0:10]) for t in words]\n",
        "     \n",
        "\n",
        "# Getting the topics\n",
        "for id, t in enumerate(topics): \n",
        "    print(f\"------ Topic {id} ------\")\n",
        "    print(t, end=\"\\n\\n\")\n",
        "     "
      ],
      "metadata": {
        "id": "mv6gGB2KzN3z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0db4e77-534c-49ad-d6ff-78fd3ced8602"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16582\n",
            "7703\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.ldamulticore:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------ Topic 0 ------\n",
            "time go know life feel help think friend day talk\n",
            "\n",
            "------ Topic 1 ------\n",
            "feel life think time thing know people good day depression\n",
            "\n",
            "------ Topic 2 ------\n",
            "feel year go friend life time know day not people\n",
            "\n",
            "------ Topic 3 ------\n",
            "feel know i‚äôm don‚äôt think people friend thing life time\n",
            "\n",
            "------ Topic 4 ------\n",
            "year feel know life think work time new go people\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute Perplexity\n",
        "# a measure of how good the model is. lower the better\n",
        "base_perplexity = base_model.log_perplexity(corpus)\n",
        "print('\\nPerplexity: ', base_perplexity) \n",
        "\n",
        "# Compute Coherence Score\n",
        "coherence_model = CoherenceModel(model=base_model, texts=df['lemma_tokens'], \n",
        "                                   dictionary=id2word, coherence='c_v')\n",
        "coherence_lda_model_base = coherence_model.get_coherence()\n",
        "print('\\nCoherence Score: ', coherence_lda_model_base)"
      ],
      "metadata": {
        "id": "VMhdAqZg7eIb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40a47fba-17e2-40a3-e525-e53b4f3479ae"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Perplexity:  -7.161994797233211\n",
            "\n",
            "Coherence Score:  0.31758733549214\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating Topic Distance Visualization \n",
        "pyLDAvis.enable_notebook()\n",
        "pyLDAvis.gensim.prepare(base_model, corpus, id2word)"
      ],
      "metadata": {
        "id": "GO2EJ9uw7nHQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 951
        },
        "outputId": "cc103100-df67-44ca-a0c0-f9bb58a57f9e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "/usr/local/lib/python3.9/dist-packages/pyLDAvis/_prepare.py:243: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
            "  default_term_info = default_term_info.sort_values(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
              "topic                                                \n",
              "3      0.040270  0.011080       1        1  28.691390\n",
              "2     -0.009813  0.004053       2        1  25.850242\n",
              "0     -0.030290 -0.006757       3        1  18.680467\n",
              "1      0.010297 -0.033656       4        1  13.518304\n",
              "4     -0.010463  0.025280       5        1  13.259597, topic_info=         Term         Freq        Total Category  logprob  loglift\n",
              "3077    i‚äôm  2026.000000  2026.000000  Default  30.0000  30.0000\n",
              "104      year  3568.000000  3568.000000  Default  29.0000  29.0000\n",
              "3355  don‚äôt  1453.000000  1453.000000  Default  28.0000  28.0000\n",
              "47       life  3762.000000  3762.000000  Default  27.0000  27.0000\n",
              "98        new  1012.000000  1012.000000  Default  26.0000  26.0000\n",
              "...       ...          ...          ...      ...      ...      ...\n",
              "33     friend   240.523193  2972.920071   Topic5  -5.1935  -0.4940\n",
              "655      talk   193.597963  1800.169049   Topic5  -5.4106  -0.2094\n",
              "48       live   176.188616  1537.315998   Topic5  -5.5048  -0.1458\n",
              "420       try   182.562128  2392.032607   Topic5  -5.4693  -0.5524\n",
              "570      well   172.661363  1466.092572   Topic5  -5.5250  -0.1186\n",
              "\n",
              "[459 rows x 6 columns], token_table=      Topic      Freq         Term\n",
              "term                              \n",
              "5731      5  0.778750   accusation\n",
              "412       1  0.301723     actually\n",
              "412       2  0.192980     actually\n",
              "412       3  0.197575     actually\n",
              "412       4  0.117932     actually\n",
              "...     ...       ...          ...\n",
              "5494      5  0.068409           äú\n",
              "5589      4  0.846452        äúare\n",
              "7274      1  0.821565  äúnormal‚äù\n",
              "6993      1  0.880354         äúoh\n",
              "6515      1  0.771386         üòî\n",
              "\n",
              "[1097 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[4, 3, 1, 2, 5])"
            ],
            "text/html": [
              "\n",
              "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
              "\n",
              "\n",
              "<div id=\"ldavis_el1361405861644940002909141144\" style=\"background-color:white;\"></div>\n",
              "<script type=\"text/javascript\">\n",
              "\n",
              "var ldavis_el1361405861644940002909141144_data = {\"mdsDat\": {\"x\": [0.040269755058980665, -0.009813290657837379, -0.030290238830809463, 0.010297153213088482, -0.010463378783422291], \"y\": [0.01107985748125567, 0.004052643960676525, -0.006756772753997683, -0.033655980306781, 0.025280251618846502], \"topics\": [1, 2, 3, 4, 5], \"cluster\": [1, 1, 1, 1, 1], \"Freq\": [28.691390178245964, 25.850241817234576, 18.680467470775262, 13.518303945795699, 13.259596587948487]}, \"tinfo\": {\"Term\": [\"i\\u201a\\u00e4\\u00f4m\", \"year\", \"don\\u201a\\u00e4\\u00f4t\", \"life\", \"new\", \"feel\", \"not\", \"think\", \"i\\u201a\\u00e4\\u00f4ve\", \"work\", \"job\", \"it\\u201a\\u00e4\\u00f4s\", \"can\\u201a\\u00e4\\u00f4t\", \"fuck\", \"go\", \"know\", \"day\", \"amp\", \"love\", \"time\", \"depression\", \"fucking\", \"thing\", \"month\", \"get\", \"good\", \"pay\", \"week\", \"start\", \"money\", \"judgment\", \"anymorei\", \"don\\u201a\\u00e4\\u00f4t\", \"\\u00e4\\u00fa\", \"that\\u201a\\u00e4\\u00f9\", \"\\u00e4\\u00faoh\", \"they\\u201a\\u00e4\\u00f4d\", \"passively\", \"seeker\", \"i\\u201a\\u00e4\\u00f4m\", \"who\\u201a\\u00e4\\u00f4s\", \"it\\u201a\\u00e4\\u00f4ll\", \"she\\u201a\\u00e4\\u00f4d\", \"bojack\", \"hawaii\", \"\\u00e4\\u00fanormal\\u201a\\u00e4\\u00f9\", \"here\\u201a\\u00e4\\u00f9\", \"knot\", \"we\\u201a\\u00e4\\u00f4ve\", \"doormat\", \"\\uf8ff\\u00fc\\u00f2\\u00ee\", \"won\\u201a\\u00e4\\u00f4t\", \"unblocked\", \"puzzle\", \"hyper\", \"organized\", \"somethingi\", \"crime\", \"can\\u201a\\u00e4\\u00f4t\", \"that\\u201a\\u00e4\\u00f4s\", \"doesn\\u201a\\u00e4\\u00f4t\", \"i\\u201a\\u00e4\\u00f4ve\", \"you\\u201a\\u00e4\\u00f4ve\", \"she\\u201a\\u00e4\\u00f4s\", \"what\\u201a\\u00e4\\u00f4s\", \"shouldn\\u201a\\u00e4\\u00f4t\", \"it\\u201a\\u00e4\\u00f4s\", \"there\\u201a\\u00e4\\u00f4s\", \"isn\\u201a\\u00e4\\u00f4t\", \"wouldn\\u201a\\u00e4\\u00f4t\", \"i\\u201a\\u00e4\\u00f4d\", \"you\\u201a\\u00e4\\u00f4re\", \"he\\u201a\\u00e4\\u00f4s\", \"i\\u201a\\u00e4\\u00f4ll\", \"haven\\u201a\\u00e4\\u00f4t\", \"feel\", \"aren\\u201a\\u00e4\\u00f4t\", \"care\", \"know\", \"didn\\u201a\\u00e4\\u00f4t\", \"deserve\", \"hate\", \"cry\", \"hurt\", \"tell\", \"understand\", \"talk\", \"person\", \"people\", \"try\", \"thing\", \"think\", \"friend\", \"need\", \"love\", \"bad\", \"help\", \"feeling\", \"anymore\", \"time\", \"good\", \"life\", \"say\", \"end\", \"make\", \"well\", \"go\", \"start\", \"depression\", \"year\", \"coz\", \"steve\", \"mike\", \"mdd\", \"selfie\", \"sunny\", \"emo\", \"tumour\", \"feb\", \"tshirt\", \"basketball\", \"gad\", \"equivalent\", \"establish\", \"testing\", \"amphetamine\", \"advanced\", \"scientifically\", \"drinker\", \"maoi\", \"capsule\", \"dysphoria\", \"checkup\", \"premonition\", \"ibs\", \"legend\", \"roller\", \"prop\", \"athlete\", \"crybaby\", \"mini\", \"n\", \"coaster\", \"infront\", \"strip\", \"not\", \"off\", \"vyvanse\", \"cognitive\", \"class\", \"year\", \"school\", \"awhile\", \"go\", \"day\", \"party\", \"friend\", \"attack\", \"high\", \"try\", \"ve\", \"grade\", \"fun\", \"new\", \"hit\", \"panic\", \"fail\", \"weed\", \"lose\", \"feel\", \"anymore\", \"contact\", \"live\", \"lot\", \"s\", \"pass\", \"time\", \"people\", \"long\", \"work\", \"bad\", \"take\", \"life\", \"find\", \"away\", \"anxiety\", \"good\", \"hope\", \"way\", \"know\", \"thing\", \"help\", \"well\", \"start\", \"think\", \"depression\", \"tell\", \"emptypost\", \"sara\", \"ladder\", \"cataract\", \"kim\", \"monthly\", \"arthritis\", \"beating\", \"scamme\", \"paxil\", \"ge\", \"intimidate\", \"hud\", \"pizza\", \"wornout\", \"mm\", \"salvage\", \"eh\", \"grope\", \"sofa\", \"unpaid\", \"tyrant\", \"escort\", \"preserve\", \"queue\", \"ambulance\", \"medicaid\", \"ampnbsp\", \"vegas\", \"belly\", \"assist\", \"king\", \"eld\", \"skydive\", \"wife\", \"bill\", \"rehab\", \"pay\", \"u\", \"loan\", \"rent\", \"bank\", \"refuse\", \"john\", \"k\", \"money\", \"mum\", \"dump\", \"debt\", \"fuck\", \"ex\", \"go\", \"house\", \"time\", \"help\", \"dad\", \"kid\", \"home\", \"talk\", \"come\", \"need\", \"month\", \"say\", \"leave\", \"car\", \"depression\", \"mom\", \"shit\", \"life\", \"live\", \"job\", \"tell\", \"day\", \"know\", \"find\", \"think\", \"not\", \"love\", \"thing\", \"friend\", \"start\", \"end\", \"year\", \"people\", \"feel\", \"work\", \"good\", \"try\", \"i\\u201a\\u00e4\\u00f2m\", \"danny\", \"i\\u201a\\u00e4\\u00f2ve\", \"nan\", \"tap\", \"oversleep\", \"atypical\", \"electrical\", \"iv\", \"apple\", \"ibuprofen\", \"temperature\", \"hum\", \"ketamine\", \"monotone\", \"heartbeat\", \"amp\", \"furthermore\", \"can\\u201a\\u00e4\\u00f2t\", \"mathematical\", \"creativity\", \"briefly\", \"sensitivity\", \"fckin\", \"\\u00e4\\u00faare\", \"rift\", \"norway\", \"alpha\", \"thoughtless\", \"ashe\", \"dwell\", \"movement\", \"hungover\", \"mg\", \"effexor\", \"singer\", \"gti\", \"strategy\", \"body\", \"gt\", \"root\", \"tick\", \"life\", \"irrational\", \"matter\", \"dream\", \"worry\", \"negative\", \"death\", \"think\", \"problem\", \"love\", \"thing\", \"good\", \"wake\", \"depression\", \"thought\", \"sleep\", \"alive\", \"reason\", \"feel\", \"people\", \"time\", \"world\", \"tell\", \"day\", \"thank\", \"hard\", \"mind\", \"doctor\", \"know\", \"family\", \"live\", \"well\", \"find\", \"happy\", \"year\", \"try\", \"take\", \"friend\", \"work\", \"help\", \"go\", \"bad\", \"way\", \"end\", \"come\", \"fluoxetine\", \"promotion\", \"celebration\", \"accusation\", \"dancing\", \"bupropion\", \"adrenaline\", \"reckon\", \"collar\", \"quirky\", \"placebo\", \"psychotherapy\", \"raman\", \"douche\", \"extreamly\", \"hella\", \"indians\", \"peru\", \"unremarkable\", \"ending\", \"futility\", \"ness\", \"procrastinating\", \"today\\u201a\\u00e4\\u00f4s\", \"winner\", \"pt\", \"hoping\", \"homeschool\", \"calculus\", \"selfdoubt\", \"entertainment\", \"firework\", \"shrink\", \"employ\", \"decline\", \"celebrate\", \"indian\", \"army\", \"downvote\", \"reflection\", \"new\", \"birthday\", \"year\", \"variety\", \"boss\", \"gathering\", \"job\", \"performance\", \"work\", \"ticket\", \"loser\", \"fucking\", \"today\", \"fuck\", \"christmas\", \"ugly\", \"wish\", \"eve\", \"get\", \"week\", \"know\", \"think\", \"start\", \"watch\", \"life\", \"month\", \"kill\", \"feel\", \"spend\", \"old\", \"want\", \"end\", \"actually\", \"time\", \"way\", \"day\", \"happy\", \"people\", \"go\", \"depression\", \"bad\", \"thing\", \"friend\", \"talk\", \"live\", \"try\", \"well\"], \"Freq\": [2026.0, 3568.0, 1453.0, 3762.0, 1012.0, 7319.0, 1568.0, 3198.0, 918.0, 1896.0, 1073.0, 840.0, 690.0, 1225.0, 3191.0, 4628.0, 2469.0, 149.0, 1450.0, 3508.0, 1891.0, 740.0, 2711.0, 1135.0, 1263.0, 1933.0, 336.0, 795.0, 1642.0, 494.0, 6.649627349374549, 4.8805174977828205, 1211.1003941249521, 12.163097069979878, 5.676044378156545, 5.656822766437564, 8.878656384386318, 5.6190642875546, 8.030632307793137, 1644.8984114881785, 16.483788834912016, 23.61594216826896, 7.078508644394671, 3.907570847949096, 3.895489948870409, 3.8810063057008186, 3.8639228156749907, 6.175608839076492, 8.402014156654909, 4.567091364110572, 3.0449771941087054, 100.10567096478258, 3.7790543888195334, 3.010124446477316, 7.530678756466191, 3.0065929650970444, 2.9991562573005046, 3.748215990242659, 529.2130626352213, 141.00124564499643, 150.8044742726733, 683.5494438664554, 32.56904542654989, 60.482908193214776, 50.726662317435206, 31.46973032851885, 584.0840718061153, 86.1443405750958, 60.937971419232824, 58.902375900843815, 78.4200509777197, 73.84431505461049, 41.37865514360185, 137.1705063415959, 86.26172379705821, 3272.0396888959044, 24.69885886183742, 524.8234953890935, 1893.6459970176447, 134.92154332864234, 143.26585684427462, 533.0078242047917, 349.9428351248824, 333.3743739428555, 820.0340162162938, 268.1725936619428, 706.9615749552042, 417.9618345574204, 985.7373534812858, 835.8890117649336, 910.6856837325818, 1035.324862753532, 959.3398016063175, 540.154964168479, 535.9557068034541, 630.946134065502, 700.4593934326127, 314.45425443302076, 448.522633326792, 904.8363611300949, 605.3762119280549, 909.0761684674924, 428.0160889021209, 454.8728724878883, 409.80269124662965, 473.3967895227755, 648.9579603836231, 474.0614336878109, 486.6947361138986, 488.300017420624, 16.415517775146984, 6.61380646686871, 6.61259041055359, 5.779546363162089, 5.7381254818636735, 4.08950859535739, 4.875103403451752, 4.034421981658725, 4.014159804315167, 4.811626984347083, 4.008718889387312, 5.57153989973692, 5.5733624480091, 6.28685875807732, 3.1357155018522778, 3.914904353865225, 4.697978256940498, 3.1212395773087804, 3.11841757699186, 3.1129788127447044, 3.88461416690338, 5.456345754628201, 5.412161555871933, 3.0810467346893784, 3.0738794376405902, 3.0609845706508083, 3.0547946043595733, 3.051729884363789, 3.049581968825891, 4.567924751643696, 6.813368743390204, 30.41638644732957, 3.797293458131258, 8.911564844893135, 4.524071450474943, 877.7442817035994, 7.277128010714215, 5.885796134484615, 7.862816740615374, 190.6680541512764, 1447.2541264676186, 593.3712218968642, 29.075701258116023, 1226.6444480187915, 939.7246283240867, 107.05270522565466, 1062.0015868279042, 81.67897925709924, 199.51702128375038, 815.6636519145284, 99.7334890081452, 135.0996756864769, 131.2819347852219, 372.12110760474775, 124.20461730102984, 67.84022152222951, 147.54848412556626, 46.413562415669524, 330.3795561272614, 1858.2672491462251, 410.01489222050304, 69.27006287257156, 500.00279408388576, 328.5186151322071, 125.88148952331682, 110.46076971987819, 958.4314899184538, 827.2600829157045, 376.3434222694563, 571.7179228293047, 547.3995690615793, 325.54400005497934, 964.5018070831328, 414.2493195189072, 309.8646873201591, 237.89314827702768, 531.6988429207614, 261.51349186467655, 434.299307017841, 949.8913752818518, 635.2434561073717, 522.265854143168, 404.8558541077556, 430.1842756801508, 598.9498762483528, 427.7880585252553, 423.9498480595893, 63.791174348470754, 19.05501316252185, 7.0463736322081125, 5.837767371766276, 13.246536159447906, 5.732571688894583, 4.911539501726838, 5.714842139293618, 4.81618705591289, 12.084700261688486, 11.14945286572687, 4.762302138074649, 3.9550905360192448, 6.325547791890911, 3.9488822851371075, 4.736493114559051, 3.9241132246832344, 3.918350355078172, 3.9007988691782116, 7.019652087731011, 3.8610653530539274, 17.786172969785525, 3.850105776484152, 3.0791923044612184, 3.0697757986171013, 10.017979271258845, 3.0480062900342557, 23.878344080833124, 3.038646178429609, 4.559008009571031, 12.820884873303259, 5.257702441264692, 8.204541240107199, 4.541878370344271, 77.7310630285822, 69.57032101843069, 10.02035759839744, 181.3828439146465, 69.44981674181308, 23.466772704492552, 42.76852087782605, 13.972311371364585, 44.74135656076623, 26.03937488972021, 41.813271732866866, 195.33223879852625, 53.81194458046481, 25.893907947228183, 41.72581181628396, 366.6380235933075, 62.77074234366673, 776.3326991113184, 155.85654115133042, 815.3454773170133, 542.6504307617429, 156.42500929476114, 130.98798527326318, 238.45270591175517, 444.1856836741503, 331.5599895925096, 369.0259706122501, 308.50271366682375, 311.8519382417101, 284.39225267252823, 98.1086694190871, 418.8673522088209, 188.46701190042955, 260.774124934084, 663.9336761246136, 343.5791561955408, 265.46302797126725, 405.4771892464743, 455.3431534028645, 689.4790197146107, 293.5554942074305, 519.4522811377625, 323.6161282820827, 307.6489173146221, 437.0218994556111, 459.8864651637994, 317.8300473409136, 275.7315468662809, 432.7513830608109, 392.89060600835154, 583.807660758019, 318.3263024386783, 316.8120735920915, 322.5271350916552, 8.376340234822402, 9.06126148287394, 4.735758140049394, 3.920286522247966, 4.701677086003042, 6.9640364452923, 5.4134045601037295, 3.776268771369812, 5.288348149197789, 5.272441472193966, 5.248180584391464, 5.239672772824966, 4.464445198873254, 3.695899901002724, 3.6852371799194956, 3.668409540989793, 115.4099356391222, 4.311370144142051, 2.87649314721235, 3.5901419162728536, 2.83981276971225, 2.8391302424510614, 3.550928732806326, 2.8295115567122795, 3.530587877988403, 2.823603711280565, 3.516294330387202, 2.7661489906031522, 2.772517959678843, 2.7575017172516465, 8.440187382433352, 11.726428894213472, 5.258628650008367, 54.938333926115604, 12.33311593696199, 5.160777891416011, 9.549859741178082, 6.27945774623343, 106.77804035946744, 14.501765974260683, 11.960046021851209, 7.232917481424805, 711.1327987665187, 15.597924036161455, 126.42979714528026, 75.89858416161098, 87.06288951617468, 54.95847297908794, 82.86148820291626, 549.5334592684795, 162.78451531036114, 290.42976111552554, 457.1410164297667, 354.64533976986235, 110.30582454143236, 332.55455189307816, 204.47659151649233, 157.94666953698362, 72.27849555840217, 144.14035899565033, 813.7055971508512, 419.1195949708277, 462.0794392146951, 130.7418372844033, 306.09908572952463, 344.9696482231073, 103.95237235669634, 176.9220893653548, 119.38252707889889, 76.50184571443282, 437.3744922351082, 187.86533300901763, 210.74407868573243, 196.84448170653087, 184.24700792257408, 172.22727868355, 289.0343103508261, 235.39068051006532, 152.83326270077603, 251.16902439371603, 198.29965553462745, 191.60450269619182, 214.60559427653942, 183.08678870027705, 169.57180290224693, 163.57432403167806, 158.61850258571866, 5.198015836198495, 3.5908129610564465, 4.280687868359633, 4.918990304051053, 2.807345284274654, 2.7989876838318213, 3.4406601400366954, 2.7387282373443225, 3.3935570746053245, 3.362609115928267, 4.641718178573677, 2.650262362148531, 2.6460548795602303, 2.6446245435061657, 2.6314973176031895, 2.6190170209093293, 3.2637271288817327, 2.6037285313830125, 1.9527981320869061, 2.605241605033858, 2.5901192210827926, 1.9452790968868763, 1.9248350307108604, 1.9092940733177948, 1.9057393509692404, 2.531890391613432, 3.7802369853939957, 2.5145421299158257, 2.492200457258211, 1.8711726862021183, 10.526860457908006, 16.49562633548824, 9.17247594173418, 6.742830465886727, 10.825279997931748, 30.201750655621314, 4.909366581984338, 12.11787008820351, 3.6756598149713287, 6.874110604687611, 336.72795130066254, 54.04493702465449, 910.8743899024669, 5.234311953131473, 26.88610182367169, 5.690473794528826, 290.884880183149, 12.272902378692846, 438.776923004487, 11.00053619379195, 31.076150575960085, 190.7425458090108, 134.5733586523115, 262.99719767246614, 43.508228113607366, 46.85916571254973, 165.78853864918466, 31.091599970412712, 254.01570090669816, 177.76073761499177, 657.7583697433936, 494.8526119489612, 293.1807000389323, 80.28294091155759, 513.6412542024767, 214.08912000437815, 171.0928324256048, 791.3822626237497, 119.69131780657412, 133.30714877499278, 180.45459559884986, 199.02138384199813, 123.88489921785192, 368.10058551447713, 212.26668149622964, 289.6384592478352, 180.84482708007863, 313.61692804901753, 324.6508669077865, 225.47125145024916, 218.93200464221536, 271.7615808981739, 240.52319276901054, 193.59796305537398, 176.18861611126653, 182.56212800848226, 172.66136327522517], \"Total\": [2026.0, 3568.0, 1453.0, 3762.0, 1012.0, 7319.0, 1568.0, 3198.0, 918.0, 1896.0, 1073.0, 840.0, 690.0, 1225.0, 3191.0, 4628.0, 2469.0, 149.0, 1450.0, 3508.0, 1891.0, 740.0, 2711.0, 1135.0, 1263.0, 1933.0, 336.0, 795.0, 1642.0, 494.0, 7.799786130837192, 5.834350550126366, 1453.0043480116592, 14.617865253515685, 6.823201804297063, 6.81544215113713, 10.699328164516716, 6.822860560790404, 9.753352893344529, 2026.1509334728962, 20.368205985547107, 29.218977153211537, 8.768829034846787, 4.8676460583997985, 4.864750827688203, 4.86875801411466, 4.868043144686576, 7.808308965535637, 10.679843288784776, 5.819260305214329, 3.889103172259747, 128.4105733589288, 4.874878854982483, 3.8891457298303655, 9.729863837298755, 3.8911597888694893, 3.892608333361414, 4.8740062214448665, 690.305009963785, 184.1092152950018, 199.08576261492266, 918.402385115748, 42.87793542150052, 80.88872081285245, 67.88884596834559, 42.024816087285735, 840.8767661521053, 118.00927162947524, 83.51871349061135, 81.01200722397911, 110.61646215384927, 105.27948231086378, 57.29626097095239, 205.1887917282143, 126.04379992255551, 7319.202458574749, 35.02050556600916, 1081.885099097925, 4628.149253992609, 241.41114024926736, 263.11094275282494, 1216.5556449015296, 746.0923486495494, 714.2420239998781, 2068.940287388831, 555.9957462601958, 1800.1690492749913, 968.9895664186938, 2938.624565425187, 2392.032607289665, 2711.8536366235053, 3198.113091357088, 2972.920070760748, 1431.4405532181652, 1450.1004575231557, 1821.6489995959473, 2106.9874418782474, 731.8607341141446, 1206.101833743225, 3508.7933530947344, 1933.934106118612, 3762.285704644234, 1184.1736855396975, 1336.653446291956, 1131.5157848807714, 1466.0925717955422, 3191.191568698059, 1642.38638547324, 1891.375950191302, 3568.2142272023466, 18.150337868978905, 8.020411766344468, 8.019815533552135, 7.021035736047902, 7.006239653264614, 5.0043957282829705, 6.005484629544144, 5.007635118142545, 4.996846111056911, 5.99914042698213, 4.998933071165414, 6.989396364276562, 7.015117372310936, 7.979176404503913, 3.9929093287068294, 4.991294560094296, 5.99261013551263, 3.993347173619722, 3.9913221015719227, 3.992492990748834, 4.989146539490582, 7.013075870499671, 7.007099608517576, 3.990254386048336, 3.991608382069053, 3.9886417048867564, 3.9888279203322536, 3.9908692581307026, 3.991639662042895, 5.982474831664198, 8.958499272132098, 40.986605730934905, 4.97970460416933, 11.928891130035446, 5.972408794833255, 1568.733909812229, 9.984755202376009, 7.970090201109488, 10.967464984253715, 365.4343044617833, 3568.2142272023466, 1333.6322112913676, 47.54744148132315, 3191.191568698059, 2469.1310860221815, 210.86793941827386, 2972.920070760748, 159.94038198394344, 452.69666757491876, 2392.032607289665, 208.88768895255674, 297.607941823744, 296.1920008477869, 1012.0440953965594, 280.41329666819144, 137.4763852458361, 347.5167623195009, 87.75784758514166, 909.6202811567389, 7319.202458574749, 1206.101833743225, 141.84980000007363, 1537.3159979420116, 937.3177775925939, 291.6680989761868, 249.4727926166857, 3508.7933530947344, 2938.624565425187, 1124.2108250806943, 1896.64728955594, 1821.6489995959473, 952.3117883199957, 3762.285704644234, 1296.301992088317, 897.020903631998, 656.699684168535, 1933.934106118612, 747.3801627217059, 1502.9091414324519, 4628.149253992609, 2711.8536366235053, 2106.9874418782474, 1466.0925717955422, 1642.38638547324, 3198.113091357088, 1891.375950191302, 2068.940287388831, 66.98322905352688, 21.291442779863537, 8.087630551080183, 7.0532155632936275, 16.088964209293632, 7.044645910430154, 6.038565422211071, 7.04606540975676, 6.032478158804529, 15.155750200375742, 14.035952055764472, 6.034279547076244, 5.023291700034441, 8.035860040933889, 5.023534192285321, 6.025759696977578, 5.0171623196533375, 5.0197155291290505, 5.0086221206649935, 9.054113320324118, 5.011126008948607, 23.16235236204953, 5.016047930885552, 4.012538652845741, 4.008930056908449, 13.102885851611187, 4.006263860945195, 31.40460971647133, 4.003482056096396, 6.030262528771282, 17.07201169783299, 6.984460225213878, 11.021059079609698, 6.026344686980122, 122.969358047956, 111.84081864858814, 13.98191520092398, 336.41568195676643, 118.1840750954398, 36.723082471379485, 72.99437917268088, 20.986505550034632, 79.03450509753269, 43.210910262090565, 75.3791514115368, 494.9423452169137, 107.41116514191509, 45.61534219061157, 82.11770124377574, 1225.529330825238, 137.41255628726637, 3191.191568698059, 430.36033552918224, 3508.7933530947344, 2106.9874418782474, 452.11196138581136, 364.4200323325179, 788.9235114344189, 1800.1690492749913, 1242.4650063341282, 1431.4405532181652, 1135.5429763721093, 1184.1736855396975, 1065.4264091526345, 253.52523197491809, 1891.375950191302, 625.2419646611748, 996.1278244707487, 3762.285704644234, 1537.3159979420116, 1073.1541989297807, 2068.940287388831, 2469.1310860221815, 4628.149253992609, 1296.301992088317, 3198.113091357088, 1568.733909812229, 1450.1004575231557, 2711.8536366235053, 2972.920070760748, 1642.38638547324, 1336.653446291956, 3568.2142272023466, 2938.624565425187, 7319.202458574749, 1896.64728955594, 1933.934106118612, 2392.032607289665, 9.436447970975607, 10.378777899674208, 5.669465861887204, 4.716173428092984, 5.664953185595222, 8.487861382835636, 6.6204313658719816, 4.724228282015273, 6.631004586617358, 6.630160476191445, 6.626507843767179, 6.630326783226935, 5.663920602247125, 4.727682532782281, 4.722264225684033, 4.72098428389562, 149.01874772166371, 5.665636492420276, 3.7870870558447183, 4.733592166114006, 3.782464271550058, 3.789474977791363, 4.741134735362791, 3.7826241504075346, 4.725606515484779, 3.7877136436855103, 4.73278369975555, 3.769901976935215, 3.7857964381043607, 3.7750155499152873, 12.306908429058518, 18.17338797169458, 7.609458513393349, 107.04663884002204, 20.244269734428, 7.514363917806345, 15.289220587113045, 9.500902809494683, 288.1359431412507, 26.1001501002012, 21.19217488031622, 11.423757161779342, 3762.285704644234, 30.641273914224058, 451.70963286070037, 233.7984485467526, 283.60709816800966, 156.2637911013544, 266.7396246012832, 3198.113091357088, 659.35757451196, 1450.1004575231557, 2711.8536366235053, 1933.934106118612, 410.9811964485014, 1891.375950191302, 976.0095758011508, 689.6549010310029, 245.85050480501974, 656.2015970104017, 7319.202458574749, 2938.624565425187, 3508.7933530947344, 596.6345988845553, 2068.940287388831, 2469.1310860221815, 438.9394218225327, 988.300147222059, 573.704744999204, 286.95439099915455, 4628.149253992609, 1201.024658138617, 1537.3159979420116, 1466.0925717955422, 1296.301992088317, 1177.917860402074, 3568.2142272023466, 2392.032607289665, 952.3117883199957, 2972.920070760748, 1896.64728955594, 2106.9874418782474, 3191.191568698059, 1821.6489995959473, 1502.9091414324519, 1336.653446291956, 1242.4650063341282, 6.392350417382841, 4.578397070438246, 5.498301662126788, 6.420549270674705, 3.6668291578824475, 3.6650783196093846, 4.592984029085709, 3.6718434863796987, 4.593346543754288, 4.589068445513601, 6.435747115742191, 3.6758675344292326, 3.677211827547093, 3.68073291182705, 3.6752671605278944, 3.6713635889334095, 4.596280691588451, 3.6795769232917186, 2.7632547362260182, 3.68757006534475, 3.6710109599737324, 2.7642316800811404, 2.7656316102386773, 2.7645555437362725, 2.7657445113306856, 3.68522359584232, 5.51195053879469, 3.6872470137698308, 3.6756766217344854, 2.766007795600027, 15.72628932350679, 25.08426699837603, 13.823929570414021, 10.119067168115784, 16.686548532020925, 50.6345549529773, 7.413681600445636, 19.638264122277118, 5.534054426547083, 11.215857867188536, 1012.0440953965594, 125.29794866373078, 3568.2142272023466, 8.329939406644518, 58.65178906286428, 9.262449155950957, 1073.1541989297807, 23.41331377577166, 1896.64728955594, 20.807091517292122, 75.79209106716888, 740.6568377127702, 497.0595125612275, 1225.529330825238, 122.27844782560209, 135.3312326431548, 706.7960621325958, 80.74906984509752, 1263.4112383990937, 795.3678367291449, 4628.149253992609, 3198.113091357088, 1642.38638547324, 295.16184116098793, 3762.285704644234, 1135.5429763721093, 862.3456909843534, 7319.202458574749, 533.639767959093, 629.8391289174035, 997.9094819493387, 1336.653446291956, 652.9164973960885, 3508.7933530947344, 1502.9091414324519, 2469.1310860221815, 1177.917860402074, 2938.624565425187, 3191.191568698059, 1891.375950191302, 1821.6489995959473, 2711.8536366235053, 2972.920070760748, 1800.1690492749913, 1537.3159979420116, 2392.032607289665, 1466.0925717955422], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -9.5537, -9.863, -4.3489, -8.9498, -9.712, -9.7154, -9.2646, -9.7221, -9.365, -4.0428, -8.6459, -8.2863, -9.4912, -10.0853, -10.0884, -10.0921, -10.0966, -9.6276, -9.3198, -9.9294, -10.3347, -6.842, -10.1188, -10.3463, -9.4292, -10.3474, -10.3499, -10.127, -5.1768, -6.4995, -6.4322, -4.9209, -7.9649, -7.3459, -7.5218, -7.9992, -5.0782, -6.9922, -7.3384, -7.3724, -7.0862, -7.1463, -7.7255, -6.527, -6.9908, -3.3551, -8.2415, -5.1852, -3.902, -6.5435, -6.4835, -5.1697, -5.5905, -5.639, -4.7389, -5.8566, -4.8873, -5.4128, -4.5548, -4.7197, -4.634, -4.5058, -4.582, -5.1564, -5.1642, -5.001, -4.8965, -5.6974, -5.3423, -4.6405, -5.0424, -4.6358, -5.3891, -5.3282, -5.4326, -5.2883, -4.9729, -5.2869, -5.2606, -5.2573, -8.5457, -9.4548, -9.455, -9.5896, -9.5968, -9.9355, -9.7598, -9.9491, -9.9541, -9.7729, -9.9555, -9.6263, -9.626, -9.5055, -10.2011, -9.9792, -9.7968, -10.2057, -10.2066, -10.2084, -9.9869, -9.6472, -9.6553, -10.2187, -10.221, -10.2252, -10.2272, -10.2282, -10.229, -9.8249, -9.4251, -7.929, -10.0097, -9.1566, -9.8345, -4.5666, -9.3592, -9.5714, -9.2818, -6.0934, -4.0665, -4.9581, -7.9741, -4.2319, -4.4984, -6.6706, -4.376, -6.9412, -6.0481, -4.64, -6.7415, -6.4379, -6.4666, -5.4247, -6.522, -7.1268, -6.3498, -7.5064, -5.5437, -3.8166, -5.3278, -7.1059, -5.1293, -5.5494, -6.5086, -6.6393, -4.4787, -4.6258, -5.4135, -4.9953, -5.0388, -5.5585, -4.4723, -5.3175, -5.6078, -5.8721, -5.0679, -5.7775, -5.2702, -4.4876, -4.8899, -5.0858, -5.3404, -5.2797, -4.9488, -5.2853, -5.2943, -6.8635, -8.0718, -9.0666, -9.2548, -8.4354, -9.273, -9.4275, -9.276, -9.4471, -8.5272, -8.6077, -9.4584, -9.6441, -9.1745, -9.6457, -9.4638, -9.652, -9.6534, -9.6579, -9.0704, -9.6682, -8.1407, -9.671, -9.8944, -9.8975, -8.7147, -9.9046, -7.8461, -9.9077, -9.502, -8.468, -9.3594, -8.9144, -9.5058, -6.6659, -6.7768, -8.7145, -5.8185, -6.7785, -7.8635, -7.2633, -8.382, -7.2182, -7.7595, -7.2859, -5.7444, -7.0336, -7.7651, -7.288, -5.1147, -6.8796, -4.3645, -5.9702, -4.3155, -4.7226, -5.9665, -6.144, -5.5449, -4.9229, -5.2153, -5.1082, -5.2874, -5.2766, -5.3688, -6.433, -4.9816, -5.7802, -5.4555, -4.5209, -5.1797, -5.4376, -5.0141, -4.8981, -4.4832, -5.337, -4.7663, -5.2396, -5.2902, -4.9391, -4.8881, -5.2576, -5.3997, -4.949, -5.0456, -4.6495, -5.256, -5.2608, -5.2429, -8.5703, -8.4917, -9.1405, -9.3295, -9.1478, -8.7549, -9.0068, -9.3669, -9.0302, -9.0332, -9.0378, -9.0394, -9.1995, -9.3885, -9.3913, -9.3959, -5.9472, -9.2344, -9.6391, -9.4175, -9.6519, -9.6522, -9.4285, -9.6556, -9.4342, -9.6577, -9.4383, -9.6782, -9.6759, -9.6814, -8.5627, -8.2338, -9.0358, -6.6895, -8.1834, -9.0546, -8.4392, -8.8584, -6.0249, -8.0214, -8.2141, -8.717, -4.1288, -7.9485, -5.856, -6.3663, -6.2291, -6.6891, -6.2785, -4.3866, -5.6033, -5.0243, -4.5707, -4.8246, -5.9924, -4.8889, -5.3752, -5.6334, -6.4152, -5.7249, -3.9941, -4.6575, -4.5599, -5.8225, -4.9718, -4.8522, -6.0517, -5.52, -5.9133, -6.3584, -4.6149, -5.46, -5.345, -5.4133, -5.4794, -5.5469, -5.0291, -5.2344, -5.6663, -5.1696, -5.4059, -5.4402, -5.3269, -5.4857, -5.5624, -5.5984, -5.6292, -9.0281, -9.398, -9.2222, -9.0833, -9.6441, -9.6471, -9.4407, -9.6689, -9.4545, -9.4636, -9.1413, -9.7017, -9.7033, -9.7038, -9.7088, -9.7136, -9.4935, -9.7194, -10.0071, -9.7188, -9.7247, -10.011, -10.0215, -10.0296, -10.0315, -9.7474, -9.3466, -9.7543, -9.7632, -10.0498, -8.3224, -7.8733, -8.4602, -8.7679, -8.2945, -7.2685, -9.0852, -8.1817, -9.3746, -8.7486, -4.8571, -6.6865, -3.862, -9.0211, -7.3847, -8.9376, -5.0034, -8.169, -4.5924, -8.2784, -7.2399, -5.4254, -5.7742, -5.1042, -6.9034, -6.8292, -5.5656, -7.2394, -5.139, -5.4959, -4.1875, -4.4721, -4.9956, -6.2908, -4.4348, -5.31, -5.5342, -4.0026, -5.8914, -5.7837, -5.4809, -5.3829, -5.857, -4.768, -5.3185, -5.0077, -5.4787, -4.9282, -4.8936, -5.2582, -5.2876, -5.0714, -5.1935, -5.4106, -5.5048, -5.4693, -5.525], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 1.089, 1.0701, 1.0665, 1.0647, 1.0645, 1.0622, 1.062, 1.0545, 1.0542, 1.0401, 1.037, 1.0357, 1.0344, 1.0289, 1.0264, 1.0218, 1.0176, 1.014, 1.0087, 1.0063, 1.0039, 0.9996, 0.994, 0.9924, 0.9924, 0.9907, 0.9878, 0.9859, 0.9828, 0.9818, 0.9708, 0.9532, 0.9736, 0.9579, 0.9572, 0.9593, 0.8842, 0.9338, 0.9334, 0.9299, 0.9046, 0.8939, 0.9231, 0.8459, 0.8693, 0.4435, 0.8994, 0.5252, 0.3549, 0.6668, 0.6407, 0.4233, 0.4915, 0.4866, 0.3231, 0.5194, 0.3139, 0.4077, 0.1563, 0.1972, 0.1574, 0.1207, 0.1175, 0.274, 0.2532, 0.1883, 0.1473, 0.4038, 0.2594, -0.1067, 0.0871, -0.1718, 0.2309, 0.1707, 0.2329, 0.1182, -0.3442, 0.006, -0.1088, -0.7403, 1.2524, 1.16, 1.1599, 1.1583, 1.1532, 1.151, 1.1443, 1.1367, 1.1339, 1.1323, 1.1321, 1.1261, 1.1228, 1.1145, 1.1112, 1.1099, 1.1095, 1.1065, 1.1061, 1.104, 1.1026, 1.1019, 1.0946, 1.0943, 1.0916, 1.0881, 1.0861, 1.0845, 1.0837, 1.0831, 1.0791, 1.0546, 1.0818, 1.0612, 1.0751, 0.7722, 1.0365, 1.0497, 1.0201, 0.7023, 0.4505, 0.543, 0.861, 0.3967, 0.3868, 0.6749, 0.3235, 0.6808, 0.5335, 0.277, 0.6136, 0.5631, 0.5392, 0.3523, 0.5385, 0.6466, 0.4962, 0.7159, 0.3401, -0.018, 0.2739, 0.6361, 0.2297, 0.3044, 0.5126, 0.5382, 0.0551, 0.0853, 0.2585, 0.1537, 0.1505, 0.2795, -0.0083, 0.212, 0.2899, 0.3374, 0.0616, 0.3028, 0.1114, -0.2307, -0.0985, -0.042, 0.066, 0.0132, -0.3223, -0.1336, -0.2323, 1.6289, 1.5667, 1.5399, 1.4886, 1.4833, 1.4716, 1.4711, 1.4683, 1.4525, 1.4513, 1.4475, 1.441, 1.4386, 1.4384, 1.437, 1.4369, 1.432, 1.43, 1.4277, 1.4232, 1.417, 1.4136, 1.4132, 1.4129, 1.4108, 1.4092, 1.4043, 1.4037, 1.4019, 1.398, 1.3913, 1.3937, 1.3826, 1.3949, 1.219, 1.203, 1.3445, 1.06, 1.1461, 1.2299, 1.1431, 1.2709, 1.1087, 1.1712, 1.0884, 0.748, 0.9865, 1.1115, 1.0007, 0.4709, 0.8942, 0.2641, 0.662, 0.2183, 0.3211, 0.6163, 0.6545, 0.4812, 0.2783, 0.3566, 0.3221, 0.3746, 0.3434, 0.3569, 0.7283, 0.1702, 0.4785, 0.3375, -0.0569, 0.1793, 0.2808, 0.048, -0.0129, -0.2263, 0.1925, -0.1398, 0.0992, 0.1273, -0.1477, -0.1886, 0.0353, 0.0992, -0.432, -0.3345, -0.851, -0.1071, -0.1313, -0.326, 1.882, 1.8654, 1.8212, 1.8163, 1.8147, 1.8032, 1.7998, 1.7772, 1.7749, 1.772, 1.7679, 1.7657, 1.7632, 1.7549, 1.7532, 1.7489, 1.7455, 1.728, 1.7261, 1.7246, 1.7145, 1.7124, 1.7121, 1.7108, 1.7096, 1.7074, 1.704, 1.6915, 1.6896, 1.687, 1.624, 1.563, 1.6316, 1.3341, 1.5055, 1.6254, 1.5305, 1.587, 1.0084, 1.4135, 1.4291, 1.5441, 0.3352, 1.3259, 0.7278, 0.8761, 0.8202, 0.9562, 0.832, 0.2399, 0.6023, 0.3931, 0.2207, 0.3049, 0.6858, 0.2629, 0.4381, 0.5272, 0.7769, 0.4854, -0.1955, 0.0536, -0.0262, 0.483, 0.0902, 0.033, 0.5607, 0.2808, 0.4313, 0.6791, -0.358, 0.1459, 0.014, -0.0068, 0.0501, 0.0784, -0.5121, -0.3175, 0.1716, -0.47, -0.2569, -0.3965, -0.6982, -0.2964, -0.1808, -0.0995, -0.0572, 1.8136, 1.7775, 1.7701, 1.754, 1.7534, 1.7509, 1.7316, 1.7272, 1.7177, 1.7095, 1.6937, 1.6933, 1.6914, 1.6899, 1.6864, 1.6827, 1.6781, 1.6746, 1.6733, 1.673, 1.6717, 1.6691, 1.658, 1.6503, 1.648, 1.6451, 1.6433, 1.6377, 1.6319, 1.6296, 1.619, 1.6013, 1.6103, 1.6145, 1.5877, 1.5037, 1.6083, 1.5376, 1.6113, 1.5309, 0.92, 1.1796, 0.655, 1.5558, 1.2404, 1.5333, 0.715, 1.3745, 0.5566, 1.3831, 1.1289, 0.6638, 0.7138, 0.4815, 0.9871, 0.9599, 0.5704, 1.066, 0.4163, 0.5221, 0.0694, 0.1544, 0.2973, 0.7185, 0.0292, 0.352, 0.403, -0.204, 0.5256, 0.4676, 0.3103, 0.1159, 0.3584, -0.2342, 0.0631, -0.1225, 0.1466, -0.2171, -0.265, -0.1064, -0.0983, -0.28, -0.494, -0.2094, -0.1458, -0.5524, -0.1186]}, \"token.table\": {\"Topic\": [5, 1, 2, 3, 4, 5, 5, 1, 2, 1, 2, 3, 4, 5, 4, 1, 2, 3, 5, 1, 2, 3, 4, 5, 2, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 4, 1, 2, 3, 4, 5, 1, 2, 3, 5, 3, 4, 1, 2, 3, 4, 5, 2, 1, 2, 3, 4, 5, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 2, 3, 2, 3, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 1, 2, 3, 4, 5, 4, 5, 5, 4, 1, 2, 3, 4, 5, 2, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 3, 1, 2, 3, 4, 5, 5, 2, 3, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 2, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 4, 1, 1, 2, 3, 4, 5, 2, 1, 2, 3, 4, 5, 5, 1, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 5, 1, 5, 1, 2, 3, 4, 5, 2, 1, 2, 3, 4, 5, 1, 2, 4, 5, 2, 3, 2, 3, 4, 3, 1, 2, 3, 4, 4, 2, 1, 3, 4, 5, 3, 5, 1, 2, 3, 4, 5, 5, 1, 2, 3, 4, 5, 2, 3, 3, 2, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 4, 2, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 4, 5, 2, 1, 2, 4, 5, 1, 2, 3, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 3, 1, 2, 3, 4, 1, 2, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 4, 5, 1, 2, 3, 4, 5, 1, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 5, 1, 2, 3, 4, 5, 1, 5, 1, 2, 3, 4, 5, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 2, 4, 2, 2, 4, 1, 3, 5, 5, 1, 2, 3, 5, 3, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 4, 5, 1, 2, 3, 4, 5, 3, 4, 4, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 1, 2, 3, 4, 5, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 3, 5, 1, 2, 1, 2, 3, 4, 5, 3, 1, 2, 3, 4, 5, 2, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 2, 4, 1, 2, 3, 4, 5, 2, 3, 1, 2, 3, 4, 5, 2, 1, 2, 3, 4, 5, 1, 2, 5, 3, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 4, 1, 2, 3, 4, 5, 3, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 5, 1, 2, 3, 4, 5, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 1, 2, 3, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 5, 2, 3, 2, 5, 2, 3, 1, 2, 3, 4, 5, 5, 5, 2, 5, 5, 1, 3, 1, 5, 5, 1, 2, 3, 4, 5, 5, 1, 2, 3, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 4, 2, 1, 2, 3, 4, 1, 2, 3, 4, 5, 3, 1, 2, 3, 4, 1, 2, 3, 4, 5, 3, 1, 2, 3, 4, 5, 2, 1, 3, 5, 2, 4, 1, 3, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 4, 5, 2, 3, 1, 2, 3, 4, 5, 1, 2, 3, 1, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 2, 1, 2, 3, 4, 5, 2, 2, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 4, 1, 2, 3, 4, 5, 2, 4, 2, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 1, 2, 3, 4, 5, 1, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 4, 1, 2, 4, 5, 1, 2, 3, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 5, 1, 2, 3, 4, 5, 2, 2, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 1, 2, 3, 4, 5, 3, 5, 1, 2, 5, 1, 2, 3, 4, 5, 3, 2, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 3, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 5, 4, 1, 1, 1], \"Freq\": [0.7787495725384526, 0.3017231158741742, 0.19298026700581702, 0.1975750352678603, 0.11793238539244374, 0.18991708816445485, 0.6531701353634333, 0.16687219381650234, 0.8343609690825117, 0.33760353701866924, 0.17083552475643504, 0.10168781235502085, 0.29286089958246003, 0.09762029986082002, 0.7957766590098145, 0.07631906522920946, 0.15263813045841892, 0.7631906522920946, 0.07631906522920946, 0.10065847572425522, 0.04697395533798577, 0.05368452038626945, 0.7717149805526233, 0.026842260193134727, 0.801395299724493, 0.03184245908572818, 0.1592122954286409, 0.7642190180574763, 0.03184245908572818, 0.03184245908572818, 0.1796863967574508, 0.36241832566333293, 0.23907427365186248, 0.10050256089823519, 0.11725298771460771, 0.3722737064469057, 0.33993812838136156, 0.09617761681033644, 0.09120291249256042, 0.10032320374181647, 0.8569934146126521, 0.15082591192037462, 0.7541295596018731, 0.7138674783800081, 0.05710939827040065, 0.05710939827040065, 0.17132819481120196, 0.028554699135200326, 0.05092099758784825, 0.203683990351393, 0.1018419951756965, 0.6110519710541791, 0.828011232868155, 0.7946987132456503, 0.058575405037177515, 0.058575405037177515, 0.7614802654833076, 0.058575405037177515, 0.058575405037177515, 0.7515708465690061, 0.17506523151114486, 0.5126910351397813, 0.16256057211749164, 0.08128028605874582, 0.06252329696826602, 0.755237797007423, 0.3143739447522287, 0.3455883789829465, 0.1516129662634862, 0.08138048924437126, 0.10702091736246083, 0.25237951036153344, 0.6099171500403725, 0.02103162586346112, 0.04206325172692224, 0.08412650345384448, 0.34638945271013216, 0.30027738610529686, 0.13229771490196807, 0.10045843081767701, 0.12022074507689215, 0.09529933390919859, 0.09529933390919859, 0.6670953373643902, 0.047649666954599296, 0.09529933390919859, 0.8001707450480968, 0.8515390719608901, 0.1658302595001214, 0.829151297500607, 0.10729535195647001, 0.13411918994558752, 0.6258895530794084, 0.053647675978235006, 0.08941279329705835, 0.10375269618251164, 0.35914394832407875, 0.08779074292366369, 0.015961953258847944, 0.4309727379888945, 0.22211737731250614, 0.13882336082031632, 0.17352920102539543, 0.3713524901943462, 0.09370576855371353, 0.8217524347517924, 0.18754756122119237, 0.10229866975701402, 0.15344800463552102, 0.08524889146417836, 0.4603440139065631, 0.7916663964221512, 0.8185363963299243, 0.5441175070118753, 0.7921655762758385, 0.7663279164492122, 0.060842670115060324, 0.0275240650520511, 0.07677765514519518, 0.06953448013149752, 0.8017403314051427, 0.14199770066105916, 0.17749712582632393, 0.38654929624399437, 0.09860951434795774, 0.19721902869591548, 0.4852641010008777, 0.18578682724033602, 0.13310101056024073, 0.09427988248017052, 0.10167438306685056, 0.8506758295074978, 0.059248076788390946, 0.29624038394195473, 0.01974935892946365, 0.0394987178589273, 0.5924807678839095, 0.7274973702430084, 0.7135619984511397, 0.14271239969022792, 0.13902695284655567, 0.2126294572947322, 0.2453416814939218, 0.040890280248986964, 0.3598344661910853, 0.1751340780506638, 0.5226657641824498, 0.11766820869028975, 0.11766820869028975, 0.06841174923854056, 0.8032604979522163, 0.09117877298315764, 0.7294301838652612, 0.09117877298315764, 0.09117877298315764, 0.6531185860729778, 0.2591622286007519, 0.21570023995342083, 0.2672107450169243, 0.12797141101714146, 0.13119081758361043, 0.22559073047676761, 0.48643001259053015, 0.15509362720277772, 0.04934797229179291, 0.08459652392878786, 0.05509539311161361, 0.8815262897858177, 0.7931337309818386, 0.8206801178054768, 0.46911082875130805, 0.12330913212891526, 0.2117700312648762, 0.10856564893958844, 0.08578026582881061, 0.8357745148439021, 0.20348941823614233, 0.2057012597387091, 0.34504727440041527, 0.1061683921232047, 0.13934601466170615, 0.8181455614181017, 0.09635045760362501, 0.867154118432625, 0.17779533961772664, 0.38070072719968806, 0.18427535199559367, 0.13972526689775785, 0.11745022434883992, 0.2211894842702578, 0.164954869625277, 0.1387120494576193, 0.31116486770222707, 0.164954869625277, 0.07306585436662819, 0.17048699352213242, 0.5114609805663972, 0.13395406633881832, 0.10959878154994226, 0.11985702113064707, 0.059928510565323535, 0.11985702113064707, 0.059928510565323535, 0.6592136162185589, 0.25748450483931695, 0.22629028351381447, 0.22153184297263614, 0.17606230002359866, 0.11896101352945855, 0.543496969391877, 0.16722983673596215, 0.14442576809014915, 0.0988176307985231, 0.04560813729162604, 0.5592119728219945, 0.1035577727448138, 0.08284621819585103, 0.05385004182730317, 0.1988309236700425, 0.20212271294420053, 0.2300017078330558, 0.21257733602752127, 0.26833532580523173, 0.09060673338877956, 0.7584670948673938, 0.030137765358969292, 0.05525256982477704, 0.07032145250426168, 0.08539033518374634, 0.8334455445072644, 0.033723230124571395, 0.02339979233133525, 0.0516171889661807, 0.057811251642122384, 0.859215731511403, 0.8150550642673101, 0.18069934318010308, 0.7227973727204123, 0.15825596889109006, 0.15397878054268221, 0.17964191063312926, 0.3250663144789958, 0.17964191063312926, 0.7516306435951372, 0.15345714103709435, 0.10961224359792453, 0.5699836667092075, 0.04384489743916981, 0.13153469231750944, 0.16251035030679922, 0.08125517515339961, 0.6500414012271969, 0.08125517515339961, 0.712953929535024, 0.1425907859070048, 0.19758677652854442, 0.1481900823964083, 0.5927603295856332, 0.7968579049526384, 0.09073538148889174, 0.09073538148889174, 0.7258830519111339, 0.09073538148889174, 0.8466991350159037, 0.8325722749172256, 0.0988233384941751, 0.0988233384941751, 0.1976466769883502, 0.6917633694592258, 0.955463045068446, 0.02985822015838894, 0.3404023692620005, 0.18179730929816731, 0.20648583278310362, 0.12269448034938041, 0.1488792779849189, 0.8135438640728663, 0.12717558216422423, 0.06358779108211211, 0.12717558216422423, 0.06358779108211211, 0.6994657019032332, 0.855295739410197, 0.1425492899016995, 0.7974405458469821, 0.7519573068485176, 0.12532621780808625, 0.11145639221931437, 0.408673438137486, 0.07430426147954292, 0.012384043579923818, 0.3839053509776384, 0.22559801547679004, 0.1892112387869852, 0.45847338629154105, 0.07277355337960968, 0.058218842703687754, 0.8162671906466514, 0.15538818801020404, 0.4258787375094481, 0.15251062897297804, 0.12661259763794402, 0.14100039282407403, 0.2639412920058576, 0.2547824459110171, 0.21398394967036405, 0.15653300598454645, 0.11073877551034403, 0.7931002078746798, 0.8005049407362953, 0.4470432425553028, 0.25385279482510775, 0.07979011419691223, 0.11121430300734, 0.10807188412629723, 0.4290433758276025, 0.250047572536469, 0.09564661244564386, 0.15850010062420983, 0.0669526287119507, 0.22448472792300866, 0.3193700252925278, 0.2267990034686067, 0.1419422334633457, 0.08717104555085904, 0.07973125147047275, 0.1594625029409455, 0.039865625735236374, 0.039865625735236374, 0.637850011763782, 0.7821849043824951, 0.3225784673567087, 0.3572245384075335, 0.15473002605222733, 0.08442877508501968, 0.081065078866493, 0.20480945962426167, 0.1746184237433944, 0.29946243698049413, 0.10607661255439847, 0.21460114693697535, 0.3132354798970621, 0.13501529305907847, 0.20792355131098086, 0.08640978755781023, 0.2578792097428399, 0.2633426958754507, 0.44228068153441086, 0.09790946385112913, 0.08440471021649062, 0.11479040589442725, 0.7060107024782416, 0.8172135775975635, 0.8584432313306115, 0.10796280585869861, 0.10796280585869861, 0.10796280585869861, 0.6477768351521916, 0.07124561241211326, 0.07124561241211326, 0.7837017365332459, 0.07124561241211326, 0.2825683270416095, 0.20420904307208756, 0.19629396388324696, 0.1155601561570728, 0.2010430113965513, 0.20337230969332834, 0.3844958767237502, 0.24316935642838644, 0.06737295313415347, 0.10184283613302267, 0.31283382307902385, 0.2750869320298193, 0.1639145816794224, 0.18356364825298094, 0.06463508741302146, 0.25200940384990855, 0.45361692692983546, 0.12432463923262156, 0.10416388692462888, 0.06720250769330896, 0.7986228355092839, 0.03831395590296974, 0.2681976913207882, 0.07662791180593947, 0.5747093385445461, 0.1962166732376558, 0.13081111549177052, 0.6540555774588526, 0.3404312078799123, 0.2767595355831706, 0.08319765180107583, 0.14602036846719432, 0.15366096914280333, 0.3227764367906389, 0.27218451879837574, 0.11737324974205049, 0.17909538969261155, 0.10826670450344313, 0.4381221707644471, 0.2507078087864097, 0.12001095109119939, 0.07973330312223521, 0.11179102293426792, 0.6823025016132532, 0.15867500037517518, 0.04760250011255255, 0.039668750093793795, 0.07933750018758759, 0.8222414963647491, 0.8472809396220474, 0.81713508546059, 0.3322278937628569, 0.24774708649173044, 0.2577139233046161, 0.0911253651463836, 0.0711916915206122, 0.8216854044044296, 0.715578980289584, 0.05235943758216468, 0.03490629172144312, 0.13962516688577248, 0.05235943758216468, 0.21427151324003738, 0.441796934515541, 0.10603126428372983, 0.15904689642559475, 0.08173243288537507, 0.18544056440208956, 0.4422044228049828, 0.174742070301969, 0.08915411750100459, 0.10698494100120552, 0.24590470075770673, 0.2611153008045752, 0.30167690092955773, 0.06844770021090806, 0.12041725037104196, 0.8136151412684469, 0.259573386713286, 0.3505578727777368, 0.1699269077968419, 0.08964647891644414, 0.13112470050464964, 0.18142397921783096, 0.7256959168713238, 0.20447981083524558, 0.19750890819313494, 0.3624869373897535, 0.07203266063514333, 0.16265439498258172, 0.7962906076054821, 0.7062245890969985, 0.13141539548969322, 0.13141539548969322, 0.13141539548969322, 0.657076977448466, 0.4662285175200736, 0.19881216062417553, 0.20721267445336602, 0.060203682442532026, 0.06720411063352412, 0.8222108894610177, 0.10277636118262722, 0.10277636118262722, 0.751576736204003, 0.15090904946873174, 0.7545452473436586, 0.13488574960379876, 0.13488574960379876, 0.6744287480189938, 0.6527016519009016, 0.08383008857228363, 0.7544707971505527, 0.08383008857228363, 0.08383008857228363, 0.8285993317002728, 0.3916286259374305, 0.032635718828119206, 0.032635718828119206, 0.5221715012499073, 0.032635718828119206, 0.7303752350885678, 0.04789345803859461, 0.04789345803859461, 0.08381355156754057, 0.08381355156754057, 0.821383988705508, 0.06844866572545899, 0.1026729985881885, 0.034224332862729495, 0.6945131837479748, 0.049947865954477644, 0.028541637688272937, 0.10703114133102352, 0.1189234903678039, 0.15080671215613276, 0.7540335607806637, 0.8477766236412474, 0.8819172955273148, 0.7051391671839482, 0.05424147439876525, 0.05424147439876525, 0.08136221159814787, 0.09944270306440295, 0.6676777949034627, 0.05360916601414664, 0.019494242186962416, 0.10234477148155267, 0.15595393749569933, 0.8118842346953937, 0.04935466472312424, 0.028132158892180816, 0.03652245189511194, 0.07403199708468636, 0.7447715849668598, 0.09146317710119331, 0.018510404889527217, 0.07404161955810887, 0.07186392486522332, 0.1211382298365365, 0.2804815936984422, 0.24693562235909366, 0.08013759819955492, 0.271163268326401, 0.16199612453295578, 0.18513842803766375, 0.6016998911224072, 0.02314230350470797, 0.04628460700941594, 0.8974605050162643, 0.02653253535690373, 0.18572774749832613, 0.5571832424949784, 0.09286387374916306, 0.13266267678451868, 0.8460804997508931, 0.18110969250937825, 0.28538497001477786, 0.3594752987686144, 0.11799570875611008, 0.057625811252983994, 0.3386113052489273, 0.27367215081762614, 0.08929133734303904, 0.1008876149200571, 0.19829634656700879, 0.06215440515570044, 0.06215440515570044, 0.8080072670241057, 0.06215440515570044, 0.06215440515570044, 0.7158749336061815, 0.1431749867212363, 0.7684122165865667, 0.12806870276442778, 0.4092348574035474, 0.20526563597326825, 0.14887160335324404, 0.0944221925477034, 0.14217346154780053, 0.8655192587976315, 0.32569119464194574, 0.20649009458567164, 0.2665599402833216, 0.10981518666601628, 0.09104335988550068, 0.7521357449390593, 0.24160844533362094, 0.2564930140230409, 0.17648845731740848, 0.18898086318174312, 0.1366190768993192, 0.19969869591611458, 0.32524217575914427, 0.22376661692229127, 0.13725219817035889, 0.11448524586721878, 0.10892331827311723, 0.054461659136558614, 0.6263090800704241, 0.08169248870483792, 0.10892331827311723, 0.24283701411646202, 0.3344568399552737, 0.20369844773871723, 0.11830521200545586, 0.10051495456102641, 0.3045226736234891, 0.36278874475000505, 0.1583078158909113, 0.11983022175075926, 0.053868631796212874, 0.14513387670293354, 0.23749179824116398, 0.05277595516470311, 0.15832786549410932, 0.4090136525264491, 0.2709860050370253, 0.3510015577054383, 0.1290917583050396, 0.12055676602040889, 0.12802488426946076, 0.3696295640893148, 0.13861108653349305, 0.2123990778722182, 0.19998614474981585, 0.07930485050423731, 0.3623458068180657, 0.2615959971174328, 0.14051947142456694, 0.10074980970063291, 0.13433307960084387, 0.7514102108510698, 0.8450242140914643, 0.2767264430655786, 0.19038779282911805, 0.15718061966124863, 0.2789402546101032, 0.09519389641455903, 0.8545747701004244, 0.7488273623825197, 0.03736689020173607, 0.09341722550434019, 0.27090995396258655, 0.5137947402738711, 0.08407550295390617, 0.8728380310886729, 0.3294377493779701, 0.21613905250194865, 0.15687511875141433, 0.20742376812687005, 0.09063895750081717, 0.1116258392865843, 0.7813808750060901, 0.1116258392865843, 0.8297708922093123, 0.2638978336801421, 0.21911517099502706, 0.3006835923143437, 0.08316780212949933, 0.13274860724516238, 0.14345105179651482, 0.17173717468596844, 0.39398528310310404, 0.1131444915578145, 0.1777984867337085, 0.8470512891346288, 0.20606881894298282, 0.2501006178624236, 0.272116517322144, 0.08366041794693747, 0.18845609937520652, 0.8517106574677552, 0.11005102643024187, 0.11005102643024187, 0.11005102643024187, 0.6603061585814513, 0.05502551321512093, 0.3258506688178726, 0.07448015287265661, 0.5027410318904321, 0.05586011465449245, 0.046550095545410376, 0.12199107271345033, 0.731946436280702, 0.048796429085380136, 0.048796429085380136, 0.048796429085380136, 0.8481452306594728, 0.3772423512705238, 0.1746492366993166, 0.25778227336819126, 0.07894145498809109, 0.11107691454076535, 0.27517571215272596, 0.21118136048930133, 0.10239096266147943, 0.35196893414883557, 0.051195481330739714, 0.7235283548813436, 0.14129819110694664, 0.3675729167257633, 0.12153620633674432, 0.03655967182487431, 0.33298944337790926, 0.8451685633143559, 0.1555394439259656, 0.5596870154385156, 0.20653598291808548, 0.061833303527945346, 0.016573875172438957, 0.10015268073493042, 0.7010687651445129, 0.10015268073493042, 0.10015268073493042, 0.18576172014130812, 0.35405866317531376, 0.17941089210228905, 0.06827140141945512, 0.21116503229738445, 0.770978361922166, 0.8247071534598308, 0.16002748370681602, 0.4946304041847041, 0.14547953064256003, 0.11638362451404802, 0.08728771838553602, 0.14701144273292754, 0.5074265926588144, 0.0948460920857597, 0.07113456906431977, 0.1849498795672314, 0.18038039149680898, 0.4409298458810886, 0.13227895376432658, 0.10421978175371185, 0.14029586005307365, 0.8793965443879629, 0.0659815572821468, 0.1319631145642936, 0.7917786873857617, 0.10403795624633792, 0.16348821695853102, 0.5380248594453475, 0.06836779981902207, 0.12187303445999585, 0.3355311228255987, 0.2814241770555478, 0.1337360357712579, 0.14258371243805867, 0.10685271051444015, 0.21355370913680968, 0.08542148365472386, 0.08542148365472386, 0.08542148365472386, 0.5125289019283432, 0.4313771938173636, 0.2167205997647042, 0.17750449123585296, 0.08772024276190409, 0.08668823990588168, 0.8153111247681772, 0.12444218725887427, 0.7466531235532456, 0.155382115240971, 0.7769105762048552, 0.7518317655358776, 0.7476563491475312, 0.2836093907595021, 0.22901079146890277, 0.10161405979083765, 0.24721032456576922, 0.13801312598457055, 0.7231621133471922, 0.8736682158537897, 0.7517159310313214, 0.8161338709573012, 0.8140618667981527, 0.7713776259371109, 0.7483293440927474, 0.217909149073082, 0.653727447219246, 0.8158355136155343, 0.25754280509213867, 0.22096867892520775, 0.17982278698741044, 0.2194447570015856, 0.12191375388976979, 0.8170282886860977, 0.08915947507907111, 0.17831895015814223, 0.08915947507907111, 0.6241163255534978, 0.13917971633308046, 0.17713782078755694, 0.5693715668171473, 0.07591620890895297, 0.05061080593930198, 0.0715209601567256, 0.0715209601567256, 0.715209601567256, 0.0715209601567256, 0.0715209601567256, 0.054798739921293194, 0.21919495968517277, 0.5890864541539018, 0.041099054940969895, 0.09589779486226309, 0.7920345311745765, 0.7521006320448418, 0.1415616857138372, 0.23593614285639533, 0.04718722857127906, 0.5662467428553488, 0.15428495662692965, 0.431997878555403, 0.2742843673367638, 0.0719996464259005, 0.06857109183419095, 0.7972634220605367, 0.04696722576948866, 0.04696722576948866, 0.8923772896202845, 0.04696722576948866, 0.3614334664132781, 0.13849319741069535, 0.2634748633666887, 0.11991484166048011, 0.11653695879680462, 0.8288467638631057, 0.2609415077512477, 0.44465032786347664, 0.11097512398616281, 0.09147949409670178, 0.09147949409670178, 0.7512494830948259, 0.8202307542321187, 0.10252884427901483, 0.7230637611294737, 0.8563794984095715, 0.8436798832492831, 0.7982821847914278, 0.1140403121130611, 0.741759783033515, 0.07417597830335149, 0.06181331525279291, 0.04945065220223433, 0.07417597830335149, 0.34634109350705217, 0.17668415204997442, 0.26201456639229165, 0.07227988038408045, 0.14355587354060423, 0.7376593852454431, 0.04759092808035116, 0.11897732020087791, 0.04759092808035116, 0.04759092808035116, 0.0723383315074319, 0.0723383315074319, 0.0723383315074319, 0.1446766630148638, 0.651044983566887, 0.6653923145978856, 0.13307846291957712, 0.1659380689193722, 0.8296903445968611, 0.21605008501679862, 0.21315008387563353, 0.2610001027048574, 0.2291000901520415, 0.08120003195262229, 0.1104470382268423, 0.1104470382268423, 0.7731292675878961, 0.7706914600908196, 0.19676194748673406, 0.25110572345926063, 0.22112295050890113, 0.10681362863565563, 0.22487079712769606, 0.28860443814712994, 0.2618141527495061, 0.19362069901009984, 0.07732650557950528, 0.1783989459432681, 0.8727731448120463, 0.10525315541599423, 0.10525315541599423, 0.10525315541599423, 0.6315189324959654, 0.10525315541599423, 0.8371831486695137, 0.7992973012492793, 0.17431265897993978, 0.34232486040638777, 0.20686502300631407, 0.16066166761404088, 0.11550838848068298, 0.3927408930204308, 0.17998309665999943, 0.24664350283036957, 0.07277094340265408, 0.10776765664209842, 0.8826198268882331, 0.39633816645085773, 0.2049358324087362, 0.1957523870885334, 0.147901803578003, 0.054617332693837714, 0.15082212878703796, 0.7541106439351899, 0.7513318618160559, 0.28477733779523373, 0.21870899542673952, 0.15491887176060717, 0.23693474504563447, 0.10479806030864602, 0.7658497689758382, 0.04345246916174969, 0.05974714509740582, 0.07061026238784325, 0.05974714509740582, 0.8793525638097597, 0.7287562986578059, 0.025421731348528116, 0.042369552247546856, 0.07626519404558434, 0.1186347462931312, 0.8411743112850418, 0.09346381236500466, 0.335932584154606, 0.2341571799540887, 0.16114438998415237, 0.16851941927404493, 0.10030039834253879, 0.3236283303417541, 0.18729794190793306, 0.16228319173658973, 0.17197640742798526, 0.15477876668518675, 0.35040639813320645, 0.21003892285762374, 0.12704793382607485, 0.20901434274612316, 0.10348259126156097, 0.7924356338351283, 0.08753687476356001, 0.17507374952712002, 0.6127581233449201, 0.08753687476356001, 0.09612107479499779, 0.09612107479499779, 0.28836322438499334, 0.5286659113724879, 0.25792342521448164, 0.2730283329894734, 0.23227358182298624, 0.13166919607634311, 0.10487935964522568, 0.22532513143726207, 0.30177472960347596, 0.09857974500380215, 0.10260340806518183, 0.2715972566431284, 0.7234435945884515, 0.34949356352932187, 0.34113247349273523, 0.13503160409087436, 0.09824280792989311, 0.07650397383476783, 0.8334527355805291, 0.7987802436938932, 0.17269403113622517, 0.04317350778405629, 0.7771231401130133, 0.04317350778405629, 0.262302683123474, 0.09307514562445851, 0.5838350043716034, 0.025384130624852324, 0.0338455074998031, 0.3399067539811308, 0.11083915890689049, 0.11083915890689049, 0.09606060438597175, 0.3472960312415902, 0.8205332109764548, 0.48201807622208126, 0.16367031692615444, 0.14568456781339023, 0.12230309396679673, 0.0881301706525447, 0.798223790991687, 0.7237841570594928, 0.24009778491361725, 0.12004889245680862, 0.600244462284043, 0.22500129249203765, 0.47872615423837794, 0.12925606164436204, 0.10053249239005937, 0.06702166159337292, 0.7493476823336025, 0.752814566535867, 0.1254690944226445, 0.1254690944226445, 0.1946561076061895, 0.33091538293052214, 0.12166006725386844, 0.26765214795851056, 0.08516204707770791, 0.26455305293251297, 0.20743364377662948, 0.2224650672387041, 0.12526186218395502, 0.1803770815448952, 0.1965023655221255, 0.16262264732865558, 0.1965023655221255, 0.17278656278669657, 0.27103774554775933, 0.2801233876312288, 0.28877327846069667, 0.17632469767761433, 0.11311395700073372, 0.14105975814209146, 0.1253449156137054, 0.5241696471118589, 0.10255493095666804, 0.14813490027074272, 0.10255493095666804, 0.1898492659961829, 0.25899966089545484, 0.22002398377041066, 0.1068687921170566, 0.22379582349218913, 0.3226262850651449, 0.27624449355472236, 0.14869456690106045, 0.13437077834637112, 0.11800073428386906, 0.7490746618352575, 0.09363433272940719, 0.09363433272940719, 0.7512279708478131, 0.04418988063810666, 0.04418988063810666, 0.0736498010635111, 0.08837976127621332, 0.7855380101395919, 0.049096125633724494, 0.049096125633724494, 0.049096125633724494, 0.09819225126744899, 0.10571739339267117, 0.13011371494482607, 0.634304360356027, 0.06505685747241303, 0.07318896465646466, 0.7231325929804477, 0.37068684170293026, 0.1966621030408676, 0.10752748079932328, 0.0891346222415443, 0.23486265543010085, 0.7787520714550776, 0.06230016571640621, 0.031150082858203105, 0.06230016571640621, 0.07008768643095699, 0.19508107914288467, 0.30158480343170274, 0.16766427883091167, 0.10439473964943558, 0.23146106417223342, 0.2832554469954572, 0.167606773370093, 0.2128606021800181, 0.2195648731148218, 0.11900080909276603, 0.796252169666294, 0.31734043534652207, 0.16924823218481175, 0.1128321547898745, 0.3067624208349713, 0.09520213060395662, 0.7282871023906234, 0.11109464273755272, 0.03703154757918424, 0.07406309515836848, 0.04937539677224566, 0.13676308902075526, 0.40552497912506735, 0.12134921628276031, 0.08099289493237351, 0.2553097829875165, 0.7028909942917144, 0.09498526949888032, 0.04749263474944016, 0.10448379644876835, 0.04749263474944016, 0.7696266080818022, 0.0932880737068851, 0.04664403685344255, 0.04664403685344255, 0.06996605528016384, 0.8209132997113876, 0.06840944164261563, 0.06840944164261563, 0.06840944164261563, 0.8464521933624551, 0.8215647580766784, 0.8803537418330112, 0.7713860669468593], \"Term\": [\"accusation\", \"actually\", \"actually\", \"actually\", \"actually\", \"actually\", \"adrenaline\", \"advanced\", \"advanced\", \"alive\", \"alive\", \"alive\", \"alive\", \"alive\", \"alpha\", \"ambulance\", \"ambulance\", \"ambulance\", \"ambulance\", \"amp\", \"amp\", \"amp\", \"amp\", \"amp\", \"amphetamine\", \"ampnbsp\", \"ampnbsp\", \"ampnbsp\", \"ampnbsp\", \"ampnbsp\", \"anxiety\", \"anxiety\", \"anxiety\", \"anxiety\", \"anxiety\", \"anymore\", \"anymore\", \"anymore\", \"anymore\", \"anymore\", \"anymorei\", \"apple\", \"apple\", \"aren\\u201a\\u00e4\\u00f4t\", \"aren\\u201a\\u00e4\\u00f4t\", \"aren\\u201a\\u00e4\\u00f4t\", \"aren\\u201a\\u00e4\\u00f4t\", \"aren\\u201a\\u00e4\\u00f4t\", \"army\", \"army\", \"army\", \"army\", \"arthritis\", \"ashe\", \"assist\", \"assist\", \"assist\", \"assist\", \"assist\", \"athlete\", \"attack\", \"attack\", \"attack\", \"attack\", \"attack\", \"atypical\", \"away\", \"away\", \"away\", \"away\", \"away\", \"awhile\", \"awhile\", \"awhile\", \"awhile\", \"awhile\", \"bad\", \"bad\", \"bad\", \"bad\", \"bad\", \"bank\", \"bank\", \"bank\", \"bank\", \"bank\", \"basketball\", \"beating\", \"belly\", \"belly\", \"bill\", \"bill\", \"bill\", \"bill\", \"bill\", \"birthday\", \"birthday\", \"birthday\", \"birthday\", \"birthday\", \"body\", \"body\", \"body\", \"body\", \"body\", \"bojack\", \"boss\", \"boss\", \"boss\", \"boss\", \"boss\", \"briefly\", \"bupropion\", \"calculus\", \"can\\u201a\\u00e4\\u00f2t\", \"can\\u201a\\u00e4\\u00f4t\", \"can\\u201a\\u00e4\\u00f4t\", \"can\\u201a\\u00e4\\u00f4t\", \"can\\u201a\\u00e4\\u00f4t\", \"can\\u201a\\u00e4\\u00f4t\", \"capsule\", \"car\", \"car\", \"car\", \"car\", \"car\", \"care\", \"care\", \"care\", \"care\", \"care\", \"cataract\", \"celebrate\", \"celebrate\", \"celebrate\", \"celebrate\", \"celebrate\", \"celebration\", \"checkup\", \"checkup\", \"christmas\", \"christmas\", \"christmas\", \"christmas\", \"christmas\", \"class\", \"class\", \"class\", \"class\", \"class\", \"coaster\", \"cognitive\", \"cognitive\", \"cognitive\", \"cognitive\", \"collar\", \"come\", \"come\", \"come\", \"come\", \"come\", \"contact\", \"contact\", \"contact\", \"contact\", \"contact\", \"coz\", \"coz\", \"creativity\", \"crime\", \"cry\", \"cry\", \"cry\", \"cry\", \"cry\", \"crybaby\", \"dad\", \"dad\", \"dad\", \"dad\", \"dad\", \"dancing\", \"danny\", \"danny\", \"day\", \"day\", \"day\", \"day\", \"day\", \"death\", \"death\", \"death\", \"death\", \"death\", \"debt\", \"debt\", \"debt\", \"debt\", \"debt\", \"decline\", \"decline\", \"decline\", \"decline\", \"decline\", \"depression\", \"depression\", \"depression\", \"depression\", \"depression\", \"deserve\", \"deserve\", \"deserve\", \"deserve\", \"deserve\", \"didn\\u201a\\u00e4\\u00f4t\", \"didn\\u201a\\u00e4\\u00f4t\", \"didn\\u201a\\u00e4\\u00f4t\", \"didn\\u201a\\u00e4\\u00f4t\", \"didn\\u201a\\u00e4\\u00f4t\", \"doctor\", \"doctor\", \"doctor\", \"doctor\", \"doctor\", \"doesn\\u201a\\u00e4\\u00f4t\", \"doesn\\u201a\\u00e4\\u00f4t\", \"doesn\\u201a\\u00e4\\u00f4t\", \"doesn\\u201a\\u00e4\\u00f4t\", \"doesn\\u201a\\u00e4\\u00f4t\", \"don\\u201a\\u00e4\\u00f4t\", \"don\\u201a\\u00e4\\u00f4t\", \"don\\u201a\\u00e4\\u00f4t\", \"don\\u201a\\u00e4\\u00f4t\", \"don\\u201a\\u00e4\\u00f4t\", \"doormat\", \"douche\", \"downvote\", \"downvote\", \"dream\", \"dream\", \"dream\", \"dream\", \"dream\", \"drinker\", \"dump\", \"dump\", \"dump\", \"dump\", \"dump\", \"dwell\", \"dwell\", \"dwell\", \"dwell\", \"dysphoria\", \"dysphoria\", \"effexor\", \"effexor\", \"effexor\", \"eh\", \"eld\", \"eld\", \"eld\", \"eld\", \"electrical\", \"emo\", \"employ\", \"employ\", \"employ\", \"employ\", \"emptypost\", \"emptypost\", \"end\", \"end\", \"end\", \"end\", \"end\", \"ending\", \"entertainment\", \"entertainment\", \"entertainment\", \"entertainment\", \"entertainment\", \"equivalent\", \"equivalent\", \"escort\", \"establish\", \"establish\", \"eve\", \"eve\", \"eve\", \"eve\", \"eve\", \"ex\", \"ex\", \"ex\", \"ex\", \"ex\", \"extreamly\", \"fail\", \"fail\", \"fail\", \"fail\", \"fail\", \"family\", \"family\", \"family\", \"family\", \"family\", \"fckin\", \"feb\", \"feel\", \"feel\", \"feel\", \"feel\", \"feel\", \"feeling\", \"feeling\", \"feeling\", \"feeling\", \"feeling\", \"find\", \"find\", \"find\", \"find\", \"find\", \"firework\", \"firework\", \"firework\", \"firework\", \"firework\", \"fluoxetine\", \"friend\", \"friend\", \"friend\", \"friend\", \"friend\", \"fuck\", \"fuck\", \"fuck\", \"fuck\", \"fuck\", \"fucking\", \"fucking\", \"fucking\", \"fucking\", \"fucking\", \"fun\", \"fun\", \"fun\", \"fun\", \"fun\", \"furthermore\", \"futility\", \"gad\", \"gathering\", \"gathering\", \"gathering\", \"gathering\", \"ge\", \"ge\", \"ge\", \"ge\", \"get\", \"get\", \"get\", \"get\", \"get\", \"go\", \"go\", \"go\", \"go\", \"go\", \"good\", \"good\", \"good\", \"good\", \"good\", \"grade\", \"grade\", \"grade\", \"grade\", \"grade\", \"grope\", \"gt\", \"gt\", \"gt\", \"gt\", \"gti\", \"gti\", \"gti\", \"happy\", \"happy\", \"happy\", \"happy\", \"happy\", \"hard\", \"hard\", \"hard\", \"hard\", \"hard\", \"hate\", \"hate\", \"hate\", \"hate\", \"hate\", \"haven\\u201a\\u00e4\\u00f4t\", \"haven\\u201a\\u00e4\\u00f4t\", \"haven\\u201a\\u00e4\\u00f4t\", \"haven\\u201a\\u00e4\\u00f4t\", \"haven\\u201a\\u00e4\\u00f4t\", \"hawaii\", \"heartbeat\", \"hella\", \"help\", \"help\", \"help\", \"help\", \"help\", \"here\\u201a\\u00e4\\u00f9\", \"he\\u201a\\u00e4\\u00f4s\", \"he\\u201a\\u00e4\\u00f4s\", \"he\\u201a\\u00e4\\u00f4s\", \"he\\u201a\\u00e4\\u00f4s\", \"he\\u201a\\u00e4\\u00f4s\", \"high\", \"high\", \"high\", \"high\", \"high\", \"hit\", \"hit\", \"hit\", \"hit\", \"hit\", \"home\", \"home\", \"home\", \"home\", \"home\", \"homeschool\", \"hope\", \"hope\", \"hope\", \"hope\", \"hope\", \"hoping\", \"hoping\", \"house\", \"house\", \"house\", \"house\", \"house\", \"hud\", \"hum\", \"hungover\", \"hungover\", \"hungover\", \"hungover\", \"hurt\", \"hurt\", \"hurt\", \"hurt\", \"hurt\", \"hyper\", \"hyper\", \"hyper\", \"ibs\", \"ibuprofen\", \"ibuprofen\", \"indian\", \"indian\", \"indian\", \"indians\", \"infront\", \"infront\", \"infront\", \"infront\", \"intimidate\", \"irrational\", \"irrational\", \"irrational\", \"irrational\", \"irrational\", \"isn\\u201a\\u00e4\\u00f4t\", \"isn\\u201a\\u00e4\\u00f4t\", \"isn\\u201a\\u00e4\\u00f4t\", \"isn\\u201a\\u00e4\\u00f4t\", \"isn\\u201a\\u00e4\\u00f4t\", \"it\\u201a\\u00e4\\u00f4ll\", \"it\\u201a\\u00e4\\u00f4ll\", \"it\\u201a\\u00e4\\u00f4ll\", \"it\\u201a\\u00e4\\u00f4ll\", \"it\\u201a\\u00e4\\u00f4s\", \"it\\u201a\\u00e4\\u00f4s\", \"it\\u201a\\u00e4\\u00f4s\", \"it\\u201a\\u00e4\\u00f4s\", \"it\\u201a\\u00e4\\u00f4s\", \"iv\", \"iv\", \"i\\u201a\\u00e4\\u00f2m\", \"i\\u201a\\u00e4\\u00f2ve\", \"i\\u201a\\u00e4\\u00f4d\", \"i\\u201a\\u00e4\\u00f4d\", \"i\\u201a\\u00e4\\u00f4d\", \"i\\u201a\\u00e4\\u00f4d\", \"i\\u201a\\u00e4\\u00f4d\", \"i\\u201a\\u00e4\\u00f4ll\", \"i\\u201a\\u00e4\\u00f4ll\", \"i\\u201a\\u00e4\\u00f4ll\", \"i\\u201a\\u00e4\\u00f4ll\", \"i\\u201a\\u00e4\\u00f4ll\", \"i\\u201a\\u00e4\\u00f4m\", \"i\\u201a\\u00e4\\u00f4m\", \"i\\u201a\\u00e4\\u00f4m\", \"i\\u201a\\u00e4\\u00f4m\", \"i\\u201a\\u00e4\\u00f4m\", \"i\\u201a\\u00e4\\u00f4ve\", \"i\\u201a\\u00e4\\u00f4ve\", \"i\\u201a\\u00e4\\u00f4ve\", \"i\\u201a\\u00e4\\u00f4ve\", \"i\\u201a\\u00e4\\u00f4ve\", \"job\", \"job\", \"job\", \"job\", \"job\", \"john\", \"john\", \"john\", \"john\", \"john\", \"judgment\", \"k\", \"k\", \"k\", \"k\", \"k\", \"ketamine\", \"kid\", \"kid\", \"kid\", \"kid\", \"kid\", \"kill\", \"kill\", \"kill\", \"kill\", \"kill\", \"kim\", \"kim\", \"kim\", \"kim\", \"kim\", \"king\", \"king\", \"knot\", \"knot\", \"know\", \"know\", \"know\", \"know\", \"know\", \"ladder\", \"leave\", \"leave\", \"leave\", \"leave\", \"leave\", \"legend\", \"life\", \"life\", \"life\", \"life\", \"life\", \"live\", \"live\", \"live\", \"live\", \"live\", \"loan\", \"loan\", \"loan\", \"loan\", \"loan\", \"long\", \"long\", \"long\", \"long\", \"long\", \"lose\", \"lose\", \"lose\", \"lose\", \"lose\", \"loser\", \"loser\", \"loser\", \"loser\", \"loser\", \"lot\", \"lot\", \"lot\", \"lot\", \"lot\", \"love\", \"love\", \"love\", \"love\", \"love\", \"make\", \"make\", \"make\", \"make\", \"make\", \"maoi\", \"mathematical\", \"matter\", \"matter\", \"matter\", \"matter\", \"matter\", \"mdd\", \"medicaid\", \"mg\", \"mg\", \"mg\", \"mg\", \"mg\", \"mike\", \"mind\", \"mind\", \"mind\", \"mind\", \"mind\", \"mini\", \"mini\", \"mini\", \"mm\", \"mom\", \"mom\", \"mom\", \"mom\", \"mom\", \"money\", \"money\", \"money\", \"money\", \"money\", \"monotone\", \"month\", \"month\", \"month\", \"month\", \"month\", \"monthly\", \"movement\", \"movement\", \"movement\", \"movement\", \"movement\", \"mum\", \"mum\", \"mum\", \"mum\", \"mum\", \"n\", \"n\", \"n\", \"n\", \"n\", \"nan\", \"need\", \"need\", \"need\", \"need\", \"need\", \"negative\", \"negative\", \"negative\", \"negative\", \"negative\", \"ness\", \"new\", \"new\", \"new\", \"new\", \"new\", \"norway\", \"not\", \"not\", \"not\", \"not\", \"not\", \"off\", \"off\", \"off\", \"off\", \"old\", \"old\", \"old\", \"old\", \"old\", \"organized\", \"oversleep\", \"panic\", \"panic\", \"panic\", \"panic\", \"panic\", \"party\", \"party\", \"party\", \"party\", \"party\", \"pass\", \"pass\", \"pass\", \"pass\", \"pass\", \"passively\", \"paxil\", \"paxil\", \"paxil\", \"pay\", \"pay\", \"pay\", \"pay\", \"pay\", \"people\", \"people\", \"people\", \"people\", \"people\", \"performance\", \"performance\", \"performance\", \"performance\", \"performance\", \"person\", \"person\", \"person\", \"person\", \"person\", \"peru\", \"pizza\", \"pizza\", \"placebo\", \"placebo\", \"premonition\", \"preserve\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"procrastinating\", \"promotion\", \"prop\", \"psychotherapy\", \"pt\", \"puzzle\", \"queue\", \"quirky\", \"quirky\", \"raman\", \"reason\", \"reason\", \"reason\", \"reason\", \"reason\", \"reckon\", \"reflection\", \"reflection\", \"reflection\", \"reflection\", \"refuse\", \"refuse\", \"refuse\", \"refuse\", \"refuse\", \"rehab\", \"rehab\", \"rehab\", \"rehab\", \"rehab\", \"rent\", \"rent\", \"rent\", \"rent\", \"rent\", \"rift\", \"roller\", \"root\", \"root\", \"root\", \"root\", \"s\", \"s\", \"s\", \"s\", \"s\", \"salvage\", \"sara\", \"sara\", \"sara\", \"sara\", \"say\", \"say\", \"say\", \"say\", \"say\", \"scamme\", \"school\", \"school\", \"school\", \"school\", \"school\", \"scientifically\", \"seeker\", \"seeker\", \"selfdoubt\", \"selfie\", \"sensitivity\", \"she\\u201a\\u00e4\\u00f4d\", \"she\\u201a\\u00e4\\u00f4d\", \"she\\u201a\\u00e4\\u00f4s\", \"she\\u201a\\u00e4\\u00f4s\", \"she\\u201a\\u00e4\\u00f4s\", \"she\\u201a\\u00e4\\u00f4s\", \"she\\u201a\\u00e4\\u00f4s\", \"shit\", \"shit\", \"shit\", \"shit\", \"shit\", \"shouldn\\u201a\\u00e4\\u00f4t\", \"shouldn\\u201a\\u00e4\\u00f4t\", \"shouldn\\u201a\\u00e4\\u00f4t\", \"shouldn\\u201a\\u00e4\\u00f4t\", \"shouldn\\u201a\\u00e4\\u00f4t\", \"shrink\", \"shrink\", \"shrink\", \"shrink\", \"shrink\", \"singer\", \"singer\", \"skydive\", \"skydive\", \"sleep\", \"sleep\", \"sleep\", \"sleep\", \"sleep\", \"sofa\", \"sofa\", \"sofa\", \"somethingi\", \"spend\", \"spend\", \"spend\", \"spend\", \"spend\", \"start\", \"start\", \"start\", \"start\", \"start\", \"steve\", \"strategy\", \"strategy\", \"strategy\", \"strategy\", \"strategy\", \"strip\", \"sunny\", \"take\", \"take\", \"take\", \"take\", \"take\", \"talk\", \"talk\", \"talk\", \"talk\", \"talk\", \"tap\", \"tell\", \"tell\", \"tell\", \"tell\", \"tell\", \"temperature\", \"temperature\", \"testing\", \"thank\", \"thank\", \"thank\", \"thank\", \"thank\", \"that\\u201a\\u00e4\\u00f4s\", \"that\\u201a\\u00e4\\u00f4s\", \"that\\u201a\\u00e4\\u00f4s\", \"that\\u201a\\u00e4\\u00f4s\", \"that\\u201a\\u00e4\\u00f4s\", \"that\\u201a\\u00e4\\u00f9\", \"there\\u201a\\u00e4\\u00f4s\", \"there\\u201a\\u00e4\\u00f4s\", \"there\\u201a\\u00e4\\u00f4s\", \"there\\u201a\\u00e4\\u00f4s\", \"there\\u201a\\u00e4\\u00f4s\", \"they\\u201a\\u00e4\\u00f4d\", \"they\\u201a\\u00e4\\u00f4d\", \"thing\", \"thing\", \"thing\", \"thing\", \"thing\", \"think\", \"think\", \"think\", \"think\", \"think\", \"thought\", \"thought\", \"thought\", \"thought\", \"thought\", \"thoughtless\", \"tick\", \"tick\", \"tick\", \"tick\", \"ticket\", \"ticket\", \"ticket\", \"ticket\", \"time\", \"time\", \"time\", \"time\", \"time\", \"today\", \"today\", \"today\", \"today\", \"today\", \"today\\u201a\\u00e4\\u00f4s\", \"try\", \"try\", \"try\", \"try\", \"try\", \"tshirt\", \"tumour\", \"tyrant\", \"tyrant\", \"tyrant\", \"tyrant\", \"u\", \"u\", \"u\", \"u\", \"u\", \"ugly\", \"ugly\", \"ugly\", \"ugly\", \"ugly\", \"unblocked\", \"understand\", \"understand\", \"understand\", \"understand\", \"understand\", \"unpaid\", \"unremarkable\", \"variety\", \"variety\", \"variety\", \"ve\", \"ve\", \"ve\", \"ve\", \"ve\", \"vegas\", \"vyvanse\", \"vyvanse\", \"vyvanse\", \"wake\", \"wake\", \"wake\", \"wake\", \"wake\", \"want\", \"want\", \"want\", \"want\", \"want\", \"watch\", \"watch\", \"watch\", \"watch\", \"watch\", \"way\", \"way\", \"way\", \"way\", \"way\", \"weed\", \"weed\", \"weed\", \"weed\", \"weed\", \"week\", \"week\", \"week\", \"week\", \"week\", \"well\", \"well\", \"well\", \"well\", \"well\", \"we\\u201a\\u00e4\\u00f4ve\", \"we\\u201a\\u00e4\\u00f4ve\", \"we\\u201a\\u00e4\\u00f4ve\", \"what\\u201a\\u00e4\\u00f4s\", \"what\\u201a\\u00e4\\u00f4s\", \"what\\u201a\\u00e4\\u00f4s\", \"what\\u201a\\u00e4\\u00f4s\", \"what\\u201a\\u00e4\\u00f4s\", \"who\\u201a\\u00e4\\u00f4s\", \"who\\u201a\\u00e4\\u00f4s\", \"who\\u201a\\u00e4\\u00f4s\", \"who\\u201a\\u00e4\\u00f4s\", \"who\\u201a\\u00e4\\u00f4s\", \"wife\", \"wife\", \"wife\", \"wife\", \"wife\", \"winner\", \"wish\", \"wish\", \"wish\", \"wish\", \"wish\", \"won\\u201a\\u00e4\\u00f4t\", \"won\\u201a\\u00e4\\u00f4t\", \"won\\u201a\\u00e4\\u00f4t\", \"won\\u201a\\u00e4\\u00f4t\", \"won\\u201a\\u00e4\\u00f4t\", \"work\", \"work\", \"work\", \"work\", \"work\", \"world\", \"world\", \"world\", \"world\", \"world\", \"wornout\", \"worry\", \"worry\", \"worry\", \"worry\", \"worry\", \"wouldn\\u201a\\u00e4\\u00f4t\", \"wouldn\\u201a\\u00e4\\u00f4t\", \"wouldn\\u201a\\u00e4\\u00f4t\", \"wouldn\\u201a\\u00e4\\u00f4t\", \"wouldn\\u201a\\u00e4\\u00f4t\", \"year\", \"year\", \"year\", \"year\", \"year\", \"you\\u201a\\u00e4\\u00f4re\", \"you\\u201a\\u00e4\\u00f4re\", \"you\\u201a\\u00e4\\u00f4re\", \"you\\u201a\\u00e4\\u00f4re\", \"you\\u201a\\u00e4\\u00f4re\", \"you\\u201a\\u00e4\\u00f4ve\", \"you\\u201a\\u00e4\\u00f4ve\", \"you\\u201a\\u00e4\\u00f4ve\", \"you\\u201a\\u00e4\\u00f4ve\", \"you\\u201a\\u00e4\\u00f4ve\", \"\\u00e4\\u00fa\", \"\\u00e4\\u00fa\", \"\\u00e4\\u00fa\", \"\\u00e4\\u00fa\", \"\\u00e4\\u00faare\", \"\\u00e4\\u00fanormal\\u201a\\u00e4\\u00f9\", \"\\u00e4\\u00faoh\", \"\\uf8ff\\u00fc\\u00f2\\u00ee\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [4, 3, 1, 2, 5]};\n",
              "\n",
              "function LDAvis_load_lib(url, callback){\n",
              "  var s = document.createElement('script');\n",
              "  s.src = url;\n",
              "  s.async = true;\n",
              "  s.onreadystatechange = s.onload = callback;\n",
              "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
              "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
              "}\n",
              "\n",
              "if(typeof(LDAvis) !== \"undefined\"){\n",
              "   // already loaded: just create the visualization\n",
              "   !function(LDAvis){\n",
              "       new LDAvis(\"#\" + \"ldavis_el1361405861644940002909141144\", ldavis_el1361405861644940002909141144_data);\n",
              "   }(LDAvis);\n",
              "}else if(typeof define === \"function\" && define.amd){\n",
              "   // require.js is available: use it to load d3/LDAvis\n",
              "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
              "   require([\"d3\"], function(d3){\n",
              "      window.d3 = d3;\n",
              "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
              "        new LDAvis(\"#\" + \"ldavis_el1361405861644940002909141144\", ldavis_el1361405861644940002909141144_data);\n",
              "      });\n",
              "    });\n",
              "}else{\n",
              "    // require.js not available: dynamically load d3 & LDAvis\n",
              "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
              "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
              "                 new LDAvis(\"#\" + \"ldavis_el1361405861644940002909141144\", ldavis_el1361405861644940002909141144_data);\n",
              "            })\n",
              "         });\n",
              "}\n",
              "</script>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = CountVectorizer()\n",
        "data_vectorized = vectorizer.fit_transform(df['lemmas_back_to_text'])\n",
        "     \n",
        "\n",
        "# Define Search Param\n",
        "search_params = {'n_components': [10, 15, 20, 25, 30], 'learning_decay': [.5, .7, .9]}\n",
        "\n",
        "# Init the Model\n",
        "lda = LatentDirichletAllocation()\n",
        "\n",
        "# Init Grid Search Class\n",
        "model = GridSearchCV(lda, param_grid=search_params)\n",
        "\n",
        "# Do the Grid Search\n",
        "model.fit(data_vectorized)\n",
        "GridSearchCV(cv=None, error_score='raise',\n",
        "             estimator=LatentDirichletAllocation(batch_size=128, \n",
        "                                                 doc_topic_prior=None,\n",
        "                                                 evaluate_every=-1, \n",
        "                                                 learning_decay=0.7, \n",
        "                                                 learning_method=None,\n",
        "                                                 learning_offset=10.0, \n",
        "                                                 max_doc_update_iter=100, \n",
        "                                                 max_iter=10,\n",
        "                                                 mean_change_tol=0.001, \n",
        "                                                 n_components=10, \n",
        "                                                 n_jobs=1,\n",
        "                                                 perp_tol=0.1, \n",
        "                                                 random_state=None,\n",
        "                                                 topic_word_prior=None, \n",
        "                                                 total_samples=1000000.0, \n",
        "                                                 verbose=0),\n",
        "             iid=True, n_jobs=1,\n",
        "             param_grid={'n_topics': [10, 15, 20, 30], \n",
        "                         'learning_decay': [0.5, 0.7, 0.9]},\n",
        "             pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
        "             scoring=None, verbose=0)\n",
        "     \n",
        "\n",
        "# Best Model\n",
        "best_lda_model = model.best_estimator_\n",
        "\n",
        "# Model Parameters\n",
        "print(\"Best Model's Params: \", model.best_params_)\n",
        "\n",
        "# Log Likelihood Score\n",
        "print(\"Best Log Likelihood Score: \", model.best_score_)\n",
        "\n",
        "# Perplexity\n",
        "print(\"Model Perplexity: \", best_lda_model.perplexity(data_vectorized))\n",
        "     "
      ],
      "metadata": {
        "id": "gvoJelGe8CFK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "outputId": "8d6fc86c-52da-4d84-f78b-f96ded8fea8d"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-d00c56fed8bf>\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Do the Grid Search\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_vectorized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m GridSearchCV(cv=None, error_score='raise',\n\u001b[1;32m     17\u001b[0m              estimator=LatentDirichletAllocation(batch_size=128, \n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    872\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 874\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    875\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1386\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1387\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1388\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    819\u001b[0m                     )\n\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 821\u001b[0;31m                 out = parallel(\n\u001b[0m\u001b[1;32m    822\u001b[0m                     delayed(_fit_and_score)(\n\u001b[1;32m    823\u001b[0m                         \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         )\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1086\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1087\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1088\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1089\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1090\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    899\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    900\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 901\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    902\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    595\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 597\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    598\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    289\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    289\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    682\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 684\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    685\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/decomposition/_lda.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    666\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m                     \u001b[0;31m# batch update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 668\u001b[0;31m                     self._em_step(\n\u001b[0m\u001b[1;32m    669\u001b[0m                         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_update\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparallel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparallel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m                     )\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/decomposition/_lda.py\u001b[0m in \u001b[0;36m_em_step\u001b[0;34m(self, X, total_samples, batch_update, parallel)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m         \u001b[0;31m# E-step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m         _, suff_stats = self._e_step(\n\u001b[0m\u001b[1;32m    518\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcal_sstats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_init\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparallel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparallel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/decomposition/_lda.py\u001b[0m in \u001b[0;36m_e_step\u001b[0;34m(self, X, cal_sstats, random_init, parallel)\u001b[0m\n\u001b[1;32m    458\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mparallel\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m             \u001b[0mparallel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m         results = parallel(\n\u001b[0m\u001b[1;32m    461\u001b[0m             delayed(_update_doc_distribution)(\n\u001b[1;32m    462\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx_slice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         )\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1083\u001b[0m             \u001b[0;31m# remaining jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1085\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1086\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1087\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    899\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    900\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 901\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    902\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    595\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 597\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    598\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    289\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    289\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/decomposition/_lda.py\u001b[0m in \u001b[0;36m_update_doc_distribution\u001b[0;34m(X, exp_topic_word_distr, doc_topic_prior, max_doc_update_iter, mean_change_tol, cal_sstats, random_state)\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0mnorm_phi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_doc_topic_d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_topic_word_d\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0mdoc_topic_d\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp_doc_topic_d\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnts\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnorm_phi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_topic_word_d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m             \u001b[0;31m# Note: adds doc_topic_prior to doc_topic_d, in-place.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m             \u001b[0mdirichlet_expectation_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_topic_d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_topic_prior\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_doc_topic_d\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Defining a function to loop over number of topics to be used to find an \n",
        "#optimal number of tipics\n",
        "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
        "    \"\"\"\n",
        "    Compute c_v coherence for various number of topics\n",
        "\n",
        "    Parameters:\n",
        "    ----------\n",
        "    dictionary : Gensim dictionary\n",
        "    corpus : Gensim corpus\n",
        "    texts : List of input texts\n",
        "    limit : Max num of topics\n",
        "\n",
        "    Returns:\n",
        "    -------\n",
        "    model_list : List of LDA topic models\n",
        "    coherence_values : Coherence values corresponding to the \n",
        "    LDA model with respective number of topics\n",
        "    \"\"\"\n",
        "    coherence_values_topic = []\n",
        "    model_list_topic = []\n",
        "    for num_topics in range(start, limit, step):\n",
        "        model = LdaMulticore(corpus=corpus, num_topics=num_topics, id2word=id2word)\n",
        "        model_list_topic.append(model)\n",
        "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
        "        coherence_values_topic.append(coherencemodel.get_coherence())\n",
        "\n",
        "    return model_list_topic, coherence_values_topic    \n",
        "     \n",
        "\n",
        "# Can take a long time to run.\n",
        "model_list_topic, coherence_values_topic = compute_coherence_values(dictionary=id2word,\n",
        "                                                        corpus=corpus,\n",
        "                                                        texts=df['lemma_tokens'],\n",
        "                                                        start=2, limit=200, step=6)\n",
        "     "
      ],
      "metadata": {
        "id": "QHeCr3HD8jZr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_5_2 = LdaMulticore(corpus=corpus,\n",
        "                       id2word=id2word,\n",
        "                       num_topics=68,\n",
        "                       random_state=42,\n",
        "                       chunksize=2000,\n",
        "                       passes=25,\n",
        "                       decay=0.5,\n",
        "                       iterations=70)"
      ],
      "metadata": {
        "id": "v7L-Aipi8wH6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}